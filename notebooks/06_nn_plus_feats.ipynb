{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.14 (you have 1.4.11). Upgrade using: pip install --upgrade albumentations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.cpu_count())\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import math\n",
    "from glob import glob\n",
    "import wandb\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torcheval.metrics.functional import binary_auroc\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from colorama import Fore, Style\n",
    "\n",
    "b_ = Fore.BLUE\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.set_float32_matmul_precision(\"highest\")\n",
    "\n",
    "\n",
    "# Set the random seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2090888/1057363262.py:1: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_metadata_df = pd.read_csv(\"../data/train-metadata.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 401059\n"
     ]
    }
   ],
   "source": [
    "train_metadata_df = pd.read_csv(\"../data/train-metadata.csv\")\n",
    "print(f\"Train: {len(train_metadata_df)}\")\n",
    "\n",
    "train_cols = [\n",
    "    'isic_id', 'patient_id', 'target', 'age_approx', 'sex',\n",
    "    'anatom_site_general', 'clin_size_long_diam_mm',\n",
    "    'tbp_tile_type', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext',\n",
    "    'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L',\n",
    "    'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio',\n",
    "    'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB',\n",
    "    'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm',\n",
    "    'tbp_lv_eccentricity', 'tbp_lv_location', 'tbp_lv_location_simple',\n",
    "    'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', 'tbp_lv_norm_border',\n",
    "    'tbp_lv_norm_color', 'tbp_lv_perimeterMM',\n",
    "    'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt',\n",
    "    'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'tbp_lv_x', 'tbp_lv_y',\n",
    "    'tbp_lv_z'\n",
    "]\n",
    "\n",
    "train_metadata_df = train_metadata_df[train_cols].dropna().reset_index(drop=True) # dropping nan doesn't drop any pos sample\n",
    "\n",
    "N_SPLITS = 4\n",
    "gkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "train_metadata_df[\"fold\"] = -1\n",
    "for idx, (train_idx, val_idx) in enumerate(\n",
    "    gkf.split(\n",
    "        train_metadata_df,\n",
    "        train_metadata_df[\"target\"],\n",
    "        groups=train_metadata_df[\"patient_id\"],\n",
    "    )\n",
    "):\n",
    "    train_metadata_df.loc[val_idx, \"fold\"] = idx\n",
    "\n",
    "train_metadata_df = train_metadata_df.drop(columns=[\"patient_id\"])\n",
    "\n",
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "features = train_metadata_df.drop(columns=['target', 'isic_id', 'fold'])\n",
    "numerical_cols = features.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "train_metadata_df[numerical_cols] = scaler.fit_transform(train_metadata_df[numerical_cols])\n",
    "\n",
    "# Categorical\n",
    "female_male = pd.get_dummies(train_metadata_df[\"sex\"]).astype(float)\n",
    "train_metadata_df = train_metadata_df.drop(columns=[\"sex\"])\n",
    "train_metadata_df = train_metadata_df.join(female_male)\n",
    "\n",
    "anatom_site_general = pd.get_dummies(train_metadata_df[\"anatom_site_general\"]).astype(float)\n",
    "train_metadata_df = train_metadata_df.drop(columns=[\"anatom_site_general\"])\n",
    "train_metadata_df = train_metadata_df.join(anatom_site_general)\n",
    "\n",
    "train_metadata_df = train_metadata_df.drop(columns=[\"tbp_tile_type\", \"tbp_lv_location\", \"tbp_lv_location_simple\"]) # ignoring two cat columns for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "class SkinDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, file_hdf: str, transform=None):\n",
    "        assert \"isic_id\" in df.columns\n",
    "        assert \"target\" in df.columns\n",
    "\n",
    "        # add features\n",
    "        feature_cols = df.select_dtypes(include=['float64']).columns\n",
    "        self.features = df[feature_cols].values.astype('float32')\n",
    "\n",
    "        self.fp_hdf = h5py.File(file_hdf, mode=\"r\")\n",
    "        self.isic_ids = df['isic_id'].values\n",
    "        self.labels = df.target.tolist()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.isic_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        isic_id = self.isic_ids[idx]\n",
    "        image = np.array(Image.open(BytesIO(self.fp_hdf[isic_id][()])))\n",
    "        label = self.labels[idx] / 1.0\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        return image, self.features[idx], label\n",
    "\n",
    "    def get_class_samples(self, class_label):\n",
    "        indices = [i for i, label in enumerate(self.labels) if label == class_label]\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/pydantic/main.py:308: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `Union[float, json-or-python[json=list[float], python=list[float]]]` but got `tuple` - serialized value may not be as expected\n",
      "  Expected `Union[float, json-or-python[json=list[float], python=list[float]]]` but got `tuple` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "transforms_train = A.Compose(\n",
    "    [\n",
    "        A.Resize(124, 124),\n",
    "        A.CenterCrop(\n",
    "            height=124,\n",
    "            width=124,\n",
    "            p=1.0,\n",
    "        ),\n",
    "        A.CLAHE(\n",
    "            clip_limit=4, tile_grid_size=(10, 10), p=0.5\n",
    "        ),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.3, rotate_limit=60, p=0.6),\n",
    "        A.HueSaturationValue(\n",
    "            hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n",
    "        ),\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n",
    "        ),\n",
    "        A.RandomRotate90(p=0.6),\n",
    "        A.Flip(p=0.7),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms_valid = A.Compose(\n",
    "    [\n",
    "        A.Resize(124, 124),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders_and_stats(fold):\n",
    "    # dataloaders\n",
    "    train_df = train_metadata_df.loc[train_metadata_df.fold != fold]\n",
    "    valid_df = train_metadata_df.loc[train_metadata_df.fold == fold]\n",
    "\n",
    "    num_workers = 24  # based on profiling\n",
    "\n",
    "    file_hdf = \"/home/ubuntu/ayusht/skin/data/train-image.hdf5\"\n",
    "    train_dataset = SkinDataset(train_df, file_hdf, transform=transforms_train)\n",
    "    valid_dataset = SkinDataset(valid_df, file_hdf, transform=transforms_valid)\n",
    "    dataset_sizes = {\"train\": len(train_dataset), \"val\": len(valid_dataset)}\n",
    "    print(dataset_sizes)\n",
    "\n",
    "    # calculate bias value\n",
    "    neg_samples = len(train_dataset.get_class_samples(0))\n",
    "    pos_samples = len(train_dataset.get_class_samples(1))\n",
    "    p_positive = pos_samples / (neg_samples + pos_samples)\n",
    "    bias_value = math.log(p_positive / (1 - p_positive))\n",
    "    print(f\"Calculated bias value: {bias_value}\")\n",
    "\n",
    "    # calculate class weight\n",
    "    pos_weight = torch.ones([1]) * (neg_samples / pos_samples)\n",
    "    pos_weight = pos_weight.to(device)\n",
    "    print(f\"Calculated pos_weight: {pos_weight}\")\n",
    "\n",
    "    # calculate weight for each class for random sampler\n",
    "    neg_wts = 1 / neg_samples\n",
    "    pos_wts = 1 / pos_samples\n",
    "    sample_wts = []\n",
    "\n",
    "    for label in train_dataset.labels:\n",
    "        if label == 0:\n",
    "            sample_wts.append(neg_wts)\n",
    "        else:\n",
    "            sample_wts.append(pos_wts)\n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_wts, num_samples=int(len(train_dataset) / 3), replacement=True\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=sampler,\n",
    "        batch_size=128,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train_dataloader,\n",
    "        valid_dataloader,\n",
    "        dataset_sizes,\n",
    "        bias_value,\n",
    "        pos_weight,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class SkinClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"resnet18\", freeze_backbone=False, num_tabular_features=41, bias_value=None):\n",
    "        super(SkinClassifier, self).__init__()\n",
    "\n",
    "        if model_name == \"efficientnet_v2_s\":\n",
    "            self.backbone = models.efficientnet_v2_s(weights=\"IMAGENET1K_V1\")\n",
    "            if freeze_backbone:\n",
    "                self.freeze_backbone()\n",
    "            num_ftrs = self.backbone.classifier[1].in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} not supported\")\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_tabular_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs + 256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        if bias_value is not None:\n",
    "            nn.init.constant_(self.classifier[-1].bias, bias_value)\n",
    "\n",
    "    def forward(self, image, tabular_features):\n",
    "        # Forward pass through the CNN backbone\n",
    "        cnn_out = self.backbone(image)\n",
    "\n",
    "        # Forward pass through the MLP for tabular data\n",
    "        mlp_out = self.mlp(tabular_features)\n",
    "        \n",
    "        # Concatenate CNN and MLP outputs\n",
    "        combined_out = torch.cat((cnn_out, mlp_out), dim=1)\n",
    "        \n",
    "        # Pass through the final classifier\n",
    "        return self.classifier(combined_out)\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.backbone.features[6].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for param in self.backbone.features[7].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def count_parameters(self):\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        non_trainable_params = sum(\n",
    "            p.numel() for p in self.parameters() if not p.requires_grad\n",
    "        )\n",
    "        return trainable_params, non_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation utils\n",
    "def train_model(\n",
    "    model,\n",
    "    dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_step,\n",
    "    dataset_sizes,\n",
    "    scheduler=None,\n",
    "    debug=False,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    preds = []\n",
    "    gts = []\n",
    "\n",
    "    for idx, (inputs, feats, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        feats = feats.to(device)\n",
    "        labels = labels.to(device).flatten().to(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs, feats).flatten()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_val = loss.detach()\n",
    "        running_loss += loss_val * inputs.size(0)\n",
    "\n",
    "        preds.extend(outputs)\n",
    "        gts.extend(labels)\n",
    "\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            train_step += 1\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train_loss\": loss_val,\n",
    "                    \"train_step\": train_step,\n",
    "                    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                }\n",
    "            )\n",
    "            print(f\"Train Batch Loss: {loss_val}\")\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        if debug:\n",
    "            break\n",
    "\n",
    "    epoch_loss = running_loss / dataset_sizes[\"train\"]\n",
    "    epoch_auroc = binary_auroc(\n",
    "        input=torch.tensor(preds).to(device), target=torch.tensor(gts).to(device)\n",
    "    ).item()\n",
    "\n",
    "    return model, epoch_loss, epoch_auroc\n",
    "\n",
    "\n",
    "def validate_model(\n",
    "    model, dataloader, criterion, optimizer, valid_step, dataset_sizes, debug=False\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    preds = []\n",
    "    gts = []\n",
    "\n",
    "    for idx, (inputs, feats, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        feats = feats.to(device)\n",
    "        labels = labels.to(device).flatten().to(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, feats).flatten()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        loss_val = loss.detach()\n",
    "        running_loss += loss_val * inputs.size(0)\n",
    "\n",
    "        preds.extend(outputs)\n",
    "        gts.extend(labels)\n",
    "\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            valid_step += 1\n",
    "            wandb.log({\"valid_loss\": loss_val, \"valid_step\": valid_step})\n",
    "            print(f\"valid Batch Loss: {loss_val}\")\n",
    "\n",
    "        if debug:\n",
    "            break\n",
    "\n",
    "    valid_loss = running_loss / dataset_sizes[\"val\"]\n",
    "    valid_auroc = binary_auroc(\n",
    "        input=torch.tensor(preds).to(device), target=torch.tensor(gts).to(device)\n",
    "    ).item()\n",
    "\n",
    "    return model, valid_loss, valid_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_folds(fold):\n",
    "    # Initialize wandb\n",
    "    run = wandb.init(\n",
    "        project=\"isic_lesions_24\", job_type=\"4_fold_nn\", name=f\"fold_{fold}\"\n",
    "    )\n",
    "    wandb.define_metric(\"train_step\")\n",
    "    wandb.define_metric(\"valid_step\")\n",
    "\n",
    "    model_name = \"efficientnet_v2_s\"\n",
    "    debug = False\n",
    "    epochs = 15 if not debug else 1\n",
    "\n",
    "    # Get data and stats\n",
    "    train_dataloader, valid_dataloader, dataset_sizes, bias_value, pos_weight = (\n",
    "        get_dataloaders_and_stats(fold)\n",
    "    )\n",
    "\n",
    "    # Create the model\n",
    "    model = SkinClassifier(\n",
    "        model_name=model_name, freeze_backbone=True, bias_value=bias_value\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    model = torch.compile(model)\n",
    "    print(model)\n",
    "\n",
    "    trainable_params, non_trainable_params = model.count_parameters()\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "\n",
    "    # Loss fn and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=0.001,\n",
    "        weight_decay=1e-5,\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=781 * 2, T_mult=2, eta_min=1e-6, last_epoch=-1\n",
    "    )\n",
    "\n",
    "    # train loop\n",
    "    train_step = 0\n",
    "    valid_step = 0\n",
    "    best_epoch_auroc = -np.inf\n",
    "    best_valid_loss = np.inf\n",
    "    early_stopping_patience = 4\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(\n",
    "        epochs\n",
    "    ):  # reducing epoch to 15 because quick overfitting after correct init\n",
    "        model, epoch_loss, epoch_train_auroc = train_model(\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            train_step,\n",
    "            dataset_sizes,\n",
    "            scheduler,\n",
    "            debug=debug,\n",
    "        )\n",
    "\n",
    "        model, valid_loss, epoch_valid_auroc = validate_model(\n",
    "            model,\n",
    "            valid_dataloader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            valid_step,\n",
    "            dataset_sizes,\n",
    "            debug=debug,\n",
    "        )\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"epoch_loss\": epoch_loss,\n",
    "                \"epoch_val_loss\": valid_loss,\n",
    "                \"epoch_train_auroc\": epoch_train_auroc,\n",
    "                \"epoch_valid_auroc\": epoch_valid_auroc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch: {epoch} | Train Loss: {epoch_loss} | Valid Loss: {valid_loss}\\n\")\n",
    "        print(\n",
    "            f\"Epoch: {epoch} | Train AUROC: {epoch_train_auroc} | Valid AUROC: {epoch_valid_auroc}\\n\"\n",
    "        )\n",
    "\n",
    "        # earlystopping dependent on validation loss\n",
    "        if best_valid_loss >= valid_loss:\n",
    "            print(\n",
    "                f\"{b_}Validation Loss Improved ({best_valid_loss} ---> {valid_loss}){sr_}\"\n",
    "            )\n",
    "\n",
    "            # checkpointing\n",
    "            PATH = f\"../models/{model_name}_{run.id}_valid_loss{valid_loss}_epoch{epoch}_fold{fold}.bin\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            print(f\"{b_}Model Saved{sr_}\")\n",
    "            best_valid_loss = valid_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "    # end training for this fold\n",
    "    # offload to cpu\n",
    "    model = model.to(\"cpu\")\n",
    "    del model\n",
    "    del train_dataloader\n",
    "    del valid_dataloader\n",
    "    gc.collect()\n",
    "\n",
    "    # finish wandb run\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wjhtfovh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2df5d95be745d086ee996c0d2db3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.054 MB of 0.054 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fold_0</strong> at: <a href='https://wandb.ai/ayush-thakur/isic_lesions_24/runs/wjhtfovh' target=\"_blank\">https://wandb.ai/ayush-thakur/isic_lesions_24/runs/wjhtfovh</a><br/> View project at: <a href='https://wandb.ai/ayush-thakur/isic_lesions_24' target=\"_blank\">https://wandb.ai/ayush-thakur/isic_lesions_24</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240816_025156-wjhtfovh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wjhtfovh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/ayusht/skin/notebooks/wandb/run-20240816_025524-fdcwbjee</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ayush-thakur/isic_lesions_24/runs/fdcwbjee' target=\"_blank\">fold_0</a></strong> to <a href='https://wandb.ai/ayush-thakur/isic_lesions_24' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ayush-thakur/isic_lesions_24' target=\"_blank\">https://wandb.ai/ayush-thakur/isic_lesions_24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ayush-thakur/isic_lesions_24/runs/fdcwbjee' target=\"_blank\">https://wandb.ai/ayush-thakur/isic_lesions_24/runs/fdcwbjee</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 290148, 'val': 91766}\n",
      "Calculated bias value: -6.883390138019604\n",
      "Calculated pos_weight: tensor([975.9293], device='cuda:0')\n",
      "OptimizedModule(\n",
      "  (_orig_mod): SkinClassifier(\n",
      "    (backbone): EfficientNet(\n",
      "      (features): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): FusedMBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "          )\n",
      "          (1): FusedMBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.005, mode=row)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): FusedMBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(24, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.01, mode=row)\n",
      "          )\n",
      "          (1): FusedMBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.015000000000000003, mode=row)\n",
      "          )\n",
      "          (2): FusedMBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.02, mode=row)\n",
      "          )\n",
      "          (3): FusedMBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "          )\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): FusedMBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.030000000000000006, mode=row)\n",
      "          )\n",
      "          (1): FusedMBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.035, mode=row)\n",
      "          )\n",
      "          (2): FusedMBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.04, mode=row)\n",
      "          )\n",
      "          (3): FusedMBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.045, mode=row)\n",
      "          )\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "          )\n",
      "          (1): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "                (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.05500000000000001, mode=row)\n",
      "          )\n",
      "          (2): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "                (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.06000000000000001, mode=row)\n",
      "          )\n",
      "          (3): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "                (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.065, mode=row)\n",
      "          )\n",
      "          (4): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "                (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.07, mode=row)\n",
      "          )\n",
      "          (5): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "                (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.075, mode=row)\n",
      "          )\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "                (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.08, mode=row)\n",
      "          )\n",
      "          (1): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.085, mode=row)\n",
      "          )\n",
      "          (2): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.09, mode=row)\n",
      "          )\n",
      "          (3): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.095, mode=row)\n",
      "          )\n",
      "          (4): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "          )\n",
      "          (5): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.10500000000000001, mode=row)\n",
      "          )\n",
      "          (6): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.11000000000000001, mode=row)\n",
      "          )\n",
      "          (7): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.11500000000000002, mode=row)\n",
      "          )\n",
      "          (8): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.12000000000000002, mode=row)\n",
      "          )\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=960, bias=False)\n",
      "                (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "          )\n",
      "          (1): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.13, mode=row)\n",
      "          )\n",
      "          (2): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.135, mode=row)\n",
      "          )\n",
      "          (3): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.14, mode=row)\n",
      "          )\n",
      "          (4): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.14500000000000002, mode=row)\n",
      "          )\n",
      "          (5): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.15, mode=row)\n",
      "          )\n",
      "          (6): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.155, mode=row)\n",
      "          )\n",
      "          (7): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.16, mode=row)\n",
      "          )\n",
      "          (8): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.165, mode=row)\n",
      "          )\n",
      "          (9): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.17, mode=row)\n",
      "          )\n",
      "          (10): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.175, mode=row)\n",
      "          )\n",
      "          (11): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.18, mode=row)\n",
      "          )\n",
      "          (12): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.185, mode=row)\n",
      "          )\n",
      "          (13): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.19, mode=row)\n",
      "          )\n",
      "          (14): MBConv(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2dNormActivation(\n",
      "                (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
      "                (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): SiLU(inplace=True)\n",
      "              )\n",
      "              (2): SqueezeExcitation(\n",
      "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "                (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (activation): SiLU(inplace=True)\n",
      "                (scale_activation): Sigmoid()\n",
      "              )\n",
      "              (3): Conv2dNormActivation(\n",
      "                (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.195, mode=row)\n",
      "          )\n",
      "        )\n",
      "        (7): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (classifier): Identity()\n",
      "    )\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=41, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.2, inplace=False)\n",
      "      (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.2, inplace=False)\n",
      "      (3): Linear(in_features=512, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Trainable parameters: 825857\n",
      "Non-trainable parameters: 20177488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:150: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch Loss: 314.7380065917969\n",
      "Train Batch Loss: 11.717415809631348\n",
      "Train Batch Loss: 18.08574867248535\n",
      "Train Batch Loss: 29.576379776000977\n",
      "Train Batch Loss: 26.49083709716797\n",
      "Train Batch Loss: 19.69489288330078\n",
      "Train Batch Loss: 24.176483154296875\n",
      "Train Batch Loss: 25.03244400024414\n",
      "Train Batch Loss: 22.138965606689453\n",
      "Train Batch Loss: 16.58848762512207\n",
      "Train Batch Loss: 17.642223358154297\n",
      "Train Batch Loss: 23.06472396850586\n",
      "Train Batch Loss: 17.123607635498047\n",
      "Train Batch Loss: 13.009292602539062\n",
      "Train Batch Loss: 15.282464981079102\n",
      "Train Batch Loss: 14.833134651184082\n",
      "Train Batch Loss: 12.460323333740234\n",
      "Train Batch Loss: 12.060583114624023\n",
      "Train Batch Loss: 10.933899879455566\n",
      "Train Batch Loss: 10.405517578125\n",
      "Train Batch Loss: 7.901212215423584\n",
      "Train Batch Loss: 7.223062038421631\n",
      "Train Batch Loss: 8.098145484924316\n",
      "Train Batch Loss: 6.41211462020874\n",
      "Train Batch Loss: 6.555633068084717\n",
      "Train Batch Loss: 5.717312812805176\n",
      "Train Batch Loss: 7.088094234466553\n",
      "Train Batch Loss: 5.783974647521973\n",
      "Train Batch Loss: 5.988069534301758\n",
      "Train Batch Loss: 8.05754280090332\n",
      "Train Batch Loss: 5.788995742797852\n",
      "Train Batch Loss: 4.800413131713867\n",
      "Train Batch Loss: 5.3396315574646\n",
      "Train Batch Loss: 5.229514122009277\n",
      "Train Batch Loss: 5.125535011291504\n",
      "Train Batch Loss: 5.182721138000488\n",
      "Train Batch Loss: 5.443742752075195\n",
      "Train Batch Loss: 4.4955339431762695\n",
      "Train Batch Loss: 4.837015151977539\n",
      "Train Batch Loss: 4.289941787719727\n",
      "Train Batch Loss: 4.970497131347656\n",
      "Train Batch Loss: 5.149144172668457\n",
      "Train Batch Loss: 5.705171585083008\n",
      "Train Batch Loss: 5.398159503936768\n",
      "Train Batch Loss: 4.606839179992676\n",
      "Train Batch Loss: 4.12237024307251\n",
      "Train Batch Loss: 4.390130996704102\n",
      "Train Batch Loss: 4.247798442840576\n",
      "Train Batch Loss: 4.205296039581299\n",
      "Train Batch Loss: 4.414340019226074\n",
      "Train Batch Loss: 4.58883810043335\n",
      "Train Batch Loss: 4.618663311004639\n",
      "Train Batch Loss: 4.682342529296875\n",
      "Train Batch Loss: 3.891425609588623\n",
      "Train Batch Loss: 4.011331081390381\n",
      "Train Batch Loss: 4.815316200256348\n",
      "Train Batch Loss: 4.622759819030762\n",
      "Train Batch Loss: 4.549775123596191\n",
      "Train Batch Loss: 4.5147504806518555\n",
      "Train Batch Loss: 3.9367480278015137\n",
      "Train Batch Loss: 4.04722785949707\n",
      "Train Batch Loss: 3.40911865234375\n",
      "Train Batch Loss: 4.055387496948242\n",
      "Train Batch Loss: 4.18974494934082\n",
      "Train Batch Loss: 4.302199363708496\n",
      "Train Batch Loss: 4.047906875610352\n",
      "Train Batch Loss: 7.01242733001709\n",
      "Train Batch Loss: 4.347322940826416\n",
      "Train Batch Loss: 3.872972011566162\n",
      "Train Batch Loss: 3.8902788162231445\n",
      "Train Batch Loss: 4.364540100097656\n",
      "Train Batch Loss: 4.700228691101074\n",
      "Train Batch Loss: 3.6900153160095215\n",
      "Train Batch Loss: 3.713012218475342\n",
      "Train Batch Loss: 3.989687442779541\n",
      "valid Batch Loss: 6.052285194396973\n",
      "valid Batch Loss: 8.762456893920898\n",
      "valid Batch Loss: 6.046929359436035\n",
      "valid Batch Loss: 1286.981689453125\n",
      "valid Batch Loss: 5.857936382293701\n",
      "valid Batch Loss: 1201.4764404296875\n",
      "valid Batch Loss: 5.750554084777832\n",
      "valid Batch Loss: 6.000786304473877\n",
      "valid Batch Loss: 5.989998817443848\n",
      "valid Batch Loss: 6.192706108093262\n",
      "valid Batch Loss: 5.845129013061523\n",
      "valid Batch Loss: 5.921628952026367\n",
      "valid Batch Loss: 6.065553188323975\n",
      "valid Batch Loss: 5.758467674255371\n",
      "valid Batch Loss: 5.9328203201293945\n",
      "valid Batch Loss: 5.748373031616211\n",
      "valid Batch Loss: 5.9444427490234375\n",
      "valid Batch Loss: 5.844729900360107\n",
      "valid Batch Loss: 325.1263122558594\n",
      "valid Batch Loss: 5.959528923034668\n",
      "valid Batch Loss: 6.221466541290283\n",
      "valid Batch Loss: 155.60939025878906\n",
      "valid Batch Loss: 5.794837951660156\n",
      "valid Batch Loss: 2209.00341796875\n",
      "valid Batch Loss: 269.52728271484375\n",
      "valid Batch Loss: 6.074228763580322\n",
      "valid Batch Loss: 5.898597717285156\n",
      "valid Batch Loss: 6.067074298858643\n",
      "valid Batch Loss: 14641.9658203125\n",
      "valid Batch Loss: 6.076457977294922\n",
      "valid Batch Loss: 5.688690185546875\n",
      "valid Batch Loss: 6.062142848968506\n",
      "valid Batch Loss: 334.68658447265625\n",
      "valid Batch Loss: 5.897479057312012\n",
      "valid Batch Loss: 9.23924446105957\n",
      "valid Batch Loss: 1998.17041015625\n",
      "valid Batch Loss: 5.619542598724365\n",
      "valid Batch Loss: 5.836651802062988\n",
      "valid Batch Loss: 5.856935501098633\n",
      "valid Batch Loss: 505.1673278808594\n",
      "valid Batch Loss: 634.582275390625\n",
      "valid Batch Loss: 6.222992897033691\n",
      "valid Batch Loss: 156.76040649414062\n",
      "valid Batch Loss: 6.052687644958496\n",
      "valid Batch Loss: 5.925732612609863\n",
      "valid Batch Loss: 5.999368667602539\n",
      "valid Batch Loss: 6.130527973175049\n",
      "valid Batch Loss: 6.128167629241943\n",
      "valid Batch Loss: 149.3810272216797\n",
      "valid Batch Loss: 5.993603706359863\n",
      "valid Batch Loss: 6.065932273864746\n",
      "valid Batch Loss: 6.272308349609375\n",
      "valid Batch Loss: 1998.42919921875\n",
      "valid Batch Loss: 5.922439098358154\n",
      "valid Batch Loss: 6.14466667175293\n",
      "valid Batch Loss: 6.228452682495117\n",
      "valid Batch Loss: 5.939606666564941\n",
      "valid Batch Loss: 515.688232421875\n",
      "valid Batch Loss: 6.007229804992676\n",
      "valid Batch Loss: 5.6828718185424805\n",
      "valid Batch Loss: 6.096890926361084\n",
      "valid Batch Loss: 6.008177757263184\n",
      "valid Batch Loss: 2490.275634765625\n",
      "valid Batch Loss: 13881.5263671875\n",
      "valid Batch Loss: 6.053603172302246\n",
      "valid Batch Loss: 5.774038314819336\n",
      "valid Batch Loss: 1470.579345703125\n",
      "valid Batch Loss: 5.898004531860352\n",
      "valid Batch Loss: 6.137786388397217\n",
      "valid Batch Loss: 29.393579483032227\n",
      "valid Batch Loss: 5.800732612609863\n",
      "Epoch: 0 | Train Loss: 10.01901626586914 | Valid Loss: 908.9411010742188\n",
      "\n",
      "Epoch: 0 | Train AUROC: 0.6051822054470632 | Valid AUROC: 0.8475947140270562\n",
      "\n",
      "\u001b[34mValidation Loss Improved (inf ---> 908.9411010742188)\u001b[0m\n",
      "\u001b[34mModel Saved\u001b[0m\n",
      "Train Batch Loss: 5.231145858764648\n",
      "Train Batch Loss: 3.607028007507324\n",
      "Train Batch Loss: 4.531512260437012\n",
      "Train Batch Loss: 3.6991000175476074\n",
      "Train Batch Loss: 3.682284355163574\n",
      "Train Batch Loss: 3.7717809677124023\n",
      "Train Batch Loss: 4.717092514038086\n",
      "Train Batch Loss: 3.6849026679992676\n",
      "Train Batch Loss: 3.5552971363067627\n",
      "Train Batch Loss: 4.368764400482178\n",
      "Train Batch Loss: 3.353536605834961\n",
      "Train Batch Loss: 4.296597480773926\n",
      "Train Batch Loss: 4.664370059967041\n",
      "Train Batch Loss: 3.731053352355957\n",
      "Train Batch Loss: 5.002322196960449\n",
      "Train Batch Loss: 3.877739191055298\n",
      "Train Batch Loss: 3.832022190093994\n",
      "Train Batch Loss: 4.03226375579834\n",
      "Train Batch Loss: 3.5943634510040283\n",
      "Train Batch Loss: 4.097170352935791\n",
      "Train Batch Loss: 4.338281631469727\n",
      "Train Batch Loss: 3.835679531097412\n",
      "Train Batch Loss: 3.635470390319824\n",
      "Train Batch Loss: 4.056514739990234\n",
      "Train Batch Loss: 3.7532215118408203\n",
      "Train Batch Loss: 3.7313332557678223\n",
      "Train Batch Loss: 3.7275376319885254\n",
      "Train Batch Loss: 3.8798861503601074\n",
      "Train Batch Loss: 3.326568603515625\n",
      "Train Batch Loss: 3.3222930431365967\n",
      "Train Batch Loss: 3.226522922515869\n",
      "Train Batch Loss: 3.5508310794830322\n",
      "Train Batch Loss: 3.203007459640503\n",
      "Train Batch Loss: 3.571742296218872\n",
      "Train Batch Loss: 4.210503578186035\n",
      "Train Batch Loss: 3.3078153133392334\n",
      "Train Batch Loss: 4.079291820526123\n",
      "Train Batch Loss: 3.5714354515075684\n",
      "Train Batch Loss: 3.605994701385498\n",
      "Train Batch Loss: 3.5232691764831543\n",
      "Train Batch Loss: 4.319675445556641\n",
      "Train Batch Loss: 3.58217453956604\n",
      "Train Batch Loss: 3.9107778072357178\n",
      "Train Batch Loss: 3.8144538402557373\n",
      "Train Batch Loss: 3.312730550765991\n",
      "Train Batch Loss: 3.791888475418091\n",
      "Train Batch Loss: 3.6367592811584473\n",
      "Train Batch Loss: 3.042919635772705\n",
      "Train Batch Loss: 3.7827212810516357\n",
      "Train Batch Loss: 4.090658187866211\n",
      "Train Batch Loss: 3.772153377532959\n",
      "Train Batch Loss: 3.3520734310150146\n",
      "Train Batch Loss: 3.957801342010498\n",
      "Train Batch Loss: 3.5468435287475586\n",
      "Train Batch Loss: 3.9771809577941895\n",
      "Train Batch Loss: 3.294445037841797\n",
      "Train Batch Loss: 4.149351596832275\n",
      "Train Batch Loss: 3.5339138507843018\n",
      "Train Batch Loss: 3.6605770587921143\n",
      "Train Batch Loss: 3.912226915359497\n",
      "Train Batch Loss: 3.9356164932250977\n",
      "Train Batch Loss: 3.8404901027679443\n",
      "Train Batch Loss: 3.8068039417266846\n",
      "Train Batch Loss: 3.476637125015259\n",
      "Train Batch Loss: 3.5423715114593506\n",
      "Train Batch Loss: 3.8130009174346924\n",
      "Train Batch Loss: 4.090275287628174\n",
      "Train Batch Loss: 3.814455032348633\n",
      "Train Batch Loss: 3.876858711242676\n",
      "Train Batch Loss: 3.61318302154541\n",
      "Train Batch Loss: 3.5091476440429688\n",
      "Train Batch Loss: 3.3395872116088867\n",
      "Train Batch Loss: 3.387291431427002\n",
      "Train Batch Loss: 3.514314651489258\n",
      "Train Batch Loss: 3.3831124305725098\n",
      "valid Batch Loss: 5.582870006561279\n",
      "valid Batch Loss: 5.5199055671691895\n",
      "valid Batch Loss: 5.456978797912598\n",
      "valid Batch Loss: 5.359007835388184\n",
      "valid Batch Loss: 5.2515764236450195\n",
      "valid Batch Loss: 5.207730293273926\n",
      "valid Batch Loss: 5.2402801513671875\n",
      "valid Batch Loss: 5.4092864990234375\n",
      "valid Batch Loss: 5.4121928215026855\n",
      "valid Batch Loss: 5.569483757019043\n",
      "valid Batch Loss: 5.304330825805664\n",
      "valid Batch Loss: 232.63766479492188\n",
      "valid Batch Loss: 5.469851016998291\n",
      "valid Batch Loss: 5.217404365539551\n",
      "valid Batch Loss: 5.39533805847168\n",
      "valid Batch Loss: 5.194714546203613\n",
      "valid Batch Loss: 5.352165222167969\n",
      "valid Batch Loss: 5.347322940826416\n",
      "valid Batch Loss: 5.724556922912598\n",
      "valid Batch Loss: 5.350364685058594\n",
      "valid Batch Loss: 5.685394287109375\n",
      "valid Batch Loss: 5.338733673095703\n",
      "valid Batch Loss: 5.215812683105469\n",
      "valid Batch Loss: 5.247086048126221\n",
      "valid Batch Loss: 5.457807540893555\n",
      "valid Batch Loss: 5.554887771606445\n",
      "valid Batch Loss: 5.413412094116211\n",
      "valid Batch Loss: 5.537223815917969\n",
      "valid Batch Loss: 5.502685546875\n",
      "valid Batch Loss: 5.520402908325195\n",
      "valid Batch Loss: 5.139944076538086\n",
      "valid Batch Loss: 5.5063982009887695\n",
      "valid Batch Loss: 5.603376388549805\n",
      "valid Batch Loss: 5.313699245452881\n",
      "valid Batch Loss: 5.3794732093811035\n",
      "valid Batch Loss: 5.560927867889404\n",
      "valid Batch Loss: 5.071829319000244\n",
      "valid Batch Loss: 5.304021835327148\n",
      "valid Batch Loss: 5.285355567932129\n",
      "valid Batch Loss: 605.279541015625\n",
      "valid Batch Loss: 5.4534125328063965\n",
      "valid Batch Loss: 5.638058185577393\n",
      "valid Batch Loss: 5.489195823669434\n",
      "valid Batch Loss: 5.4551472663879395\n",
      "valid Batch Loss: 5.3159284591674805\n",
      "valid Batch Loss: 5.407976150512695\n",
      "valid Batch Loss: 5.548062324523926\n",
      "valid Batch Loss: 5.55946159362793\n",
      "valid Batch Loss: 5.366806507110596\n",
      "valid Batch Loss: 5.414566993713379\n",
      "valid Batch Loss: 5.457912445068359\n",
      "valid Batch Loss: 5.780296325683594\n",
      "valid Batch Loss: 5.53099250793457\n",
      "valid Batch Loss: 5.313901424407959\n",
      "valid Batch Loss: 5.483076095581055\n",
      "valid Batch Loss: 5.661611557006836\n",
      "valid Batch Loss: 5.309505462646484\n",
      "valid Batch Loss: 214.26507568359375\n",
      "valid Batch Loss: 5.4625349044799805\n",
      "valid Batch Loss: 5.113276958465576\n",
      "valid Batch Loss: 5.5716657638549805\n",
      "valid Batch Loss: 5.452408790588379\n",
      "valid Batch Loss: 5.308666706085205\n",
      "valid Batch Loss: 5.483295440673828\n",
      "valid Batch Loss: 5.507153511047363\n",
      "valid Batch Loss: 5.175837993621826\n",
      "valid Batch Loss: 5.362845420837402\n",
      "valid Batch Loss: 5.265809059143066\n",
      "valid Batch Loss: 5.558499336242676\n",
      "valid Batch Loss: 5.825769424438477\n",
      "valid Batch Loss: 5.190530776977539\n",
      "Epoch: 1 | Train Loss: 1.2348830699920654 | Valid Loss: 16.956453323364258\n",
      "\n",
      "Epoch: 1 | Train AUROC: 0.8154462591196557 | Valid AUROC: 0.8777321403900231\n",
      "\n",
      "\u001b[34mValidation Loss Improved (908.9411010742188 ---> 16.956453323364258)\u001b[0m\n",
      "\u001b[34mModel Saved\u001b[0m\n",
      "Train Batch Loss: 3.6149377822875977\n",
      "Train Batch Loss: 4.396466255187988\n",
      "Train Batch Loss: 2.548149585723877\n",
      "Train Batch Loss: 3.701110363006592\n",
      "Train Batch Loss: 4.0060014724731445\n",
      "Train Batch Loss: 3.865910530090332\n",
      "Train Batch Loss: 2.991715908050537\n",
      "Train Batch Loss: 3.481330394744873\n",
      "Train Batch Loss: 4.576034069061279\n",
      "Train Batch Loss: 3.5016818046569824\n",
      "Train Batch Loss: 3.44138240814209\n",
      "Train Batch Loss: 3.744259834289551\n",
      "Train Batch Loss: 4.479126930236816\n",
      "Train Batch Loss: 3.143503189086914\n",
      "Train Batch Loss: 4.533753871917725\n",
      "Train Batch Loss: 3.6841297149658203\n",
      "Train Batch Loss: 3.335014820098877\n",
      "Train Batch Loss: 3.2597615718841553\n",
      "Train Batch Loss: 3.424819231033325\n",
      "Train Batch Loss: 3.6984307765960693\n",
      "Train Batch Loss: 3.050776720046997\n",
      "Train Batch Loss: 3.5503904819488525\n",
      "Train Batch Loss: 3.453064203262329\n",
      "Train Batch Loss: 3.2538979053497314\n",
      "Train Batch Loss: 3.8419766426086426\n",
      "Train Batch Loss: 3.747577667236328\n",
      "Train Batch Loss: 2.786287784576416\n",
      "Train Batch Loss: 3.347445487976074\n",
      "Train Batch Loss: 3.713533878326416\n",
      "Train Batch Loss: 3.060330390930176\n",
      "Train Batch Loss: 3.1464011669158936\n",
      "Train Batch Loss: 3.508917808532715\n",
      "Train Batch Loss: 2.706775665283203\n",
      "Train Batch Loss: 4.0657148361206055\n",
      "Train Batch Loss: 3.5759341716766357\n",
      "Train Batch Loss: 3.571993827819824\n",
      "Train Batch Loss: 4.006425380706787\n",
      "Train Batch Loss: 2.8583438396453857\n",
      "Train Batch Loss: 2.5821657180786133\n",
      "Train Batch Loss: 3.5145375728607178\n",
      "Train Batch Loss: 3.5771989822387695\n",
      "Train Batch Loss: 3.138972520828247\n",
      "Train Batch Loss: 2.990365982055664\n",
      "Train Batch Loss: 2.9646849632263184\n",
      "Train Batch Loss: 3.5759005546569824\n",
      "Train Batch Loss: 2.8705430030822754\n",
      "Train Batch Loss: 3.121973752975464\n",
      "Train Batch Loss: 3.194366931915283\n",
      "Train Batch Loss: 3.3974287509918213\n",
      "Train Batch Loss: 3.3741207122802734\n",
      "Train Batch Loss: 3.7991890907287598\n",
      "Train Batch Loss: 4.6773600578308105\n",
      "Train Batch Loss: 2.3656234741210938\n",
      "Train Batch Loss: 3.5982816219329834\n",
      "Train Batch Loss: 2.737410306930542\n",
      "Train Batch Loss: 3.470764636993408\n",
      "Train Batch Loss: 3.019059658050537\n",
      "Train Batch Loss: 2.7030701637268066\n",
      "Train Batch Loss: 3.132157564163208\n",
      "Train Batch Loss: 2.8797802925109863\n",
      "Train Batch Loss: 4.030880928039551\n",
      "Train Batch Loss: 2.7621984481811523\n",
      "Train Batch Loss: 3.2184860706329346\n",
      "Train Batch Loss: 3.4397659301757812\n",
      "Train Batch Loss: 2.7765862941741943\n",
      "Train Batch Loss: 2.8389053344726562\n",
      "Train Batch Loss: 2.5858941078186035\n",
      "Train Batch Loss: 2.9948878288269043\n",
      "Train Batch Loss: 3.569085121154785\n",
      "Train Batch Loss: 2.6160988807678223\n",
      "Train Batch Loss: 3.4709980487823486\n",
      "Train Batch Loss: 4.209360599517822\n",
      "Train Batch Loss: 2.5723705291748047\n",
      "Train Batch Loss: 3.1903676986694336\n",
      "Train Batch Loss: 2.8793234825134277\n",
      "valid Batch Loss: 4.701608657836914\n",
      "valid Batch Loss: 4.586952209472656\n",
      "valid Batch Loss: 4.3545355796813965\n",
      "valid Batch Loss: 4.339043617248535\n",
      "valid Batch Loss: 4.116154670715332\n",
      "valid Batch Loss: 3.9263038635253906\n",
      "valid Batch Loss: 4.242977142333984\n",
      "valid Batch Loss: 4.31320858001709\n",
      "valid Batch Loss: 4.329986572265625\n",
      "valid Batch Loss: 4.36915397644043\n",
      "valid Batch Loss: 4.171448707580566\n",
      "valid Batch Loss: 4.523995399475098\n",
      "valid Batch Loss: 4.324347496032715\n",
      "valid Batch Loss: 4.242153167724609\n",
      "valid Batch Loss: 4.425045967102051\n",
      "valid Batch Loss: 4.078132152557373\n",
      "valid Batch Loss: 4.171542167663574\n",
      "valid Batch Loss: 4.436363697052002\n",
      "valid Batch Loss: 4.477570533752441\n",
      "valid Batch Loss: 4.268399238586426\n",
      "valid Batch Loss: 4.820664405822754\n",
      "valid Batch Loss: 4.130270481109619\n",
      "valid Batch Loss: 3.861147165298462\n",
      "valid Batch Loss: 4.110392093658447\n",
      "valid Batch Loss: 4.420744895935059\n",
      "valid Batch Loss: 4.583860397338867\n",
      "valid Batch Loss: 4.499030113220215\n",
      "valid Batch Loss: 4.611340045928955\n",
      "valid Batch Loss: 4.409184455871582\n",
      "valid Batch Loss: 4.444063186645508\n",
      "valid Batch Loss: 4.0670366287231445\n",
      "valid Batch Loss: 4.366136074066162\n",
      "valid Batch Loss: 4.730798721313477\n",
      "valid Batch Loss: 4.393564224243164\n",
      "valid Batch Loss: 4.35175895690918\n",
      "valid Batch Loss: 4.681278705596924\n",
      "valid Batch Loss: 4.074518203735352\n",
      "valid Batch Loss: 4.188108444213867\n",
      "valid Batch Loss: 4.14501428604126\n",
      "valid Batch Loss: 538.8609619140625\n",
      "valid Batch Loss: 4.4326395988464355\n",
      "valid Batch Loss: 4.70318603515625\n",
      "valid Batch Loss: 4.492142677307129\n",
      "valid Batch Loss: 4.285506248474121\n",
      "valid Batch Loss: 4.221923828125\n",
      "valid Batch Loss: 4.425567626953125\n",
      "valid Batch Loss: 4.595357894897461\n",
      "valid Batch Loss: 4.502965450286865\n",
      "valid Batch Loss: 4.494529724121094\n",
      "valid Batch Loss: 4.428815841674805\n",
      "valid Batch Loss: 4.412187576293945\n",
      "valid Batch Loss: 4.766953468322754\n",
      "valid Batch Loss: 4.5914154052734375\n",
      "valid Batch Loss: 4.197654724121094\n",
      "valid Batch Loss: 4.474660396575928\n",
      "valid Batch Loss: 4.626965045928955\n",
      "valid Batch Loss: 4.0976033210754395\n",
      "valid Batch Loss: 5.063825607299805\n",
      "valid Batch Loss: 4.40836763381958\n",
      "valid Batch Loss: 3.932166814804077\n",
      "valid Batch Loss: 4.635626316070557\n",
      "valid Batch Loss: 4.424150466918945\n",
      "valid Batch Loss: 3.969099521636963\n",
      "valid Batch Loss: 4.5857954025268555\n",
      "valid Batch Loss: 4.518219947814941\n",
      "valid Batch Loss: 3.975808620452881\n",
      "valid Batch Loss: 4.304513931274414\n",
      "valid Batch Loss: 4.236457824707031\n",
      "valid Batch Loss: 4.606531143188477\n",
      "valid Batch Loss: 4.8231048583984375\n",
      "valid Batch Loss: 4.040482521057129\n",
      "Epoch: 2 | Train Loss: 1.1005799770355225 | Valid Loss: 9.097126960754395\n",
      "\n",
      "Epoch: 2 | Train AUROC: 0.8748668502921975 | Valid AUROC: 0.8973274340603805\n",
      "\n",
      "\u001b[34mValidation Loss Improved (16.956453323364258 ---> 9.097126960754395)\u001b[0m\n",
      "\u001b[34mModel Saved\u001b[0m\n",
      "Train Batch Loss: 2.88861083984375\n",
      "Train Batch Loss: 2.5237088203430176\n",
      "Train Batch Loss: 2.60636043548584\n",
      "Train Batch Loss: 3.3670713901519775\n",
      "Train Batch Loss: 2.7018308639526367\n",
      "Train Batch Loss: 2.937378406524658\n",
      "Train Batch Loss: 2.177177906036377\n",
      "Train Batch Loss: 3.12803053855896\n",
      "Train Batch Loss: 3.0186643600463867\n",
      "Train Batch Loss: 2.295502185821533\n",
      "Train Batch Loss: 2.968531608581543\n",
      "Train Batch Loss: 3.1576852798461914\n",
      "Train Batch Loss: 2.2504589557647705\n",
      "Train Batch Loss: 3.2033379077911377\n",
      "Train Batch Loss: 3.132646083831787\n",
      "Train Batch Loss: 2.743849277496338\n",
      "Train Batch Loss: 2.834103584289551\n",
      "Train Batch Loss: 2.3599774837493896\n",
      "Train Batch Loss: 2.763824462890625\n",
      "Train Batch Loss: 3.190650463104248\n",
      "Train Batch Loss: 2.68119740486145\n",
      "Train Batch Loss: 3.1493887901306152\n",
      "Train Batch Loss: 2.436918258666992\n",
      "Train Batch Loss: 2.4389595985412598\n",
      "Train Batch Loss: 3.1567115783691406\n",
      "Train Batch Loss: 2.629192352294922\n",
      "Train Batch Loss: 3.4238638877868652\n",
      "Train Batch Loss: 2.9192821979522705\n",
      "Train Batch Loss: 2.7992396354675293\n",
      "Train Batch Loss: 2.6980059146881104\n",
      "Train Batch Loss: 3.0753064155578613\n",
      "Train Batch Loss: 3.0380706787109375\n",
      "Train Batch Loss: 2.9880027770996094\n",
      "Train Batch Loss: 3.53129243850708\n",
      "Train Batch Loss: 2.6216769218444824\n",
      "Train Batch Loss: 2.5110549926757812\n",
      "Train Batch Loss: 2.6362860202789307\n",
      "Train Batch Loss: 2.5602283477783203\n",
      "Train Batch Loss: 2.4964966773986816\n",
      "Train Batch Loss: 2.3964428901672363\n",
      "Train Batch Loss: 2.6110000610351562\n",
      "Train Batch Loss: 2.7103233337402344\n",
      "Train Batch Loss: 2.562845230102539\n",
      "Train Batch Loss: 3.5368762016296387\n",
      "Train Batch Loss: 2.8387279510498047\n",
      "Train Batch Loss: 2.362325429916382\n",
      "Train Batch Loss: 2.5014419555664062\n",
      "Train Batch Loss: 2.4938173294067383\n",
      "Train Batch Loss: 2.653055191040039\n",
      "Train Batch Loss: 3.062866687774658\n",
      "Train Batch Loss: 3.1260714530944824\n",
      "Train Batch Loss: 2.426668167114258\n",
      "Train Batch Loss: 2.284968852996826\n",
      "Train Batch Loss: 2.702629566192627\n",
      "Train Batch Loss: 2.6450304985046387\n",
      "Train Batch Loss: 2.6902546882629395\n",
      "Train Batch Loss: 2.4792656898498535\n",
      "Train Batch Loss: 2.602360486984253\n",
      "Train Batch Loss: 2.516005039215088\n",
      "Train Batch Loss: 2.388260841369629\n",
      "Train Batch Loss: 3.0552945137023926\n",
      "Train Batch Loss: 3.6297237873077393\n",
      "Train Batch Loss: 2.7720348834991455\n",
      "Train Batch Loss: 2.756763458251953\n",
      "Train Batch Loss: 3.0230441093444824\n",
      "Train Batch Loss: 2.577216386795044\n",
      "Train Batch Loss: 2.206538677215576\n",
      "Train Batch Loss: 2.4024600982666016\n",
      "Train Batch Loss: 2.7202436923980713\n",
      "Train Batch Loss: 2.782712936401367\n",
      "Train Batch Loss: 2.572633743286133\n",
      "Train Batch Loss: 2.26545786857605\n",
      "Train Batch Loss: 2.447314739227295\n",
      "Train Batch Loss: 2.6634159088134766\n",
      "Train Batch Loss: 2.7175352573394775\n",
      "valid Batch Loss: 4.0903849601745605\n",
      "valid Batch Loss: 4.008596420288086\n",
      "valid Batch Loss: 3.8075475692749023\n",
      "valid Batch Loss: 3.671617269515991\n",
      "valid Batch Loss: 3.5094192028045654\n",
      "valid Batch Loss: 3.0407400131225586\n",
      "valid Batch Loss: 3.6149697303771973\n",
      "valid Batch Loss: 3.6452016830444336\n",
      "valid Batch Loss: 3.776553153991699\n",
      "valid Batch Loss: 3.801668882369995\n",
      "valid Batch Loss: 3.591250419616699\n",
      "valid Batch Loss: 3.9534988403320312\n",
      "valid Batch Loss: 3.7661123275756836\n",
      "valid Batch Loss: 3.653095245361328\n",
      "valid Batch Loss: 3.969326972961426\n",
      "valid Batch Loss: 3.4063522815704346\n",
      "valid Batch Loss: 3.4060537815093994\n",
      "valid Batch Loss: 3.8652329444885254\n",
      "valid Batch Loss: 3.8609251976013184\n",
      "valid Batch Loss: 3.6801233291625977\n",
      "valid Batch Loss: 4.429300308227539\n",
      "valid Batch Loss: 3.5689048767089844\n",
      "valid Batch Loss: 3.153989791870117\n",
      "valid Batch Loss: 3.53908109664917\n",
      "valid Batch Loss: 3.925797462463379\n",
      "valid Batch Loss: 3.9491732120513916\n",
      "valid Batch Loss: 3.961557388305664\n",
      "valid Batch Loss: 3.9229512214660645\n",
      "valid Batch Loss: 3.8935129642486572\n",
      "valid Batch Loss: 3.705084800720215\n",
      "valid Batch Loss: 3.317869186401367\n",
      "valid Batch Loss: 3.7793467044830322\n",
      "valid Batch Loss: 4.383658409118652\n",
      "valid Batch Loss: 3.8795313835144043\n",
      "valid Batch Loss: 3.6171486377716064\n",
      "valid Batch Loss: 4.070446968078613\n",
      "valid Batch Loss: 3.4823241233825684\n",
      "valid Batch Loss: 3.590636730194092\n",
      "valid Batch Loss: 3.483384132385254\n",
      "valid Batch Loss: 315.8963928222656\n",
      "valid Batch Loss: 3.885056495666504\n",
      "valid Batch Loss: 4.214127063751221\n",
      "valid Batch Loss: 3.9012906551361084\n",
      "valid Batch Loss: 3.6413557529449463\n",
      "valid Batch Loss: 5.116346836090088\n",
      "valid Batch Loss: 3.945160388946533\n",
      "valid Batch Loss: 4.185263156890869\n",
      "valid Batch Loss: 4.045660018920898\n",
      "valid Batch Loss: 4.017127513885498\n",
      "valid Batch Loss: 3.9550538063049316\n",
      "valid Batch Loss: 3.8668413162231445\n",
      "valid Batch Loss: 4.314426422119141\n",
      "valid Batch Loss: 4.064465522766113\n",
      "valid Batch Loss: 3.6067566871643066\n",
      "valid Batch Loss: 4.019112586975098\n",
      "valid Batch Loss: 4.0546875\n",
      "valid Batch Loss: 3.428356647491455\n",
      "valid Batch Loss: 3.951221466064453\n",
      "valid Batch Loss: 3.783074378967285\n",
      "valid Batch Loss: 3.356501579284668\n",
      "valid Batch Loss: 4.110108375549316\n",
      "valid Batch Loss: 3.9249706268310547\n",
      "valid Batch Loss: 3.3175125122070312\n",
      "valid Batch Loss: 3.9868531227111816\n",
      "valid Batch Loss: 3.9420371055603027\n",
      "valid Batch Loss: 3.3268635272979736\n",
      "valid Batch Loss: 3.7062089443206787\n",
      "valid Batch Loss: 3.6352877616882324\n",
      "valid Batch Loss: 4.092425346374512\n",
      "valid Batch Loss: 4.260848045349121\n",
      "valid Batch Loss: 3.450538158416748\n",
      "Epoch: 3 | Train Loss: 0.9175333380699158 | Valid Loss: 6.480223655700684\n",
      "\n",
      "Epoch: 3 | Train AUROC: 0.9228979292031575 | Valid AUROC: 0.9169097428897608\n",
      "\n",
      "\u001b[34mValidation Loss Improved (9.097126960754395 ---> 6.480223655700684)\u001b[0m\n",
      "\u001b[34mModel Saved\u001b[0m\n",
      "Train Batch Loss: 2.482151746749878\n",
      "Train Batch Loss: 2.490640163421631\n",
      "Train Batch Loss: 2.431950807571411\n",
      "Train Batch Loss: 2.0697202682495117\n",
      "Train Batch Loss: 2.2721197605133057\n",
      "Train Batch Loss: 2.240253448486328\n",
      "Train Batch Loss: 2.4119796752929688\n",
      "Train Batch Loss: 2.6311416625976562\n",
      "Train Batch Loss: 2.86301851272583\n",
      "Train Batch Loss: 2.44004487991333\n",
      "Train Batch Loss: 2.3126235008239746\n",
      "Train Batch Loss: 2.2777843475341797\n",
      "Train Batch Loss: 2.036531925201416\n",
      "Train Batch Loss: 2.7639858722686768\n",
      "Train Batch Loss: 2.347529411315918\n",
      "Train Batch Loss: 2.753816604614258\n",
      "Train Batch Loss: 2.2731027603149414\n",
      "Train Batch Loss: 2.2331366539001465\n",
      "Train Batch Loss: 2.0669355392456055\n",
      "Train Batch Loss: 1.9551690816879272\n",
      "Train Batch Loss: 2.396432399749756\n",
      "Train Batch Loss: 2.2161293029785156\n",
      "Train Batch Loss: 2.2929751873016357\n",
      "Train Batch Loss: 4.777671813964844\n",
      "Train Batch Loss: 2.7539567947387695\n",
      "Train Batch Loss: 2.2884016036987305\n",
      "Train Batch Loss: 2.155160427093506\n",
      "Train Batch Loss: 2.485166311264038\n",
      "Train Batch Loss: 2.4379825592041016\n",
      "Train Batch Loss: 1.853841781616211\n",
      "Train Batch Loss: 2.047402858734131\n",
      "Train Batch Loss: 2.333585262298584\n",
      "Train Batch Loss: 2.966749906539917\n",
      "Train Batch Loss: 3.1185970306396484\n",
      "Train Batch Loss: 1.821091651916504\n",
      "Train Batch Loss: 2.4356255531311035\n",
      "Train Batch Loss: 2.093710422515869\n",
      "Train Batch Loss: 2.20021915435791\n",
      "Train Batch Loss: 1.4808647632598877\n",
      "Train Batch Loss: 1.5023471117019653\n",
      "Train Batch Loss: 2.9461727142333984\n",
      "Train Batch Loss: 2.523280620574951\n",
      "Train Batch Loss: 2.2369601726531982\n",
      "Train Batch Loss: 2.305211067199707\n",
      "Train Batch Loss: 2.2546744346618652\n",
      "Train Batch Loss: 3.1626133918762207\n",
      "Train Batch Loss: 1.8051722049713135\n",
      "Train Batch Loss: 2.079646587371826\n",
      "Train Batch Loss: 2.573726177215576\n",
      "Train Batch Loss: 1.9828193187713623\n",
      "Train Batch Loss: 2.371199369430542\n",
      "Train Batch Loss: 1.983929991722107\n",
      "Train Batch Loss: 2.4123575687408447\n",
      "Train Batch Loss: 2.8008809089660645\n",
      "Train Batch Loss: 1.962432622909546\n",
      "Train Batch Loss: 1.760502576828003\n",
      "Train Batch Loss: 2.174619674682617\n",
      "Train Batch Loss: 2.4773311614990234\n",
      "Train Batch Loss: 2.487760543823242\n",
      "Train Batch Loss: 2.3971309661865234\n",
      "Train Batch Loss: 3.1085457801818848\n",
      "Train Batch Loss: 2.1044154167175293\n",
      "Train Batch Loss: 2.825712203979492\n",
      "Train Batch Loss: 3.7713472843170166\n",
      "Train Batch Loss: 2.073495388031006\n",
      "Train Batch Loss: 2.10221529006958\n",
      "Train Batch Loss: 2.0598387718200684\n",
      "Train Batch Loss: 2.531189441680908\n",
      "Train Batch Loss: 2.6917636394500732\n",
      "Train Batch Loss: 2.689352512359619\n",
      "Train Batch Loss: 2.6008896827697754\n",
      "Train Batch Loss: 1.724203109741211\n",
      "Train Batch Loss: 2.237839698791504\n",
      "Train Batch Loss: 1.8866722583770752\n",
      "Train Batch Loss: 1.7082256078720093\n",
      "valid Batch Loss: 3.4818508625030518\n",
      "valid Batch Loss: 3.3559107780456543\n",
      "valid Batch Loss: 3.1644444465637207\n",
      "valid Batch Loss: 3.0819787979125977\n",
      "valid Batch Loss: 2.8091533184051514\n",
      "valid Batch Loss: 2.521040201187134\n",
      "valid Batch Loss: 2.979614734649658\n",
      "valid Batch Loss: 2.897712230682373\n",
      "valid Batch Loss: 3.322463274002075\n",
      "valid Batch Loss: 3.155771493911743\n",
      "valid Batch Loss: 2.9391064643859863\n",
      "valid Batch Loss: 3.336454153060913\n",
      "valid Batch Loss: 2.9897353649139404\n",
      "valid Batch Loss: 2.99676513671875\n",
      "valid Batch Loss: 3.377509832382202\n",
      "valid Batch Loss: 2.806424617767334\n",
      "valid Batch Loss: 2.8529868125915527\n",
      "valid Batch Loss: 3.2214722633361816\n",
      "valid Batch Loss: 3.140570640563965\n",
      "valid Batch Loss: 3.027337074279785\n",
      "valid Batch Loss: 3.690030813217163\n",
      "valid Batch Loss: 3.009934902191162\n",
      "valid Batch Loss: 2.580263137817383\n",
      "valid Batch Loss: 2.9417548179626465\n",
      "valid Batch Loss: 3.2663068771362305\n",
      "valid Batch Loss: 3.3157968521118164\n",
      "valid Batch Loss: 3.4425058364868164\n",
      "valid Batch Loss: 3.2573189735412598\n",
      "valid Batch Loss: 3.3086161613464355\n",
      "valid Batch Loss: 3.113528251647949\n",
      "valid Batch Loss: 2.82631778717041\n",
      "valid Batch Loss: 3.097034454345703\n",
      "valid Batch Loss: 3.7771708965301514\n",
      "valid Batch Loss: 3.252819538116455\n",
      "valid Batch Loss: 2.9847311973571777\n",
      "valid Batch Loss: 3.4342691898345947\n",
      "valid Batch Loss: 2.9329848289489746\n",
      "valid Batch Loss: 2.889530658721924\n",
      "valid Batch Loss: 2.877842426300049\n",
      "valid Batch Loss: 506.74420166015625\n",
      "valid Batch Loss: 3.125501871109009\n",
      "valid Batch Loss: 3.6434364318847656\n",
      "valid Batch Loss: 3.2858171463012695\n",
      "valid Batch Loss: 3.028000831604004\n",
      "valid Batch Loss: 2.950327157974243\n",
      "valid Batch Loss: 3.247008800506592\n",
      "valid Batch Loss: 3.5294456481933594\n",
      "valid Batch Loss: 3.3783318996429443\n",
      "valid Batch Loss: 3.346957206726074\n",
      "valid Batch Loss: 3.368664026260376\n",
      "valid Batch Loss: 3.2933669090270996\n",
      "valid Batch Loss: 3.6009788513183594\n",
      "valid Batch Loss: 3.4543447494506836\n",
      "valid Batch Loss: 2.9924488067626953\n",
      "valid Batch Loss: 3.3451449871063232\n",
      "valid Batch Loss: 3.3204238414764404\n",
      "valid Batch Loss: 2.9010775089263916\n",
      "valid Batch Loss: 3.361489772796631\n",
      "valid Batch Loss: 3.18392276763916\n",
      "valid Batch Loss: 2.6483535766601562\n",
      "valid Batch Loss: 3.5410351753234863\n",
      "valid Batch Loss: 3.2985777854919434\n",
      "valid Batch Loss: 2.694000482559204\n",
      "valid Batch Loss: 3.349231004714966\n",
      "valid Batch Loss: 3.241783618927002\n",
      "valid Batch Loss: 2.727123975753784\n",
      "valid Batch Loss: 3.086568593978882\n",
      "valid Batch Loss: 3.0526235103607178\n",
      "valid Batch Loss: 3.438396692276001\n",
      "valid Batch Loss: 3.572768449783325\n",
      "valid Batch Loss: 2.8862736225128174\n",
      "Epoch: 4 | Train Loss: 0.7947489619255066 | Valid Loss: 5.206560134887695\n",
      "\n",
      "Epoch: 4 | Train AUROC: 0.9404653667834623 | Valid AUROC: 0.9113183405165474\n",
      "\n",
      "\u001b[34mValidation Loss Improved (6.480223655700684 ---> 5.206560134887695)\u001b[0m\n",
      "\u001b[34mModel Saved\u001b[0m\n",
      "Train Batch Loss: 2.8278884887695312\n",
      "Train Batch Loss: 2.5818421840667725\n",
      "Train Batch Loss: 2.31704044342041\n",
      "Train Batch Loss: 1.9995356798171997\n",
      "Train Batch Loss: 1.7829780578613281\n",
      "Train Batch Loss: 2.278590440750122\n",
      "Train Batch Loss: 2.149766206741333\n",
      "Train Batch Loss: 2.408125638961792\n",
      "Train Batch Loss: 2.5110726356506348\n",
      "Train Batch Loss: 1.627685785293579\n",
      "Train Batch Loss: 2.2999353408813477\n",
      "Train Batch Loss: 2.4058685302734375\n",
      "Train Batch Loss: 1.7705875635147095\n",
      "Train Batch Loss: 1.8335124254226685\n",
      "Train Batch Loss: 1.7865833044052124\n",
      "Train Batch Loss: 2.7351391315460205\n",
      "Train Batch Loss: 2.5878841876983643\n",
      "Train Batch Loss: 1.9504456520080566\n",
      "Train Batch Loss: 2.349544048309326\n",
      "Train Batch Loss: 1.829160451889038\n",
      "Train Batch Loss: 2.914006233215332\n",
      "Train Batch Loss: 2.15455961227417\n",
      "Train Batch Loss: 1.8528801202774048\n",
      "Train Batch Loss: 2.369136333465576\n",
      "Train Batch Loss: 2.425440549850464\n",
      "Train Batch Loss: 2.3593053817749023\n",
      "Train Batch Loss: 2.425203323364258\n",
      "Train Batch Loss: 1.97335946559906\n",
      "Train Batch Loss: 1.8516473770141602\n",
      "Train Batch Loss: 1.7700085639953613\n",
      "Train Batch Loss: 2.8456106185913086\n",
      "Train Batch Loss: 2.0159947872161865\n",
      "Train Batch Loss: 2.7947545051574707\n",
      "Train Batch Loss: 2.2721121311187744\n",
      "Train Batch Loss: 2.5732204914093018\n",
      "Train Batch Loss: 2.0528554916381836\n",
      "Train Batch Loss: 2.478626012802124\n",
      "Train Batch Loss: 2.6407723426818848\n",
      "Train Batch Loss: 1.7086446285247803\n",
      "Train Batch Loss: 2.966573476791382\n",
      "Train Batch Loss: 2.625920534133911\n",
      "Train Batch Loss: 2.131997585296631\n",
      "Train Batch Loss: 2.3765745162963867\n",
      "Train Batch Loss: 1.733772873878479\n",
      "Train Batch Loss: 1.752278447151184\n",
      "Train Batch Loss: 1.891018033027649\n",
      "Train Batch Loss: 3.183619499206543\n",
      "Train Batch Loss: 2.5722784996032715\n",
      "Train Batch Loss: 2.2045083045959473\n",
      "Train Batch Loss: 2.9744608402252197\n",
      "Train Batch Loss: 2.268298625946045\n",
      "Train Batch Loss: 3.426985263824463\n",
      "Train Batch Loss: 1.879746913909912\n",
      "Train Batch Loss: 1.7891795635223389\n",
      "Train Batch Loss: 2.8094961643218994\n",
      "Train Batch Loss: 2.2078356742858887\n",
      "Train Batch Loss: 2.694699764251709\n",
      "Train Batch Loss: 2.488115072250366\n",
      "Train Batch Loss: 2.349114418029785\n",
      "Train Batch Loss: 3.5649659633636475\n",
      "Train Batch Loss: 2.261157751083374\n",
      "Train Batch Loss: 1.8945996761322021\n",
      "Train Batch Loss: 2.128060817718506\n",
      "Train Batch Loss: 2.586366653442383\n",
      "Train Batch Loss: 2.3196725845336914\n",
      "Train Batch Loss: 2.3747236728668213\n",
      "Train Batch Loss: 1.97647225856781\n",
      "Train Batch Loss: 2.1866044998168945\n",
      "Train Batch Loss: 1.342961072921753\n",
      "Train Batch Loss: 1.9726073741912842\n",
      "Train Batch Loss: 2.3810391426086426\n",
      "Train Batch Loss: 2.31410813331604\n",
      "Train Batch Loss: 2.9663538932800293\n",
      "Train Batch Loss: 1.649292230606079\n",
      "Train Batch Loss: 2.4837377071380615\n",
      "valid Batch Loss: 3.7707133293151855\n",
      "valid Batch Loss: 3.6985840797424316\n",
      "valid Batch Loss: 3.485684871673584\n",
      "valid Batch Loss: 3.3427927494049072\n",
      "valid Batch Loss: 3.152313232421875\n",
      "valid Batch Loss: 2.8309783935546875\n",
      "valid Batch Loss: 3.341259002685547\n",
      "valid Batch Loss: 3.2222042083740234\n",
      "valid Batch Loss: 3.635873556137085\n",
      "valid Batch Loss: 3.463062286376953\n",
      "valid Batch Loss: 3.290226459503174\n",
      "valid Batch Loss: 162.090087890625\n",
      "valid Batch Loss: 3.299862861633301\n",
      "valid Batch Loss: 3.334406852722168\n",
      "valid Batch Loss: 3.718295097351074\n",
      "valid Batch Loss: 3.1347427368164062\n",
      "valid Batch Loss: 3.1262054443359375\n",
      "valid Batch Loss: 3.548847198486328\n",
      "valid Batch Loss: 3.461512565612793\n",
      "valid Batch Loss: 3.3418869972229004\n",
      "valid Batch Loss: 4.004410743713379\n",
      "valid Batch Loss: 3.2958457469940186\n",
      "valid Batch Loss: 2.8907718658447266\n",
      "valid Batch Loss: 3.2581591606140137\n",
      "valid Batch Loss: 3.585455894470215\n",
      "valid Batch Loss: 3.664564609527588\n",
      "valid Batch Loss: 3.724850654602051\n",
      "valid Batch Loss: 3.548163890838623\n",
      "valid Batch Loss: 3.5817978382110596\n",
      "valid Batch Loss: 3.3895955085754395\n",
      "valid Batch Loss: 3.1277594566345215\n",
      "valid Batch Loss: 3.3975026607513428\n",
      "valid Batch Loss: 4.113738059997559\n",
      "valid Batch Loss: 3.58516788482666\n",
      "valid Batch Loss: 3.255209445953369\n",
      "valid Batch Loss: 3.74900221824646\n",
      "valid Batch Loss: 3.234978675842285\n",
      "valid Batch Loss: 3.1786048412323\n",
      "valid Batch Loss: 3.1722605228424072\n",
      "valid Batch Loss: 676.8131713867188\n",
      "valid Batch Loss: 88.55430603027344\n",
      "valid Batch Loss: 3.9822590351104736\n",
      "valid Batch Loss: 3.606729507446289\n",
      "valid Batch Loss: 3.3310515880584717\n",
      "valid Batch Loss: 3.628136157989502\n",
      "valid Batch Loss: 3.6089558601379395\n",
      "valid Batch Loss: 3.854341506958008\n",
      "valid Batch Loss: 3.686988115310669\n",
      "valid Batch Loss: 3.6490838527679443\n",
      "valid Batch Loss: 3.664750099182129\n",
      "valid Batch Loss: 3.6229705810546875\n",
      "valid Batch Loss: 3.9150447845458984\n",
      "valid Batch Loss: 3.815199136734009\n",
      "valid Batch Loss: 3.250558376312256\n",
      "valid Batch Loss: 3.617114543914795\n",
      "valid Batch Loss: 3.6548805236816406\n",
      "valid Batch Loss: 3.2011234760284424\n",
      "valid Batch Loss: 3.636756658554077\n",
      "valid Batch Loss: 3.4696176052093506\n",
      "valid Batch Loss: 2.9472713470458984\n",
      "valid Batch Loss: 3.8300578594207764\n",
      "valid Batch Loss: 3.615264415740967\n",
      "valid Batch Loss: 4.113028526306152\n",
      "valid Batch Loss: 3.6637301445007324\n",
      "valid Batch Loss: 3.5828680992126465\n",
      "valid Batch Loss: 3.0340700149536133\n",
      "valid Batch Loss: 3.380331039428711\n",
      "valid Batch Loss: 3.345834255218506\n",
      "valid Batch Loss: 3.756437063217163\n",
      "valid Batch Loss: 308.84893798828125\n",
      "valid Batch Loss: 3.1827054023742676\n",
      "Epoch: 5 | Train Loss: 0.7488159537315369 | Valid Loss: 24.107667922973633\n",
      "\n",
      "Epoch: 5 | Train AUROC: 0.9452037010396455 | Valid AUROC: 0.912606177044671\n",
      "\n",
      "Train Batch Loss: 1.4605674743652344\n",
      "Train Batch Loss: 2.0854508876800537\n",
      "Train Batch Loss: 1.7992229461669922\n",
      "Train Batch Loss: 2.1126842498779297\n",
      "Train Batch Loss: 1.9924981594085693\n",
      "Train Batch Loss: 1.8060623407363892\n",
      "Train Batch Loss: 2.1427721977233887\n",
      "Train Batch Loss: 2.1930885314941406\n",
      "Train Batch Loss: 1.8038898706436157\n",
      "Train Batch Loss: 2.3815181255340576\n",
      "Train Batch Loss: 1.8440701961517334\n",
      "Train Batch Loss: 2.093651294708252\n",
      "Train Batch Loss: 2.8273441791534424\n",
      "Train Batch Loss: 2.2653379440307617\n",
      "Train Batch Loss: 2.234577178955078\n",
      "Train Batch Loss: 2.2487564086914062\n",
      "Train Batch Loss: 2.6182966232299805\n",
      "Train Batch Loss: 2.2010011672973633\n",
      "Train Batch Loss: 2.366980791091919\n",
      "Train Batch Loss: 2.2450098991394043\n",
      "Train Batch Loss: 2.535466194152832\n",
      "Train Batch Loss: 2.9222335815429688\n",
      "Train Batch Loss: 2.1963815689086914\n",
      "Train Batch Loss: 3.405545949935913\n",
      "Train Batch Loss: 1.890519380569458\n",
      "Train Batch Loss: 3.001976490020752\n",
      "Train Batch Loss: 3.698099136352539\n",
      "Train Batch Loss: 2.544909954071045\n",
      "Train Batch Loss: 2.4465959072113037\n",
      "Train Batch Loss: 2.8042869567871094\n",
      "Train Batch Loss: 2.297654867172241\n",
      "Train Batch Loss: 2.049266815185547\n",
      "Train Batch Loss: 2.6559529304504395\n",
      "Train Batch Loss: 3.070056438446045\n",
      "Train Batch Loss: 2.568955659866333\n",
      "Train Batch Loss: 3.2092268466949463\n",
      "Train Batch Loss: 1.9105076789855957\n",
      "Train Batch Loss: 2.9527134895324707\n",
      "Train Batch Loss: 1.901242733001709\n",
      "Train Batch Loss: 2.7138309478759766\n",
      "Train Batch Loss: 2.6553993225097656\n",
      "Train Batch Loss: 2.0926427841186523\n",
      "Train Batch Loss: 1.785456657409668\n",
      "Train Batch Loss: 2.354168653488159\n",
      "Train Batch Loss: 2.155045509338379\n",
      "Train Batch Loss: 2.3132166862487793\n",
      "Train Batch Loss: 2.546886444091797\n",
      "Train Batch Loss: 2.3473262786865234\n",
      "Train Batch Loss: 2.4292848110198975\n",
      "Train Batch Loss: 2.264331817626953\n",
      "Train Batch Loss: 1.99803626537323\n",
      "Train Batch Loss: 1.7073609828948975\n",
      "Train Batch Loss: 2.1894893646240234\n",
      "Train Batch Loss: 2.4243388175964355\n",
      "Train Batch Loss: 3.419280767440796\n",
      "Train Batch Loss: 1.984802007675171\n",
      "Train Batch Loss: 3.3315916061401367\n",
      "Train Batch Loss: 2.514472007751465\n",
      "Train Batch Loss: 3.366029739379883\n",
      "Train Batch Loss: 3.456393003463745\n",
      "Train Batch Loss: 2.0564334392547607\n",
      "Train Batch Loss: 2.2508654594421387\n",
      "Train Batch Loss: 3.4078049659729004\n",
      "Train Batch Loss: 2.3071436882019043\n",
      "Train Batch Loss: 2.0730159282684326\n",
      "Train Batch Loss: 2.9535224437713623\n",
      "Train Batch Loss: 2.4095375537872314\n",
      "Train Batch Loss: 2.804539203643799\n",
      "Train Batch Loss: 4.011298179626465\n",
      "Train Batch Loss: 3.0728297233581543\n",
      "Train Batch Loss: 2.3246779441833496\n",
      "Train Batch Loss: 2.4557695388793945\n",
      "Train Batch Loss: 2.5093770027160645\n",
      "Train Batch Loss: 2.3150811195373535\n",
      "Train Batch Loss: 2.7572832107543945\n",
      "valid Batch Loss: 4.205903053283691\n",
      "valid Batch Loss: 3.9647984504699707\n",
      "valid Batch Loss: 3.827364921569824\n",
      "valid Batch Loss: 3.70497727394104\n",
      "valid Batch Loss: 3.6477694511413574\n",
      "valid Batch Loss: 82.49278259277344\n",
      "valid Batch Loss: 3.662580966949463\n",
      "valid Batch Loss: 3.49182391166687\n",
      "valid Batch Loss: 3.946503162384033\n",
      "valid Batch Loss: 3.840442180633545\n",
      "valid Batch Loss: 3.6394643783569336\n",
      "valid Batch Loss: 382.02044677734375\n",
      "valid Batch Loss: 3.77614688873291\n",
      "valid Batch Loss: 3.8641786575317383\n",
      "valid Batch Loss: 4.128109931945801\n",
      "valid Batch Loss: 3.27532696723938\n",
      "valid Batch Loss: 3.55265212059021\n",
      "valid Batch Loss: 3.9621634483337402\n",
      "valid Batch Loss: 137.109130859375\n",
      "valid Batch Loss: 3.6672215461730957\n",
      "valid Batch Loss: 4.246870040893555\n",
      "valid Batch Loss: 3.6517810821533203\n",
      "valid Batch Loss: 3.1424970626831055\n",
      "valid Batch Loss: 16.47696876525879\n",
      "valid Batch Loss: 44.88111877441406\n",
      "valid Batch Loss: 4.228243350982666\n",
      "valid Batch Loss: 3.979647159576416\n",
      "valid Batch Loss: 3.863375425338745\n",
      "valid Batch Loss: 392.97625732421875\n",
      "valid Batch Loss: 3.8571369647979736\n",
      "valid Batch Loss: 3.538282632827759\n",
      "valid Batch Loss: 3.795816421508789\n",
      "valid Batch Loss: 5.047181129455566\n",
      "valid Batch Loss: 3.937558174133301\n",
      "valid Batch Loss: 3.6228208541870117\n",
      "valid Batch Loss: 4.247231483459473\n",
      "valid Batch Loss: 3.7327880859375\n",
      "valid Batch Loss: 3.4434380531311035\n",
      "valid Batch Loss: 3.4504897594451904\n",
      "valid Batch Loss: 1130.313232421875\n",
      "valid Batch Loss: 229.33346557617188\n",
      "valid Batch Loss: 4.439698696136475\n",
      "valid Batch Loss: 3.986910343170166\n",
      "valid Batch Loss: 3.72691011428833\n",
      "valid Batch Loss: 85.17395782470703\n",
      "valid Batch Loss: 3.9945647716522217\n",
      "valid Batch Loss: 4.283834457397461\n",
      "valid Batch Loss: 4.113755226135254\n",
      "valid Batch Loss: 390.4017333984375\n",
      "valid Batch Loss: 4.079624176025391\n",
      "valid Batch Loss: 4.059082984924316\n",
      "valid Batch Loss: 4.351398944854736\n",
      "valid Batch Loss: 52.96796417236328\n",
      "valid Batch Loss: 3.5015463829040527\n",
      "valid Batch Loss: 4.065033435821533\n",
      "valid Batch Loss: 3.9950292110443115\n",
      "valid Batch Loss: 3.481990337371826\n",
      "valid Batch Loss: 3.9735071659088135\n",
      "valid Batch Loss: 3.813107490539551\n",
      "valid Batch Loss: 3.2548840045928955\n",
      "valid Batch Loss: 4.321854114532471\n",
      "valid Batch Loss: 3.9302728176116943\n",
      "valid Batch Loss: 86.24320983886719\n",
      "valid Batch Loss: 22.796642303466797\n",
      "valid Batch Loss: 3.9454636573791504\n",
      "valid Batch Loss: 3.305014133453369\n",
      "valid Batch Loss: 18.029829025268555\n",
      "valid Batch Loss: 3.7194395065307617\n",
      "valid Batch Loss: 4.201502799987793\n",
      "valid Batch Loss: 479.8038330078125\n",
      "valid Batch Loss: 3.456841468811035\n",
      "Epoch: 6 | Train Loss: 0.8166038393974304 | Valid Loss: 36.01655960083008\n",
      "\n",
      "Epoch: 6 | Train AUROC: 0.9380569931626055 | Valid AUROC: 0.9100908834989679\n",
      "\n",
      "Train Batch Loss: 2.0622994899749756\n",
      "Train Batch Loss: 2.188281536102295\n",
      "Train Batch Loss: 2.127671241760254\n",
      "Train Batch Loss: 1.885481357574463\n",
      "Train Batch Loss: 2.9611635208129883\n",
      "Train Batch Loss: 1.6926928758621216\n",
      "Train Batch Loss: 1.9339015483856201\n",
      "Train Batch Loss: 2.0698986053466797\n",
      "Train Batch Loss: 2.951298713684082\n",
      "Train Batch Loss: 4.373232364654541\n",
      "Train Batch Loss: 2.3941376209259033\n",
      "Train Batch Loss: 3.0578854084014893\n",
      "Train Batch Loss: 3.6493427753448486\n",
      "Train Batch Loss: 3.1504440307617188\n",
      "Train Batch Loss: 2.855001211166382\n",
      "Train Batch Loss: 3.6476354598999023\n",
      "Train Batch Loss: 3.0911102294921875\n",
      "Train Batch Loss: 1.7618532180786133\n",
      "Train Batch Loss: 2.5261454582214355\n",
      "Train Batch Loss: 2.4678139686584473\n",
      "Train Batch Loss: 2.7920289039611816\n",
      "Train Batch Loss: 2.056906223297119\n",
      "Train Batch Loss: 2.221458673477173\n",
      "Train Batch Loss: 1.8265855312347412\n",
      "Train Batch Loss: 2.424921989440918\n",
      "Train Batch Loss: 2.657275915145874\n",
      "Train Batch Loss: 2.967400550842285\n",
      "Train Batch Loss: 2.2864296436309814\n",
      "Train Batch Loss: 2.3392772674560547\n",
      "Train Batch Loss: 2.3254804611206055\n",
      "Train Batch Loss: 1.8314080238342285\n",
      "Train Batch Loss: 3.8113820552825928\n",
      "Train Batch Loss: 3.373734712600708\n",
      "Train Batch Loss: 2.8638811111450195\n",
      "Train Batch Loss: 2.4274516105651855\n",
      "Train Batch Loss: 2.272935152053833\n",
      "Train Batch Loss: 1.9241877794265747\n",
      "Train Batch Loss: 2.332205057144165\n",
      "Train Batch Loss: 2.339146614074707\n",
      "Train Batch Loss: 1.730017900466919\n",
      "Train Batch Loss: 3.7184386253356934\n",
      "Train Batch Loss: 2.461782932281494\n",
      "Train Batch Loss: 1.8553955554962158\n",
      "Train Batch Loss: 1.8810641765594482\n",
      "Train Batch Loss: 2.6226885318756104\n",
      "Train Batch Loss: 1.78461492061615\n",
      "Train Batch Loss: 2.585179328918457\n",
      "Train Batch Loss: 2.1587727069854736\n",
      "Train Batch Loss: 2.2349636554718018\n",
      "Train Batch Loss: 2.049086570739746\n",
      "Train Batch Loss: 2.1785120964050293\n",
      "Train Batch Loss: 2.040071964263916\n",
      "Train Batch Loss: 1.967482328414917\n",
      "Train Batch Loss: 1.866851568222046\n",
      "Train Batch Loss: 1.6850852966308594\n",
      "Train Batch Loss: 1.878234624862671\n",
      "Train Batch Loss: 2.178011417388916\n",
      "Train Batch Loss: 1.9826507568359375\n",
      "Train Batch Loss: 2.520599603652954\n",
      "Train Batch Loss: 2.1420388221740723\n",
      "Train Batch Loss: 2.1137750148773193\n",
      "Train Batch Loss: 1.770883321762085\n",
      "Train Batch Loss: 2.3308186531066895\n",
      "Train Batch Loss: 1.8286728858947754\n",
      "Train Batch Loss: 2.376511335372925\n",
      "Train Batch Loss: 1.8947198390960693\n",
      "Train Batch Loss: 1.8768043518066406\n",
      "Train Batch Loss: 1.7782797813415527\n",
      "Train Batch Loss: 2.5403318405151367\n",
      "Train Batch Loss: 2.539310932159424\n",
      "Train Batch Loss: 2.3718924522399902\n",
      "Train Batch Loss: 2.2209300994873047\n",
      "Train Batch Loss: 2.625701665878296\n",
      "Train Batch Loss: 1.8851633071899414\n",
      "Train Batch Loss: 1.599137783050537\n",
      "valid Batch Loss: 3.8819186687469482\n",
      "valid Batch Loss: 3.782658815383911\n",
      "valid Batch Loss: 3.5898542404174805\n",
      "valid Batch Loss: 3.5272135734558105\n",
      "valid Batch Loss: 3.2968826293945312\n",
      "valid Batch Loss: 2.8746132850646973\n",
      "valid Batch Loss: 3.450072765350342\n",
      "valid Batch Loss: 3.3169033527374268\n",
      "valid Batch Loss: 3.660346269607544\n",
      "valid Batch Loss: 3.616114854812622\n",
      "valid Batch Loss: 3.4166951179504395\n",
      "valid Batch Loss: 75.38338470458984\n",
      "valid Batch Loss: 3.431311845779419\n",
      "valid Batch Loss: 3.448279857635498\n",
      "valid Batch Loss: 3.7629034519195557\n",
      "valid Batch Loss: 3.2378742694854736\n",
      "valid Batch Loss: 3.1655972003936768\n",
      "valid Batch Loss: 3.6779332160949707\n",
      "valid Batch Loss: 40.063690185546875\n",
      "valid Batch Loss: 3.385525703430176\n",
      "valid Batch Loss: 4.1502790451049805\n",
      "valid Batch Loss: 3.360567569732666\n",
      "valid Batch Loss: 2.9228758811950684\n",
      "valid Batch Loss: 3.3356926441192627\n",
      "valid Batch Loss: 3.5908031463623047\n",
      "valid Batch Loss: 3.8464598655700684\n",
      "valid Batch Loss: 3.86063551902771\n",
      "valid Batch Loss: 3.7262604236602783\n",
      "valid Batch Loss: 3.6653125286102295\n",
      "valid Batch Loss: 3.495636224746704\n",
      "valid Batch Loss: 3.2778518199920654\n",
      "valid Batch Loss: 3.5319180488586426\n",
      "valid Batch Loss: 4.283102035522461\n",
      "valid Batch Loss: 3.7386279106140137\n",
      "valid Batch Loss: 3.3611717224121094\n",
      "valid Batch Loss: 3.9620003700256348\n",
      "valid Batch Loss: 3.522336006164551\n",
      "valid Batch Loss: 3.1770949363708496\n",
      "valid Batch Loss: 3.201049327850342\n",
      "valid Batch Loss: 62.805458068847656\n",
      "valid Batch Loss: 3.4869861602783203\n",
      "valid Batch Loss: 4.30025053024292\n",
      "valid Batch Loss: 3.794893980026245\n",
      "valid Batch Loss: 3.4680442810058594\n",
      "valid Batch Loss: 3.4212067127227783\n",
      "valid Batch Loss: 3.877535343170166\n",
      "valid Batch Loss: 4.038744926452637\n",
      "valid Batch Loss: 3.7891294956207275\n",
      "valid Batch Loss: 3.9544425010681152\n",
      "valid Batch Loss: 3.8310680389404297\n",
      "valid Batch Loss: 3.847308397293091\n",
      "valid Batch Loss: 4.007745742797852\n",
      "valid Batch Loss: 4.049744606018066\n",
      "valid Batch Loss: 3.2854340076446533\n",
      "valid Batch Loss: 3.8512845039367676\n",
      "valid Batch Loss: 3.933837413787842\n",
      "valid Batch Loss: 3.183307647705078\n",
      "valid Batch Loss: 3.7608802318573\n",
      "valid Batch Loss: 3.7782559394836426\n",
      "valid Batch Loss: 3.0437772274017334\n",
      "valid Batch Loss: 3.9112865924835205\n",
      "valid Batch Loss: 3.5620384216308594\n",
      "valid Batch Loss: 29.5032958984375\n",
      "valid Batch Loss: 3.906078577041626\n",
      "valid Batch Loss: 3.744739532470703\n",
      "valid Batch Loss: 3.0410914421081543\n",
      "valid Batch Loss: 3.5501153469085693\n",
      "valid Batch Loss: 3.4656801223754883\n",
      "valid Batch Loss: 3.913851737976074\n",
      "valid Batch Loss: 25.546321868896484\n",
      "valid Batch Loss: 3.301957368850708\n",
      "Epoch: 7 | Train Loss: 0.7955120801925659 | Valid Loss: 6.250068187713623\n",
      "\n",
      "Epoch: 7 | Train AUROC: 0.9421687410139425 | Valid AUROC: 0.9162611500829472\n",
      "\n",
      "Train Batch Loss: 2.0299251079559326\n",
      "Train Batch Loss: 1.8460180759429932\n",
      "Train Batch Loss: 1.7372205257415771\n",
      "Train Batch Loss: 1.8008769750595093\n",
      "Train Batch Loss: 2.5245954990386963\n",
      "Train Batch Loss: 4.603184700012207\n",
      "Train Batch Loss: 2.369535207748413\n",
      "Train Batch Loss: 1.845779299736023\n",
      "Train Batch Loss: 1.8584493398666382\n",
      "Train Batch Loss: 1.7810313701629639\n",
      "Train Batch Loss: 2.2893998622894287\n",
      "Train Batch Loss: 2.889315128326416\n",
      "Train Batch Loss: 1.674311637878418\n",
      "Train Batch Loss: 1.901037335395813\n",
      "Train Batch Loss: 1.6422584056854248\n",
      "Train Batch Loss: 1.806046485900879\n",
      "Train Batch Loss: 2.0944759845733643\n",
      "Train Batch Loss: 1.415079951286316\n",
      "Train Batch Loss: 5.0826616287231445\n",
      "Train Batch Loss: 2.695648431777954\n",
      "Train Batch Loss: 2.2310330867767334\n",
      "Train Batch Loss: 2.3565194606781006\n",
      "Train Batch Loss: 2.2166154384613037\n",
      "Train Batch Loss: 2.441474199295044\n",
      "Train Batch Loss: 2.2239737510681152\n",
      "Train Batch Loss: 2.4075450897216797\n",
      "Train Batch Loss: 1.7639353275299072\n",
      "Train Batch Loss: 1.5819401741027832\n",
      "Train Batch Loss: 1.8152490854263306\n",
      "Train Batch Loss: 2.515439510345459\n",
      "Train Batch Loss: 2.049478530883789\n",
      "Train Batch Loss: 1.9431216716766357\n",
      "Train Batch Loss: 3.509934902191162\n",
      "Train Batch Loss: 3.177855968475342\n",
      "Train Batch Loss: 1.8893920183181763\n",
      "Train Batch Loss: 2.1487069129943848\n",
      "Train Batch Loss: 2.2207326889038086\n",
      "Train Batch Loss: 2.1388700008392334\n",
      "Train Batch Loss: 1.4454437494277954\n",
      "Train Batch Loss: 2.5447211265563965\n",
      "Train Batch Loss: 1.7794854640960693\n",
      "Train Batch Loss: 2.8896896839141846\n",
      "Train Batch Loss: 1.9664850234985352\n",
      "Train Batch Loss: 2.000701665878296\n",
      "Train Batch Loss: 3.4894633293151855\n",
      "Train Batch Loss: 2.1171913146972656\n",
      "Train Batch Loss: 1.7406253814697266\n",
      "Train Batch Loss: 1.9670498371124268\n",
      "Train Batch Loss: 1.5996350049972534\n",
      "Train Batch Loss: 1.5282483100891113\n",
      "Train Batch Loss: 2.334688186645508\n",
      "Train Batch Loss: 1.859002709388733\n",
      "Train Batch Loss: 1.907395362854004\n",
      "Train Batch Loss: 2.0425634384155273\n",
      "Train Batch Loss: 1.7532508373260498\n",
      "Train Batch Loss: 1.7549033164978027\n",
      "Train Batch Loss: 2.114290714263916\n",
      "Train Batch Loss: 2.328209638595581\n",
      "Train Batch Loss: 4.107044219970703\n",
      "Train Batch Loss: 2.706737995147705\n",
      "Train Batch Loss: 2.0937411785125732\n",
      "Train Batch Loss: 1.902590274810791\n",
      "Train Batch Loss: 2.5677809715270996\n",
      "Train Batch Loss: 2.4113807678222656\n",
      "Train Batch Loss: 1.5825138092041016\n",
      "Train Batch Loss: 1.850466012954712\n",
      "Train Batch Loss: 1.921079158782959\n",
      "Train Batch Loss: 2.559610605239868\n",
      "Train Batch Loss: 1.7853646278381348\n",
      "Train Batch Loss: 1.702237844467163\n",
      "Train Batch Loss: 1.6915998458862305\n",
      "Train Batch Loss: 2.3411612510681152\n",
      "Train Batch Loss: 3.0803370475769043\n",
      "Train Batch Loss: 1.7704216241836548\n",
      "Train Batch Loss: 2.627978563308716\n",
      "valid Batch Loss: 4.009270668029785\n",
      "valid Batch Loss: 3.9416913986206055\n",
      "valid Batch Loss: 3.8094730377197266\n",
      "valid Batch Loss: 3.5527474880218506\n",
      "valid Batch Loss: 3.5948121547698975\n",
      "valid Batch Loss: 3.181042194366455\n",
      "valid Batch Loss: 3.604072093963623\n",
      "valid Batch Loss: 3.3070874214172363\n",
      "valid Batch Loss: 4.057985782623291\n",
      "valid Batch Loss: 3.7603206634521484\n",
      "valid Batch Loss: 3.5951309204101562\n",
      "valid Batch Loss: 4.168372631072998\n",
      "valid Batch Loss: 3.617131233215332\n",
      "valid Batch Loss: 3.6836917400360107\n",
      "valid Batch Loss: 4.095455169677734\n",
      "valid Batch Loss: 3.283730983734131\n",
      "valid Batch Loss: 3.339672327041626\n",
      "valid Batch Loss: 3.780275821685791\n",
      "valid Batch Loss: 3.532874584197998\n",
      "valid Batch Loss: 3.6134414672851562\n",
      "valid Batch Loss: 4.410455703735352\n",
      "valid Batch Loss: 3.6899826526641846\n",
      "valid Batch Loss: 3.2025115489959717\n",
      "valid Batch Loss: 3.559091567993164\n",
      "valid Batch Loss: 3.6785311698913574\n",
      "valid Batch Loss: 4.059634685516357\n",
      "valid Batch Loss: 4.0470476150512695\n",
      "valid Batch Loss: 3.8849220275878906\n",
      "valid Batch Loss: 3.8708508014678955\n",
      "valid Batch Loss: 3.5998549461364746\n",
      "valid Batch Loss: 3.5662388801574707\n",
      "valid Batch Loss: 3.720104455947876\n",
      "valid Batch Loss: 4.447623252868652\n",
      "valid Batch Loss: 3.8746490478515625\n",
      "valid Batch Loss: 3.563736915588379\n",
      "valid Batch Loss: 4.152088165283203\n",
      "valid Batch Loss: 3.6476759910583496\n",
      "valid Batch Loss: 3.323577880859375\n",
      "valid Batch Loss: 3.3778023719787598\n",
      "valid Batch Loss: 87.64852142333984\n",
      "valid Batch Loss: 3.7199134826660156\n",
      "valid Batch Loss: 4.47091007232666\n",
      "valid Batch Loss: 4.17002534866333\n",
      "valid Batch Loss: 3.671710968017578\n",
      "valid Batch Loss: 3.7119486331939697\n",
      "valid Batch Loss: 4.167405605316162\n",
      "valid Batch Loss: 4.3505096435546875\n",
      "valid Batch Loss: 4.020427703857422\n",
      "valid Batch Loss: 4.092981338500977\n",
      "valid Batch Loss: 4.109576225280762\n",
      "valid Batch Loss: 4.010612487792969\n",
      "valid Batch Loss: 4.3082709312438965\n",
      "valid Batch Loss: 4.409337043762207\n",
      "valid Batch Loss: 3.5430331230163574\n",
      "valid Batch Loss: 3.9469974040985107\n",
      "valid Batch Loss: 3.9879090785980225\n",
      "valid Batch Loss: 3.5553879737854004\n",
      "valid Batch Loss: 3.9395394325256348\n",
      "valid Batch Loss: 3.8887977600097656\n",
      "valid Batch Loss: 3.293288230895996\n",
      "valid Batch Loss: 4.227543830871582\n",
      "valid Batch Loss: 3.800105333328247\n",
      "valid Batch Loss: 3.248041868209839\n",
      "valid Batch Loss: 4.108940124511719\n",
      "valid Batch Loss: 4.013753890991211\n",
      "valid Batch Loss: 3.352036952972412\n",
      "valid Batch Loss: 3.7566709518432617\n",
      "valid Batch Loss: 3.520141363143921\n",
      "valid Batch Loss: 3.964182138442993\n",
      "valid Batch Loss: 4.203529357910156\n",
      "valid Batch Loss: 3.3918418884277344\n",
      "Epoch: 8 | Train Loss: 0.7340614795684814 | Valid Loss: 4.135467529296875\n",
      "\n",
      "Epoch: 8 | Train AUROC: 0.9490855065869183 | Valid AUROC: 0.9115164891898602\n",
      "\n",
      "\u001b[34mValidation Loss Improved (5.206560134887695 ---> 4.135467529296875)\u001b[0m\n",
      "\u001b[34mModel Saved\u001b[0m\n",
      "Train Batch Loss: 1.5460691452026367\n",
      "Train Batch Loss: 1.8897738456726074\n",
      "Train Batch Loss: 1.4582198858261108\n",
      "Train Batch Loss: 2.052367687225342\n",
      "Train Batch Loss: 2.8297977447509766\n",
      "Train Batch Loss: 2.407947540283203\n",
      "Train Batch Loss: 2.289100408554077\n",
      "Train Batch Loss: 1.8427305221557617\n",
      "Train Batch Loss: 1.5070868730545044\n",
      "Train Batch Loss: 1.6975337266921997\n",
      "Train Batch Loss: 2.192847728729248\n",
      "Train Batch Loss: 2.526381731033325\n",
      "Train Batch Loss: 1.9225890636444092\n",
      "Train Batch Loss: 2.857081413269043\n",
      "Train Batch Loss: 1.5525118112564087\n",
      "Train Batch Loss: 1.8985588550567627\n",
      "Train Batch Loss: 2.3974437713623047\n",
      "Train Batch Loss: 1.6644914150238037\n",
      "Train Batch Loss: 1.9907855987548828\n",
      "Train Batch Loss: 2.4161319732666016\n",
      "Train Batch Loss: 2.0928268432617188\n",
      "Train Batch Loss: 1.3824982643127441\n",
      "Train Batch Loss: 2.460296392440796\n",
      "Train Batch Loss: 1.7328603267669678\n",
      "Train Batch Loss: 2.2983360290527344\n",
      "Train Batch Loss: 1.8537808656692505\n",
      "Train Batch Loss: 1.78702712059021\n",
      "Train Batch Loss: 1.3264923095703125\n",
      "Train Batch Loss: 1.4556456804275513\n",
      "Train Batch Loss: 0.9932971000671387\n",
      "Train Batch Loss: 2.289186477661133\n",
      "Train Batch Loss: 1.3544645309448242\n",
      "Train Batch Loss: 2.0045809745788574\n",
      "Train Batch Loss: 2.2112412452697754\n",
      "Train Batch Loss: 1.9739274978637695\n",
      "Train Batch Loss: 1.7835404872894287\n",
      "Train Batch Loss: 2.179638624191284\n",
      "Train Batch Loss: 1.8762824535369873\n",
      "Train Batch Loss: 1.6619160175323486\n",
      "Train Batch Loss: 1.856003761291504\n",
      "Train Batch Loss: 1.5713543891906738\n",
      "Train Batch Loss: 1.9676175117492676\n",
      "Train Batch Loss: 1.8186547756195068\n",
      "Train Batch Loss: 2.0712227821350098\n",
      "Train Batch Loss: 2.180835723876953\n",
      "Train Batch Loss: 1.4578220844268799\n",
      "Train Batch Loss: 1.8538345098495483\n",
      "Train Batch Loss: 2.2564644813537598\n",
      "Train Batch Loss: 1.6354176998138428\n",
      "Train Batch Loss: 1.7455732822418213\n",
      "Train Batch Loss: 1.4121589660644531\n",
      "Train Batch Loss: 0.9394053220748901\n",
      "Train Batch Loss: 1.6796220541000366\n",
      "Train Batch Loss: 1.6695213317871094\n",
      "Train Batch Loss: 1.3876782655715942\n",
      "Train Batch Loss: 1.656032681465149\n",
      "Train Batch Loss: 1.4645627737045288\n",
      "Train Batch Loss: 1.6007354259490967\n",
      "Train Batch Loss: 1.7948631048202515\n",
      "Train Batch Loss: 2.0503482818603516\n",
      "Train Batch Loss: 2.1936397552490234\n",
      "Train Batch Loss: 1.4538275003433228\n",
      "Train Batch Loss: 1.302335262298584\n",
      "Train Batch Loss: 1.702336311340332\n",
      "Train Batch Loss: 1.9550604820251465\n",
      "Train Batch Loss: 2.362638473510742\n",
      "Train Batch Loss: 2.5009970664978027\n",
      "Train Batch Loss: 2.1197092533111572\n",
      "Train Batch Loss: 2.6364378929138184\n",
      "Train Batch Loss: 2.4394445419311523\n",
      "Train Batch Loss: 1.937197208404541\n",
      "Train Batch Loss: 1.886237382888794\n",
      "Train Batch Loss: 1.9180341958999634\n",
      "Train Batch Loss: 2.031109571456909\n",
      "Train Batch Loss: 1.449493169784546\n",
      "valid Batch Loss: 2.8758766651153564\n",
      "valid Batch Loss: 2.8130273818969727\n",
      "valid Batch Loss: 2.5848867893218994\n",
      "valid Batch Loss: 2.442559003829956\n",
      "valid Batch Loss: 2.22462797164917\n",
      "valid Batch Loss: 2.0258209705352783\n",
      "valid Batch Loss: 2.3408303260803223\n",
      "valid Batch Loss: 2.1474361419677734\n",
      "valid Batch Loss: 2.8765034675598145\n",
      "valid Batch Loss: 2.4653749465942383\n",
      "valid Batch Loss: 2.360191822052002\n",
      "valid Batch Loss: 2.6783485412597656\n",
      "valid Batch Loss: 2.4751434326171875\n",
      "valid Batch Loss: 2.336980104446411\n",
      "valid Batch Loss: 2.785792827606201\n",
      "valid Batch Loss: 2.191117286682129\n",
      "valid Batch Loss: 2.238349437713623\n",
      "valid Batch Loss: 2.4931282997131348\n",
      "valid Batch Loss: 2.1829400062561035\n",
      "valid Batch Loss: 2.3723440170288086\n",
      "valid Batch Loss: 3.0638012886047363\n",
      "valid Batch Loss: 2.4286372661590576\n",
      "valid Batch Loss: 2.0066699981689453\n",
      "valid Batch Loss: 2.4418365955352783\n",
      "valid Batch Loss: 2.437345027923584\n",
      "valid Batch Loss: 2.6522581577301025\n",
      "valid Batch Loss: 2.781534194946289\n",
      "valid Batch Loss: 2.784637689590454\n",
      "valid Batch Loss: 2.722787380218506\n",
      "valid Batch Loss: 2.494185447692871\n",
      "valid Batch Loss: 2.3410141468048096\n",
      "valid Batch Loss: 2.477370262145996\n",
      "valid Batch Loss: 3.180799961090088\n",
      "valid Batch Loss: 2.7397818565368652\n",
      "valid Batch Loss: 2.4368367195129395\n",
      "valid Batch Loss: 2.9325835704803467\n",
      "valid Batch Loss: 2.455695152282715\n",
      "valid Batch Loss: 2.337982654571533\n",
      "valid Batch Loss: 2.2970895767211914\n",
      "valid Batch Loss: 2.8845949172973633\n",
      "valid Batch Loss: 2.285677433013916\n",
      "valid Batch Loss: 3.169462203979492\n",
      "valid Batch Loss: 2.9146761894226074\n",
      "valid Batch Loss: 2.3875808715820312\n",
      "valid Batch Loss: 2.350393295288086\n",
      "valid Batch Loss: 2.899628162384033\n",
      "valid Batch Loss: 3.140507698059082\n",
      "valid Batch Loss: 2.77130126953125\n",
      "valid Batch Loss: 2.775603771209717\n",
      "valid Batch Loss: 2.9063072204589844\n",
      "valid Batch Loss: 2.769406318664551\n",
      "valid Batch Loss: 3.00532603263855\n",
      "valid Batch Loss: 2.896317481994629\n",
      "valid Batch Loss: 2.4147074222564697\n",
      "valid Batch Loss: 2.817790985107422\n",
      "valid Batch Loss: 2.7908365726470947\n",
      "valid Batch Loss: 2.4419846534729004\n",
      "valid Batch Loss: 2.7933311462402344\n",
      "valid Batch Loss: 2.6874196529388428\n",
      "valid Batch Loss: 2.106140375137329\n",
      "valid Batch Loss: 2.9892868995666504\n",
      "valid Batch Loss: 2.6244091987609863\n",
      "valid Batch Loss: 2.1333889961242676\n",
      "valid Batch Loss: 2.8843064308166504\n",
      "valid Batch Loss: 2.7341830730438232\n",
      "valid Batch Loss: 2.108250141143799\n",
      "valid Batch Loss: 2.510209083557129\n",
      "valid Batch Loss: 2.4500017166137695\n",
      "valid Batch Loss: 2.841632843017578\n",
      "valid Batch Loss: 2.8803043365478516\n",
      "valid Batch Loss: 2.2745211124420166\n",
      "Epoch: 9 | Train Loss: 0.6672838926315308 | Valid Loss: 2.633877754211426\n",
      "\n",
      "Epoch: 9 | Train AUROC: 0.9542936613073245 | Valid AUROC: 0.9163220489871304\n",
      "\n",
      "\u001b[34mValidation Loss Improved (4.135467529296875 ---> 2.633877754211426)\u001b[0m\n",
      "\u001b[34mModel Saved\u001b[0m\n",
      "Train Batch Loss: 2.2528343200683594\n",
      "Train Batch Loss: 1.4789942502975464\n",
      "Train Batch Loss: 1.8092774152755737\n",
      "Train Batch Loss: 2.053074359893799\n",
      "Train Batch Loss: 1.423729658126831\n",
      "Train Batch Loss: 1.3927807807922363\n",
      "Train Batch Loss: 2.0713493824005127\n",
      "Train Batch Loss: 1.9689692258834839\n",
      "Train Batch Loss: 1.391903281211853\n",
      "Train Batch Loss: 1.8773353099822998\n",
      "Train Batch Loss: 1.7408442497253418\n",
      "Train Batch Loss: 1.8512217998504639\n",
      "Train Batch Loss: 1.8492993116378784\n",
      "Train Batch Loss: 2.161925792694092\n",
      "Train Batch Loss: 2.8148837089538574\n",
      "Train Batch Loss: 2.0991625785827637\n",
      "Train Batch Loss: 1.8110181093215942\n",
      "Train Batch Loss: 2.4389960765838623\n",
      "Train Batch Loss: 1.5444544553756714\n",
      "Train Batch Loss: 2.1072838306427\n",
      "Train Batch Loss: 2.0386345386505127\n",
      "Train Batch Loss: 3.261772632598877\n",
      "Train Batch Loss: 1.6224217414855957\n",
      "Train Batch Loss: 1.7347702980041504\n",
      "Train Batch Loss: 2.9650392532348633\n",
      "Train Batch Loss: 1.3880308866500854\n",
      "Train Batch Loss: 1.8167718648910522\n",
      "Train Batch Loss: 2.2188568115234375\n",
      "Train Batch Loss: 1.9899232387542725\n",
      "Train Batch Loss: 1.9320048093795776\n",
      "Train Batch Loss: 1.7424683570861816\n",
      "Train Batch Loss: 3.660618305206299\n",
      "Train Batch Loss: 1.3877085447311401\n",
      "Train Batch Loss: 1.4487195014953613\n",
      "Train Batch Loss: 1.7929935455322266\n",
      "Train Batch Loss: 1.6639344692230225\n",
      "Train Batch Loss: 1.5346267223358154\n",
      "Train Batch Loss: 1.463904619216919\n",
      "Train Batch Loss: 2.08103609085083\n",
      "Train Batch Loss: 1.511857509613037\n",
      "Train Batch Loss: 2.017620086669922\n",
      "Train Batch Loss: 1.9622929096221924\n",
      "Train Batch Loss: 1.8609583377838135\n",
      "Train Batch Loss: 1.7261561155319214\n",
      "Train Batch Loss: 1.576744794845581\n",
      "Train Batch Loss: 3.196895122528076\n",
      "Train Batch Loss: 1.6928479671478271\n",
      "Train Batch Loss: 1.9461381435394287\n",
      "Train Batch Loss: 2.0900967121124268\n",
      "Train Batch Loss: 1.6285805702209473\n",
      "Train Batch Loss: 1.6934616565704346\n",
      "Train Batch Loss: 1.5286005735397339\n",
      "Train Batch Loss: 1.4452353715896606\n",
      "Train Batch Loss: 1.383148193359375\n",
      "Train Batch Loss: 2.326663017272949\n",
      "Train Batch Loss: 1.4243407249450684\n",
      "Train Batch Loss: 1.2868454456329346\n",
      "Train Batch Loss: 1.1481033563613892\n",
      "Train Batch Loss: 2.767279863357544\n",
      "Train Batch Loss: 1.6057630777359009\n",
      "Train Batch Loss: 2.5365912914276123\n",
      "Train Batch Loss: 1.8750463724136353\n",
      "Train Batch Loss: 2.201575517654419\n",
      "Train Batch Loss: 1.87608802318573\n",
      "Train Batch Loss: 2.0986075401306152\n",
      "Train Batch Loss: 2.2800111770629883\n",
      "Train Batch Loss: 1.9641317129135132\n",
      "Train Batch Loss: 1.2321244478225708\n",
      "Train Batch Loss: 1.5116716623306274\n",
      "Train Batch Loss: 1.379792332649231\n",
      "Train Batch Loss: 1.5223135948181152\n",
      "Train Batch Loss: 2.26186466217041\n",
      "Train Batch Loss: 1.5858439207077026\n",
      "Train Batch Loss: 2.40680193901062\n",
      "Train Batch Loss: 1.4149129390716553\n",
      "valid Batch Loss: 3.0589075088500977\n",
      "valid Batch Loss: 2.9776995182037354\n",
      "valid Batch Loss: 2.7642111778259277\n",
      "valid Batch Loss: 2.6850032806396484\n",
      "valid Batch Loss: 2.431373119354248\n",
      "valid Batch Loss: 8.465179443359375\n",
      "valid Batch Loss: 2.5834298133850098\n",
      "valid Batch Loss: 2.3669638633728027\n",
      "valid Batch Loss: 3.071617603302002\n",
      "valid Batch Loss: 2.8134679794311523\n",
      "valid Batch Loss: 2.5074939727783203\n",
      "valid Batch Loss: 21.525903701782227\n",
      "valid Batch Loss: 2.680119514465332\n",
      "valid Batch Loss: 2.6141700744628906\n",
      "valid Batch Loss: 3.0270276069641113\n",
      "valid Batch Loss: 2.3858158588409424\n",
      "valid Batch Loss: 2.3778436183929443\n",
      "valid Batch Loss: 2.7208476066589355\n",
      "valid Batch Loss: 14.745878219604492\n",
      "valid Batch Loss: 2.6798808574676514\n",
      "valid Batch Loss: 3.2953169345855713\n",
      "valid Batch Loss: 2.725471019744873\n",
      "valid Batch Loss: 2.1291091442108154\n",
      "valid Batch Loss: 22.522092819213867\n",
      "valid Batch Loss: 7.042723655700684\n",
      "valid Batch Loss: 2.8618054389953613\n",
      "valid Batch Loss: 3.1819329261779785\n",
      "valid Batch Loss: 2.872429370880127\n",
      "valid Batch Loss: 197.8058624267578\n",
      "valid Batch Loss: 2.6770875453948975\n",
      "valid Batch Loss: 2.644341468811035\n",
      "valid Batch Loss: 2.712026596069336\n",
      "valid Batch Loss: 15.829217910766602\n",
      "valid Batch Loss: 2.9242920875549316\n",
      "valid Batch Loss: 4.743788719177246\n",
      "valid Batch Loss: 3.0892105102539062\n",
      "valid Batch Loss: 2.6664929389953613\n",
      "valid Batch Loss: 2.3951497077941895\n",
      "valid Batch Loss: 2.5719187259674072\n",
      "valid Batch Loss: 112.94739532470703\n",
      "valid Batch Loss: 2.652590274810791\n",
      "valid Batch Loss: 3.3839640617370605\n",
      "valid Batch Loss: 3.0649356842041016\n",
      "valid Batch Loss: 2.613131523132324\n",
      "valid Batch Loss: 11.09486198425293\n",
      "valid Batch Loss: 3.0264439582824707\n",
      "valid Batch Loss: 3.3792152404785156\n",
      "valid Batch Loss: 2.997011184692383\n",
      "valid Batch Loss: 81.01080322265625\n",
      "valid Batch Loss: 3.0095102787017822\n",
      "valid Batch Loss: 3.0391974449157715\n",
      "valid Batch Loss: 3.194471597671509\n",
      "valid Batch Loss: 116.0157699584961\n",
      "valid Batch Loss: 2.4892959594726562\n",
      "valid Batch Loss: 2.916372060775757\n",
      "valid Batch Loss: 2.9967286586761475\n",
      "valid Batch Loss: 2.614572763442993\n",
      "valid Batch Loss: 8.936203002929688\n",
      "valid Batch Loss: 2.849618911743164\n",
      "valid Batch Loss: 2.2825803756713867\n",
      "valid Batch Loss: 3.1611995697021484\n",
      "valid Batch Loss: 2.8188822269439697\n",
      "valid Batch Loss: 50.821067810058594\n",
      "valid Batch Loss: 123.27975463867188\n",
      "valid Batch Loss: 2.964517593383789\n",
      "valid Batch Loss: 2.3466925621032715\n",
      "valid Batch Loss: 2.698519229888916\n",
      "valid Batch Loss: 2.6161224842071533\n",
      "valid Batch Loss: 3.0127851963043213\n",
      "valid Batch Loss: 3.079496383666992\n",
      "valid Batch Loss: 2.461139678955078\n",
      "Epoch: 10 | Train Loss: 0.6337578296661377 | Valid Loss: 16.05156135559082\n",
      "\n",
      "Epoch: 10 | Train AUROC: 0.9567319748176547 | Valid AUROC: 0.9123851750512382\n",
      "\n",
      "Train Batch Loss: 1.5767972469329834\n",
      "Train Batch Loss: 1.8595426082611084\n",
      "Train Batch Loss: 1.156615138053894\n",
      "Train Batch Loss: 1.8327033519744873\n",
      "Train Batch Loss: 1.5537137985229492\n",
      "Train Batch Loss: 0.7132081985473633\n",
      "Train Batch Loss: 1.7294037342071533\n",
      "Train Batch Loss: 1.388639211654663\n",
      "Train Batch Loss: 1.1773828268051147\n",
      "Train Batch Loss: 1.1544790267944336\n",
      "Train Batch Loss: 1.9924744367599487\n",
      "Train Batch Loss: 1.8682537078857422\n",
      "Train Batch Loss: 1.5837839841842651\n",
      "Train Batch Loss: 1.7027943134307861\n",
      "Train Batch Loss: 1.3702945709228516\n",
      "Train Batch Loss: 1.4761314392089844\n",
      "Train Batch Loss: 1.2494373321533203\n",
      "Train Batch Loss: 1.1686148643493652\n",
      "Train Batch Loss: 1.9890809059143066\n",
      "Train Batch Loss: 1.5986396074295044\n",
      "Train Batch Loss: 1.5659258365631104\n",
      "Train Batch Loss: 1.9026739597320557\n",
      "Train Batch Loss: 1.5927112102508545\n",
      "Train Batch Loss: 1.9604414701461792\n",
      "Train Batch Loss: 1.6167948246002197\n",
      "Train Batch Loss: 1.5694713592529297\n",
      "Train Batch Loss: 1.5612473487854004\n",
      "Train Batch Loss: 2.0468251705169678\n",
      "Train Batch Loss: 2.052107572555542\n",
      "Train Batch Loss: 2.103243827819824\n",
      "Train Batch Loss: 1.5340849161148071\n",
      "Train Batch Loss: 1.5321340560913086\n",
      "Train Batch Loss: 1.332942247390747\n",
      "Train Batch Loss: 1.320397973060608\n",
      "Train Batch Loss: 1.6734782457351685\n",
      "Train Batch Loss: 2.1098880767822266\n",
      "Train Batch Loss: 1.975583553314209\n",
      "Train Batch Loss: 1.8433213233947754\n",
      "Train Batch Loss: 1.2773536443710327\n",
      "Train Batch Loss: 1.3858914375305176\n",
      "Train Batch Loss: 1.747644305229187\n",
      "Train Batch Loss: 1.755523443222046\n",
      "Train Batch Loss: 1.6785035133361816\n",
      "Train Batch Loss: 1.7505812644958496\n",
      "Train Batch Loss: 2.145596504211426\n",
      "Train Batch Loss: 1.6139025688171387\n",
      "Train Batch Loss: 1.7647541761398315\n",
      "Train Batch Loss: 1.2160730361938477\n",
      "Train Batch Loss: 1.4975647926330566\n",
      "Train Batch Loss: 1.0724351406097412\n",
      "Train Batch Loss: 2.1374306678771973\n",
      "Train Batch Loss: 1.2166850566864014\n",
      "Train Batch Loss: 1.7659151554107666\n",
      "Train Batch Loss: 2.0229170322418213\n",
      "Train Batch Loss: 1.7286508083343506\n",
      "Train Batch Loss: 1.4340308904647827\n",
      "Train Batch Loss: 1.6513526439666748\n",
      "Train Batch Loss: 1.4891716241836548\n",
      "Train Batch Loss: 1.612051248550415\n",
      "Train Batch Loss: 1.5875859260559082\n",
      "Train Batch Loss: 1.5458941459655762\n",
      "Train Batch Loss: 1.293387532234192\n",
      "Train Batch Loss: 1.303948163986206\n",
      "Train Batch Loss: 1.7077512741088867\n",
      "Train Batch Loss: 1.659342646598816\n",
      "Train Batch Loss: 1.338204026222229\n",
      "Train Batch Loss: 1.6923325061798096\n",
      "Train Batch Loss: 1.5417134761810303\n",
      "Train Batch Loss: 1.4866629838943481\n",
      "Train Batch Loss: 3.765230178833008\n",
      "Train Batch Loss: 1.9796091318130493\n",
      "Train Batch Loss: 1.8602168560028076\n",
      "Train Batch Loss: 1.5526096820831299\n",
      "Train Batch Loss: 1.622913122177124\n",
      "Train Batch Loss: 1.8254916667938232\n",
      "valid Batch Loss: 2.897665500640869\n",
      "valid Batch Loss: 2.8871617317199707\n",
      "valid Batch Loss: 2.444988489151001\n",
      "valid Batch Loss: 2.561338424682617\n",
      "valid Batch Loss: 2.2131052017211914\n",
      "valid Batch Loss: 2.053527355194092\n",
      "valid Batch Loss: 2.310685157775879\n",
      "valid Batch Loss: 2.142087936401367\n",
      "valid Batch Loss: 2.9144132137298584\n",
      "valid Batch Loss: 2.4703376293182373\n",
      "valid Batch Loss: 2.248384952545166\n",
      "valid Batch Loss: 3.356104612350464\n",
      "valid Batch Loss: 2.385801315307617\n",
      "valid Batch Loss: 2.340622663497925\n",
      "valid Batch Loss: 2.8090505599975586\n",
      "valid Batch Loss: 2.294036865234375\n",
      "valid Batch Loss: 2.231649875640869\n",
      "valid Batch Loss: 2.537412405014038\n",
      "valid Batch Loss: 3.1734941005706787\n",
      "valid Batch Loss: 2.4023656845092773\n",
      "valid Batch Loss: 3.1465954780578613\n",
      "valid Batch Loss: 2.4947521686553955\n",
      "valid Batch Loss: 1.8736051321029663\n",
      "valid Batch Loss: 2.3990044593811035\n",
      "valid Batch Loss: 2.451507091522217\n",
      "valid Batch Loss: 2.615997314453125\n",
      "valid Batch Loss: 2.9967844486236572\n",
      "valid Batch Loss: 2.80965518951416\n",
      "valid Batch Loss: 2.7726409435272217\n",
      "valid Batch Loss: 2.5784683227539062\n",
      "valid Batch Loss: 2.3491554260253906\n",
      "valid Batch Loss: 2.5440595149993896\n",
      "valid Batch Loss: 3.349417209625244\n",
      "valid Batch Loss: 2.7800564765930176\n",
      "valid Batch Loss: 2.4421072006225586\n",
      "valid Batch Loss: 3.058382749557495\n",
      "valid Batch Loss: 2.463681221008301\n",
      "valid Batch Loss: 2.3290467262268066\n",
      "valid Batch Loss: 2.3872060775756836\n",
      "valid Batch Loss: 7.257336139678955\n",
      "valid Batch Loss: 2.3749756813049316\n",
      "valid Batch Loss: 3.17961049079895\n",
      "valid Batch Loss: 2.861060857772827\n",
      "valid Batch Loss: 2.436221122741699\n",
      "valid Batch Loss: 2.268474578857422\n",
      "valid Batch Loss: 2.824003219604492\n",
      "valid Batch Loss: 3.1463370323181152\n",
      "valid Batch Loss: 2.73368501663208\n",
      "valid Batch Loss: 2.9070751667022705\n",
      "valid Batch Loss: 2.8531529903411865\n",
      "valid Batch Loss: 2.8924150466918945\n",
      "valid Batch Loss: 2.9983410835266113\n",
      "valid Batch Loss: 2.9540228843688965\n",
      "valid Batch Loss: 2.321197509765625\n",
      "valid Batch Loss: 2.800569534301758\n",
      "valid Batch Loss: 2.692920684814453\n",
      "valid Batch Loss: 2.3735294342041016\n",
      "valid Batch Loss: 2.817883253097534\n",
      "valid Batch Loss: 2.7919297218322754\n",
      "valid Batch Loss: 1.962909460067749\n",
      "valid Batch Loss: 2.9891276359558105\n",
      "valid Batch Loss: 2.6264734268188477\n",
      "valid Batch Loss: 2.08870792388916\n",
      "valid Batch Loss: 2.8404269218444824\n",
      "valid Batch Loss: 2.6981167793273926\n",
      "valid Batch Loss: 2.1525092124938965\n",
      "valid Batch Loss: 2.517573118209839\n",
      "valid Batch Loss: 2.552342653274536\n",
      "valid Batch Loss: 2.813338279724121\n",
      "valid Batch Loss: 2.9747400283813477\n",
      "valid Batch Loss: 2.2496352195739746\n",
      "Epoch: 11 | Train Loss: 0.5692880749702454 | Valid Loss: 2.817314624786377\n",
      "\n",
      "Epoch: 11 | Train AUROC: 0.961454814028285 | Valid AUROC: 0.9192772689451427\n",
      "\n",
      "Train Batch Loss: 1.4323489665985107\n",
      "Train Batch Loss: 2.1464853286743164\n",
      "Train Batch Loss: 1.5146244764328003\n",
      "Train Batch Loss: 1.8925195932388306\n",
      "Train Batch Loss: 1.3315505981445312\n",
      "Train Batch Loss: 1.6372263431549072\n",
      "Train Batch Loss: 1.486139178276062\n",
      "Train Batch Loss: 1.4011459350585938\n",
      "Train Batch Loss: 1.4336376190185547\n",
      "Train Batch Loss: 1.9098578691482544\n",
      "Train Batch Loss: 1.697826623916626\n",
      "Train Batch Loss: 1.5209846496582031\n",
      "Train Batch Loss: 1.2954643964767456\n",
      "Train Batch Loss: 1.3831316232681274\n",
      "Train Batch Loss: 1.5819525718688965\n",
      "Train Batch Loss: 1.9678105115890503\n",
      "Train Batch Loss: 2.696845531463623\n",
      "Train Batch Loss: 1.2569236755371094\n",
      "Train Batch Loss: 1.9405134916305542\n",
      "Train Batch Loss: 1.4988194704055786\n",
      "Train Batch Loss: 1.279451608657837\n",
      "Train Batch Loss: 1.882157325744629\n",
      "Train Batch Loss: 0.8935869336128235\n",
      "Train Batch Loss: 1.460167646408081\n",
      "Train Batch Loss: 1.6925795078277588\n",
      "Train Batch Loss: 2.7509021759033203\n",
      "Train Batch Loss: 1.6267262697219849\n",
      "Train Batch Loss: 1.6570236682891846\n",
      "Train Batch Loss: 2.090789794921875\n",
      "Train Batch Loss: 1.9459741115570068\n",
      "Train Batch Loss: 1.7248286008834839\n",
      "Train Batch Loss: 1.3782209157943726\n",
      "Train Batch Loss: 2.1238327026367188\n",
      "Train Batch Loss: 1.6488821506500244\n",
      "Train Batch Loss: 1.8286347389221191\n",
      "Train Batch Loss: 3.457533359527588\n",
      "Train Batch Loss: 2.3657736778259277\n",
      "Train Batch Loss: 0.9005343914031982\n",
      "Train Batch Loss: 1.4409565925598145\n",
      "Train Batch Loss: 1.3537110090255737\n",
      "Train Batch Loss: 2.0795741081237793\n",
      "Train Batch Loss: 1.18699312210083\n",
      "Train Batch Loss: 1.6466699838638306\n",
      "Train Batch Loss: 1.61149263381958\n",
      "Train Batch Loss: 1.1275116205215454\n",
      "Train Batch Loss: 1.6881381273269653\n",
      "Train Batch Loss: 1.7830898761749268\n",
      "Train Batch Loss: 1.9267754554748535\n",
      "Train Batch Loss: 1.8394354581832886\n",
      "Train Batch Loss: 1.9105563163757324\n",
      "Train Batch Loss: 1.680964469909668\n",
      "Train Batch Loss: 1.4918173551559448\n",
      "Train Batch Loss: 1.4424058198928833\n",
      "Train Batch Loss: 1.1948593854904175\n",
      "Train Batch Loss: 1.4751091003417969\n",
      "Train Batch Loss: 2.0240111351013184\n",
      "Train Batch Loss: 1.2763402462005615\n",
      "Train Batch Loss: 1.5132715702056885\n",
      "Train Batch Loss: 1.766001582145691\n",
      "Train Batch Loss: 1.45737624168396\n",
      "Train Batch Loss: 1.231554388999939\n",
      "Train Batch Loss: 1.6815855503082275\n",
      "Train Batch Loss: 1.2938122749328613\n",
      "Train Batch Loss: 0.8348609209060669\n",
      "Train Batch Loss: 0.911927342414856\n",
      "Train Batch Loss: 2.580132484436035\n",
      "Train Batch Loss: 1.69744873046875\n",
      "Train Batch Loss: 1.0750021934509277\n",
      "Train Batch Loss: 1.6876530647277832\n",
      "Train Batch Loss: 1.2396552562713623\n",
      "Train Batch Loss: 1.4962799549102783\n",
      "Train Batch Loss: 2.2628679275512695\n",
      "Train Batch Loss: 1.9678022861480713\n",
      "Train Batch Loss: 1.53941011428833\n",
      "Train Batch Loss: 1.3939337730407715\n",
      "valid Batch Loss: 2.5108728408813477\n",
      "valid Batch Loss: 2.4567694664001465\n",
      "valid Batch Loss: 2.062089443206787\n",
      "valid Batch Loss: 5.916779518127441\n",
      "valid Batch Loss: 1.9050168991088867\n",
      "valid Batch Loss: 2.203876495361328\n",
      "valid Batch Loss: 2.0577192306518555\n",
      "valid Batch Loss: 1.7835614681243896\n",
      "valid Batch Loss: 2.575199604034424\n",
      "valid Batch Loss: 2.101038932800293\n",
      "valid Batch Loss: 1.9428532123565674\n",
      "valid Batch Loss: 2.413194179534912\n",
      "valid Batch Loss: 2.1155238151550293\n",
      "valid Batch Loss: 1.9511094093322754\n",
      "valid Batch Loss: 2.507716655731201\n",
      "valid Batch Loss: 1.9637866020202637\n",
      "valid Batch Loss: 1.8783881664276123\n",
      "valid Batch Loss: 2.174525737762451\n",
      "valid Batch Loss: 1.7139523029327393\n",
      "valid Batch Loss: 2.1082849502563477\n",
      "valid Batch Loss: 2.7686045169830322\n",
      "valid Batch Loss: 6.3248162269592285\n",
      "valid Batch Loss: 1.6396617889404297\n",
      "valid Batch Loss: 30.112384796142578\n",
      "valid Batch Loss: 3.5249059200286865\n",
      "valid Batch Loss: 2.1860857009887695\n",
      "valid Batch Loss: 2.568523406982422\n",
      "valid Batch Loss: 2.3465938568115234\n",
      "valid Batch Loss: 122.2408676147461\n",
      "valid Batch Loss: 2.132235527038574\n",
      "valid Batch Loss: 2.082165479660034\n",
      "valid Batch Loss: 2.1375749111175537\n",
      "valid Batch Loss: 7.244441986083984\n",
      "valid Batch Loss: 2.395355701446533\n",
      "valid Batch Loss: 2.067143440246582\n",
      "valid Batch Loss: 2.6736817359924316\n",
      "valid Batch Loss: 2.166381359100342\n",
      "valid Batch Loss: 1.940596103668213\n",
      "valid Batch Loss: 2.0579960346221924\n",
      "valid Batch Loss: 6.9658026695251465\n",
      "valid Batch Loss: 8.406754493713379\n",
      "valid Batch Loss: 2.767348527908325\n",
      "valid Batch Loss: 6.525002956390381\n",
      "valid Batch Loss: 2.136176586151123\n",
      "valid Batch Loss: 1.9518153667449951\n",
      "valid Batch Loss: 2.5719916820526123\n",
      "valid Batch Loss: 2.769336462020874\n",
      "valid Batch Loss: 2.311098575592041\n",
      "valid Batch Loss: 2.4588966369628906\n",
      "valid Batch Loss: 2.4041213989257812\n",
      "valid Batch Loss: 2.5281753540039062\n",
      "valid Batch Loss: 2.6649394035339355\n",
      "valid Batch Loss: 8.372377395629883\n",
      "valid Batch Loss: 2.0165276527404785\n",
      "valid Batch Loss: 2.3942151069641113\n",
      "valid Batch Loss: 2.344980239868164\n",
      "valid Batch Loss: 2.0602335929870605\n",
      "valid Batch Loss: 2.4505159854888916\n",
      "valid Batch Loss: 2.4309825897216797\n",
      "valid Batch Loss: 1.690372347831726\n",
      "valid Batch Loss: 2.522942304611206\n",
      "valid Batch Loss: 2.2375869750976562\n",
      "valid Batch Loss: 34.33015441894531\n",
      "valid Batch Loss: 5.645167350769043\n",
      "valid Batch Loss: 2.2759246826171875\n",
      "valid Batch Loss: 1.884859323501587\n",
      "valid Batch Loss: 29.298828125\n",
      "valid Batch Loss: 2.136852741241455\n",
      "valid Batch Loss: 2.439920425415039\n",
      "valid Batch Loss: 2.5437755584716797\n",
      "valid Batch Loss: 1.9090718030929565\n",
      "Epoch: 12 | Train Loss: 0.5341458320617676 | Valid Loss: 9.749747276306152\n",
      "\n",
      "Epoch: 12 | Train AUROC: 0.9629498503080185 | Valid AUROC: 0.9137418312365412\n",
      "\n",
      "Train Batch Loss: 1.4665358066558838\n",
      "Train Batch Loss: 1.3309316635131836\n",
      "Train Batch Loss: 1.7048242092132568\n",
      "Train Batch Loss: 1.0249056816101074\n",
      "Train Batch Loss: 1.7422536611557007\n",
      "Train Batch Loss: 1.9010331630706787\n",
      "Train Batch Loss: 1.6989308595657349\n",
      "Train Batch Loss: 1.5253522396087646\n",
      "Train Batch Loss: 1.8862450122833252\n",
      "Train Batch Loss: 1.916686773300171\n",
      "Train Batch Loss: 2.0083887577056885\n",
      "Train Batch Loss: 1.8681201934814453\n",
      "Train Batch Loss: 1.614428162574768\n",
      "Train Batch Loss: 2.902379035949707\n",
      "Train Batch Loss: 1.54986572265625\n",
      "Train Batch Loss: 1.4169988632202148\n",
      "Train Batch Loss: 1.2319839000701904\n",
      "Train Batch Loss: 1.6950902938842773\n",
      "Train Batch Loss: 1.3140528202056885\n",
      "Train Batch Loss: 1.1202409267425537\n",
      "Train Batch Loss: 1.6259605884552002\n",
      "Train Batch Loss: 1.2547943592071533\n",
      "Train Batch Loss: 1.876137137413025\n",
      "Train Batch Loss: 1.933276653289795\n",
      "Train Batch Loss: 1.5376896858215332\n",
      "Train Batch Loss: 1.1218677759170532\n",
      "Train Batch Loss: 1.8461555242538452\n",
      "Train Batch Loss: 1.4261116981506348\n",
      "Train Batch Loss: 1.1476575136184692\n",
      "Train Batch Loss: 1.5680843591690063\n",
      "Train Batch Loss: 1.3111796379089355\n",
      "Train Batch Loss: 1.2447121143341064\n",
      "Train Batch Loss: 1.2396023273468018\n",
      "Train Batch Loss: 1.2122794389724731\n",
      "Train Batch Loss: 1.042186975479126\n",
      "Train Batch Loss: 3.718153715133667\n",
      "Train Batch Loss: 1.4690449237823486\n",
      "Train Batch Loss: 1.2305810451507568\n",
      "Train Batch Loss: 1.0517010688781738\n",
      "Train Batch Loss: 1.628806710243225\n",
      "Train Batch Loss: 1.7507882118225098\n",
      "Train Batch Loss: 1.4507012367248535\n",
      "Train Batch Loss: 1.389143943786621\n",
      "Train Batch Loss: 1.5326393842697144\n",
      "Train Batch Loss: 1.1766183376312256\n",
      "Train Batch Loss: 1.6419177055358887\n",
      "Train Batch Loss: 1.0256052017211914\n",
      "Train Batch Loss: 1.5956588983535767\n",
      "Train Batch Loss: 1.265366554260254\n",
      "Train Batch Loss: 1.3626368045806885\n",
      "Train Batch Loss: 1.5223774909973145\n",
      "Train Batch Loss: 2.0146305561065674\n",
      "Train Batch Loss: 1.8132444620132446\n",
      "Train Batch Loss: 1.6640887260437012\n",
      "Train Batch Loss: 1.2272292375564575\n",
      "Train Batch Loss: 1.5210496187210083\n",
      "Train Batch Loss: 1.5705071687698364\n",
      "Train Batch Loss: 1.0972076654434204\n",
      "Train Batch Loss: 0.9272044897079468\n",
      "Train Batch Loss: 1.3032959699630737\n",
      "Train Batch Loss: 0.830340564250946\n",
      "Train Batch Loss: 1.0139683485031128\n",
      "Train Batch Loss: 1.0516607761383057\n",
      "Train Batch Loss: 1.4198341369628906\n",
      "Train Batch Loss: 1.5371369123458862\n",
      "Train Batch Loss: 1.3310279846191406\n",
      "Train Batch Loss: 1.5604827404022217\n",
      "Train Batch Loss: 1.8097248077392578\n",
      "Train Batch Loss: 1.5717167854309082\n",
      "Train Batch Loss: 0.9254758358001709\n",
      "Train Batch Loss: 1.5704867839813232\n",
      "Train Batch Loss: 1.3953073024749756\n",
      "Train Batch Loss: 1.5796808004379272\n",
      "Train Batch Loss: 1.0062637329101562\n",
      "Train Batch Loss: 1.4610559940338135\n",
      "valid Batch Loss: 2.667858839035034\n",
      "valid Batch Loss: 2.671009063720703\n",
      "valid Batch Loss: 2.2661948204040527\n",
      "valid Batch Loss: 2.3587355613708496\n",
      "valid Batch Loss: 2.014470338821411\n",
      "valid Batch Loss: 1.8715518712997437\n",
      "valid Batch Loss: 2.170225143432617\n",
      "valid Batch Loss: 1.935664176940918\n",
      "valid Batch Loss: 2.736844062805176\n",
      "valid Batch Loss: 2.2515926361083984\n",
      "valid Batch Loss: 2.1191816329956055\n",
      "valid Batch Loss: 2.600773334503174\n",
      "valid Batch Loss: 2.277759075164795\n",
      "valid Batch Loss: 2.1447267532348633\n",
      "valid Batch Loss: 2.6273694038391113\n",
      "valid Batch Loss: 2.1222171783447266\n",
      "valid Batch Loss: 2.0021915435791016\n",
      "valid Batch Loss: 2.340106964111328\n",
      "valid Batch Loss: 1.884323239326477\n",
      "valid Batch Loss: 2.270282030105591\n",
      "valid Batch Loss: 2.9641222953796387\n",
      "valid Batch Loss: 2.342421054840088\n",
      "valid Batch Loss: 1.782452940940857\n",
      "valid Batch Loss: 2.237537384033203\n",
      "valid Batch Loss: 2.2142252922058105\n",
      "valid Batch Loss: 2.3868956565856934\n",
      "valid Batch Loss: 2.741316318511963\n",
      "valid Batch Loss: 2.568732261657715\n",
      "valid Batch Loss: 2.5047144889831543\n",
      "valid Batch Loss: 2.3535635471343994\n",
      "valid Batch Loss: 2.2323050498962402\n",
      "valid Batch Loss: 2.3198087215423584\n",
      "valid Batch Loss: 3.088418483734131\n",
      "valid Batch Loss: 2.572371482849121\n",
      "valid Batch Loss: 2.2683959007263184\n",
      "valid Batch Loss: 2.823418140411377\n",
      "valid Batch Loss: 2.3044486045837402\n",
      "valid Batch Loss: 2.1044955253601074\n",
      "valid Batch Loss: 2.228952407836914\n",
      "valid Batch Loss: 5.1507391929626465\n",
      "valid Batch Loss: 2.1755480766296387\n",
      "valid Batch Loss: 2.9908578395843506\n",
      "valid Batch Loss: 2.645411491394043\n",
      "valid Batch Loss: 2.302274465560913\n",
      "valid Batch Loss: 2.149909496307373\n",
      "valid Batch Loss: 2.7563507556915283\n",
      "valid Batch Loss: 2.9743332862854004\n",
      "valid Batch Loss: 2.5235509872436523\n",
      "valid Batch Loss: 2.650352954864502\n",
      "valid Batch Loss: 2.627070426940918\n",
      "valid Batch Loss: 2.7429823875427246\n",
      "valid Batch Loss: 2.863741397857666\n",
      "valid Batch Loss: 2.752178430557251\n",
      "valid Batch Loss: 2.1379880905151367\n",
      "valid Batch Loss: 2.6472673416137695\n",
      "valid Batch Loss: 2.5280120372772217\n",
      "valid Batch Loss: 2.228728771209717\n",
      "valid Batch Loss: 2.6250927448272705\n",
      "valid Batch Loss: 2.628715991973877\n",
      "valid Batch Loss: 1.8479483127593994\n",
      "valid Batch Loss: 2.7131989002227783\n",
      "valid Batch Loss: 2.4244351387023926\n",
      "valid Batch Loss: 1.903620958328247\n",
      "valid Batch Loss: 2.70625638961792\n",
      "valid Batch Loss: 2.458448886871338\n",
      "valid Batch Loss: 2.010828733444214\n",
      "valid Batch Loss: 2.3327932357788086\n",
      "valid Batch Loss: 2.3071656227111816\n",
      "valid Batch Loss: 2.621131420135498\n",
      "valid Batch Loss: 3.195101737976074\n",
      "valid Batch Loss: 2.048407793045044\n",
      "Epoch: 13 | Train Loss: 0.4976235330104828 | Valid Loss: 2.7584095001220703\n",
      "\n",
      "Epoch: 13 | Train AUROC: 0.965944195048697 | Valid AUROC: 0.9175244452616238\n",
      "\n",
      "Train Batch Loss: 1.3311693668365479\n",
      "Train Batch Loss: 1.6958928108215332\n",
      "Train Batch Loss: 1.6758015155792236\n",
      "Train Batch Loss: 1.4825836420059204\n",
      "Train Batch Loss: 1.7311458587646484\n",
      "Train Batch Loss: 1.108672857284546\n",
      "Train Batch Loss: 1.0976414680480957\n",
      "Train Batch Loss: 1.6669223308563232\n",
      "Train Batch Loss: 4.13170051574707\n",
      "Train Batch Loss: 1.414632797241211\n",
      "Train Batch Loss: 3.1346588134765625\n",
      "Train Batch Loss: 1.1824021339416504\n",
      "Train Batch Loss: 1.7369399070739746\n",
      "Train Batch Loss: 1.290229320526123\n",
      "Train Batch Loss: 1.4456233978271484\n",
      "Train Batch Loss: 1.559757113456726\n",
      "Train Batch Loss: 1.2329270839691162\n",
      "Train Batch Loss: 1.0965771675109863\n",
      "Train Batch Loss: 1.5191965103149414\n",
      "Train Batch Loss: 1.3044896125793457\n",
      "Train Batch Loss: 1.5050811767578125\n",
      "Train Batch Loss: 1.5155493021011353\n",
      "Train Batch Loss: 1.8314377069473267\n",
      "Train Batch Loss: 1.1268529891967773\n",
      "Train Batch Loss: 1.205980896949768\n",
      "Train Batch Loss: 1.8546267747879028\n",
      "Train Batch Loss: 1.7094308137893677\n",
      "Train Batch Loss: 1.268139362335205\n",
      "Train Batch Loss: 1.1866257190704346\n",
      "Train Batch Loss: 1.1733949184417725\n",
      "Train Batch Loss: 1.0602824687957764\n",
      "Train Batch Loss: 1.821763038635254\n",
      "Train Batch Loss: 2.0103983879089355\n",
      "Train Batch Loss: 1.6328482627868652\n",
      "Train Batch Loss: 1.6818093061447144\n",
      "Train Batch Loss: 1.5113846063613892\n",
      "Train Batch Loss: 1.0863956212997437\n",
      "Train Batch Loss: 1.1174763441085815\n",
      "Train Batch Loss: 1.3160228729248047\n",
      "Train Batch Loss: 4.085243225097656\n",
      "Train Batch Loss: 1.9349122047424316\n",
      "Train Batch Loss: 4.449480056762695\n",
      "Train Batch Loss: 3.806872844696045\n",
      "Train Batch Loss: 3.236245632171631\n",
      "Train Batch Loss: 2.3718442916870117\n",
      "Train Batch Loss: 1.8825335502624512\n",
      "Train Batch Loss: 3.1405129432678223\n",
      "Train Batch Loss: 3.0120582580566406\n",
      "Train Batch Loss: 2.4226624965667725\n",
      "Train Batch Loss: 5.057180404663086\n",
      "Train Batch Loss: 8.254602432250977\n",
      "Train Batch Loss: 3.7167415618896484\n",
      "Train Batch Loss: 16.458467483520508\n",
      "Train Batch Loss: 5.978833198547363\n",
      "Train Batch Loss: 13.457010269165039\n",
      "Train Batch Loss: 10.454168319702148\n",
      "Train Batch Loss: 5.643540382385254\n",
      "Train Batch Loss: 4.2208781242370605\n",
      "Train Batch Loss: 3.8663501739501953\n",
      "Train Batch Loss: 3.955282688140869\n",
      "Train Batch Loss: 3.3617100715637207\n",
      "Train Batch Loss: 3.3666152954101562\n",
      "Train Batch Loss: 6.581635475158691\n",
      "Train Batch Loss: 5.481438636779785\n",
      "Train Batch Loss: 3.1866230964660645\n",
      "Train Batch Loss: 6.019307613372803\n",
      "Train Batch Loss: 3.6621041297912598\n",
      "Train Batch Loss: 3.198021411895752\n",
      "Train Batch Loss: 2.8613505363464355\n",
      "Train Batch Loss: 3.1108274459838867\n",
      "Train Batch Loss: 3.5528926849365234\n",
      "Train Batch Loss: 2.708155870437622\n",
      "Train Batch Loss: 3.1226158142089844\n",
      "Train Batch Loss: 3.238290309906006\n",
      "Train Batch Loss: 3.219223737716675\n",
      "valid Batch Loss: 4.446511745452881\n",
      "valid Batch Loss: 4.417332649230957\n",
      "valid Batch Loss: 4.509667873382568\n",
      "valid Batch Loss: 4.353493690490723\n",
      "valid Batch Loss: 3.6891729831695557\n",
      "valid Batch Loss: 3.627213954925537\n",
      "valid Batch Loss: 3.6529667377471924\n",
      "valid Batch Loss: 4.122806072235107\n",
      "valid Batch Loss: 4.734158992767334\n",
      "valid Batch Loss: 4.323101997375488\n",
      "valid Batch Loss: 4.4350385665893555\n",
      "valid Batch Loss: 4.534905433654785\n",
      "valid Batch Loss: 4.404086112976074\n",
      "valid Batch Loss: 3.858025312423706\n",
      "valid Batch Loss: 4.33404541015625\n",
      "valid Batch Loss: 3.8301825523376465\n",
      "valid Batch Loss: 3.591890335083008\n",
      "valid Batch Loss: 4.931110382080078\n",
      "valid Batch Loss: 4.339938163757324\n",
      "valid Batch Loss: 4.342465400695801\n",
      "valid Batch Loss: 4.772772789001465\n",
      "valid Batch Loss: 4.492425441741943\n",
      "valid Batch Loss: 3.5954675674438477\n",
      "valid Batch Loss: 4.081422805786133\n",
      "valid Batch Loss: 4.430039405822754\n",
      "valid Batch Loss: 4.391994953155518\n",
      "valid Batch Loss: 4.815625190734863\n",
      "valid Batch Loss: 4.408370494842529\n",
      "valid Batch Loss: 4.342226982116699\n",
      "valid Batch Loss: 4.228180408477783\n",
      "valid Batch Loss: 3.9005541801452637\n",
      "valid Batch Loss: 4.046445846557617\n",
      "valid Batch Loss: 4.855716705322266\n",
      "valid Batch Loss: 4.299285411834717\n",
      "valid Batch Loss: 4.170238494873047\n",
      "valid Batch Loss: 4.928302764892578\n",
      "valid Batch Loss: 4.001307010650635\n",
      "valid Batch Loss: 4.020519256591797\n",
      "valid Batch Loss: 3.9839439392089844\n",
      "valid Batch Loss: 4.77422571182251\n",
      "valid Batch Loss: 4.170831680297852\n",
      "valid Batch Loss: 5.06184720993042\n",
      "valid Batch Loss: 4.869002819061279\n",
      "valid Batch Loss: 4.396160125732422\n",
      "valid Batch Loss: 4.046295642852783\n",
      "valid Batch Loss: 4.764615058898926\n",
      "valid Batch Loss: 5.281959533691406\n",
      "valid Batch Loss: 4.726928234100342\n",
      "valid Batch Loss: 5.10525369644165\n",
      "valid Batch Loss: 4.504640102386475\n",
      "valid Batch Loss: 4.429776191711426\n",
      "valid Batch Loss: 4.8329925537109375\n",
      "valid Batch Loss: 4.532092094421387\n",
      "valid Batch Loss: 3.922679901123047\n",
      "valid Batch Loss: 4.712797164916992\n",
      "valid Batch Loss: 4.78999137878418\n",
      "valid Batch Loss: 4.03813362121582\n",
      "valid Batch Loss: 4.448470115661621\n",
      "valid Batch Loss: 4.492547035217285\n",
      "valid Batch Loss: 3.7525837421417236\n",
      "valid Batch Loss: 4.533204078674316\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_validate_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[125], line 63\u001b[0m, in \u001b[0;36mtrain_and_validate_folds\u001b[0;34m(fold)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\n\u001b[1;32m     50\u001b[0m     epochs\n\u001b[1;32m     51\u001b[0m ):  \u001b[38;5;66;03m# reducing epoch to 15 because quick overfitting after correct init\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     model, epoch_loss, epoch_train_auroc \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     53\u001b[0m         model,\n\u001b[1;32m     54\u001b[0m         train_dataloader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m         debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m     61\u001b[0m     )\n\u001b[0;32m---> 63\u001b[0m     model, valid_loss, epoch_valid_auroc \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m     74\u001b[0m         {\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m         }\n\u001b[1;32m     81\u001b[0m     )\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Valid Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[124], line 72\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, dataloader, criterion, optimizer, valid_step, dataset_sizes, debug)\u001b[0m\n\u001b[1;32m     69\u001b[0m gts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (inputs, feats, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m---> 72\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     feats \u001b[38;5;241m=\u001b[39m feats\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     74\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_validate_folds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
