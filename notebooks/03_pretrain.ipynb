{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "\n",
    "import os\n",
    "\n",
    "print(os.cpu_count())\n",
    "\n",
    "import copy\n",
    "import wandb\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torcheval.metrics.functional import binary_auroc\n",
    "\n",
    "from torchvision.models import (\n",
    "    convnext_tiny,\n",
    "    convnext_small,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from colorama import Fore, Style\n",
    "b_ = Fore.BLUE\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = dict(\n",
    "    image_size = 224,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1153511/1246710557.py:1: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_metadata_df = pd.read_csv(\"../data/stratified_5_fold_train_metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "train_metadata_df = pd.read_csv(\"../data/stratified_5_fold_train_metadata.csv\")\n",
    "\n",
    "def add_path(row):\n",
    "    return f\"../data/train-image/image/{row.isic_id}.jpg\"\n",
    "\n",
    "train_metadata_df[\"path\"] = train_metadata_df.apply(lambda row: add_path(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainSkinDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform=None):\n",
    "        assert \"path\" in df.columns\n",
    "        assert \"tbp_lv_symm_2axis\" in df.columns\n",
    "        # TODO: add more features\n",
    "\n",
    "        self.paths = df.path.tolist()\n",
    "        self.tbp_lv_symm_2axis = df.tbp_lv_symm_2axis.tolist() # continuous, float\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image = read_image(self.paths[idx]).to(torch.float32) / 255.0\n",
    "        label = self.tbp_lv_symm_2axis[idx]\n",
    "        if self.transform:\n",
    "            image = image.numpy().transpose((1,2,0))\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_metadata_df.loc[\n",
    "    train_metadata_df.fold != 0\n",
    "]\n",
    "valid_df = train_metadata_df.loc[\n",
    "    train_metadata_df.fold == 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # placeholders\n",
    "# psum = torch.tensor([0.0, 0.0, 0.0])\n",
    "# psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "# num_workers = 24 # based on profiling\n",
    "\n",
    "# simple_transform = A.Compose([\n",
    "#     A.Resize(configs[\"image_size\"], configs[\"image_size\"]),\n",
    "#     ToTensorV2(),\n",
    "# ])\n",
    "\n",
    "# _train_dataset = PretrainSkinDataset(train_df, transform=simple_transform)\n",
    "# _train_dataloader = DataLoader(\n",
    "#     _train_dataset, batch_size=128, shuffle=True,\n",
    "#     num_workers=num_workers, pin_memory=True, persistent_workers=True\n",
    "# )\n",
    "\n",
    "# # loop through images\n",
    "# for inputs, labels in tqdm(_train_dataloader):\n",
    "#     psum += inputs.sum(axis=[0, 2, 3])\n",
    "#     psum_sq += (inputs**2).sum(axis=[0, 2, 3])\n",
    "\n",
    "# count = len(train_df) * configs[\"image_size\"] * configs[\"image_size\"]\n",
    "# total_mean = psum / count\n",
    "# total_var = (psum_sq / count) - (total_mean**2)\n",
    "# total_std = torch.sqrt(total_var)\n",
    "# print(\"mean: \" + str(total_mean))\n",
    "# print(\"std:  \" + str(total_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/pydantic/main.py:164: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/pydantic/main.py:308: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `Union[float, json-or-python[json=list[float], python=list[float]]]` but got `tuple` - serialized value may not be as expected\n",
      "  Expected `Union[float, json-or-python[json=list[float], python=list[float]]]` but got `tuple` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "transforms_train = A.Compose([\n",
    "    A.Transpose(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.75),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=5),\n",
    "        A.MedianBlur(blur_limit=5),\n",
    "        A.GaussianBlur(blur_limit=5),\n",
    "        A.GaussNoise(var_limit=(5.0, 30.0)),\n",
    "    ], p=0.7),\n",
    "\n",
    "    A.OneOf([\n",
    "        A.OpticalDistortion(distort_limit=1.0),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=1.),\n",
    "        A.ElasticTransform(alpha=3),\n",
    "    ], p=0.7),\n",
    "    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n",
    "    A.Resize(configs[\"image_size\"], configs[\"image_size\"]),\n",
    "    A.Normalize(\n",
    "        mean=(0.6962, 0.5209, 0.4193),\n",
    "        std=(0.1395, 0.1320, 0.1240)\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "transforms_val = A.Compose([\n",
    "    A.Resize(configs[\"image_size\"], configs[\"image_size\"]),\n",
    "    A.Normalize(\n",
    "        mean=(0.6962, 0.5209, 0.4193),\n",
    "        std=(0.1395, 0.1320, 0.1240)\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate statistics\n",
    "# mean_train = train_df['tbp_lv_symm_2axis'].mean()\n",
    "# median_train = train_df['tbp_lv_symm_2axis'].median()\n",
    "# mean_valid = valid_df['tbp_lv_symm_2axis'].mean()\n",
    "# median_valid = valid_df['tbp_lv_symm_2axis'].median()\n",
    "\n",
    "# # Plot the distributions\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(train_df['tbp_lv_symm_2axis'], color='blue', label='Train Set', kde=True, alpha=0.5)\n",
    "# sns.histplot(valid_df['tbp_lv_symm_2axis'], color='green', label='Validation Set', kde=True, alpha=0.5)\n",
    "\n",
    "# # Add mean and median lines and annotations\n",
    "# plt.axvline(mean_train, color='blue', linestyle='dashed', linewidth=1)\n",
    "# plt.axvline(median_train, color='blue', linestyle='solid', linewidth=1)\n",
    "# plt.axvline(mean_valid, color='green', linestyle='dashed', linewidth=1)\n",
    "# plt.axvline(median_valid, color='green', linestyle='solid', linewidth=1)\n",
    "\n",
    "# plt.text(mean_train, plt.ylim()[1]*0.8, f'Mean: {mean_train:.2f}', color='blue', ha='center')\n",
    "# plt.text(median_train, plt.ylim()[1]*0.7, f'Median: {median_train:.2f}', color='blue', ha='center')\n",
    "# plt.text(mean_valid, plt.ylim()[1]*0.6, f'Mean: {mean_valid:.2f}', color='green', ha='center')\n",
    "# plt.text(median_valid, plt.ylim()[1]*0.5, f'Median: {median_valid:.2f}', color='green', ha='center')\n",
    "\n",
    "# # Add title and labels\n",
    "# plt.title('Distribution of tbp_lv_symm_2axis in Train and Validation Sets')\n",
    "# plt.xlabel('tbp_lv_symm_2axis')\n",
    "# plt.ylabel('Density')\n",
    "# plt.legend()\n",
    "\n",
    "# # Show plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 320848, 'val': 80211}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_workers = 24 # based on profiling\n",
    "\n",
    "train_dataset = PretrainSkinDataset(train_df, transform=transforms_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True, persistent_workers=True, drop_last=True)\n",
    "\n",
    "valid_dataset = PretrainSkinDataset(valid_df, transform=transforms_val)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=128, shuffle=False, num_workers=num_workers, pin_memory=True, persistent_workers=True, drop_last=True)\n",
    "\n",
    "dataset_sizes = {\"train\": len(train_dataset), \"val\": len(valid_dataset)}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): ConvNeXt(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "        (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "        )\n",
      "        (1): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
      "        )\n",
      "        (2): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
      "        )\n",
      "        (1): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
      "        )\n",
      "        (2): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
      "        )\n",
      "        (1): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
      "        )\n",
      "        (2): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
      "        )\n",
      "        (3): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
      "        )\n",
      "        (4): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
      "        )\n",
      "        (5): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
      "        )\n",
      "        (6): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
      "        )\n",
      "        (7): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
      "        )\n",
      "        (8): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
      "        )\n",
      "        (1): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
      "        )\n",
      "        (2): CNBlock(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (1): Permute()\n",
      "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (4): GELU(approximate='none')\n",
      "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (6): Permute()\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (classifier): Sequential(\n",
      "      (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): Linear(in_features=768, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_ft = convnext_tiny()\n",
    "model_ft.classifier[2] = nn.Linear(model_ft.classifier[2].in_features, 1, bias=False)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "model_ft = torch.compile(model_ft)\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, scheduler=None):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).flatten().to(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs).flatten()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_val = loss.detach()\n",
    "        running_loss += loss_val * inputs.size(0)\n",
    "\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            wandb.log({\"train_loss\": loss_val})\n",
    "            print(f\"Train Batch Loss: {loss_val}\")\n",
    "\n",
    "    epoch_loss = running_loss / dataset_sizes[\"train\"]\n",
    "\n",
    "    return model, epoch_loss\n",
    "\n",
    "\n",
    "def validate_model(model, dataloader, criterion, optimizer):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).flatten().to(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs).flatten()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        loss_val = loss.detach()\n",
    "        running_loss += loss_val * inputs.size(0)\n",
    "\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            wandb.log({\"valid_loss\": loss_val})\n",
    "            print(f\"valid Batch Loss: {loss_val}\")\n",
    "\n",
    "    valid_loss = running_loss / dataset_sizes[\"val\"]\n",
    "\n",
    "    return model, valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mayush-thakur\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/ayusht/skin/notebooks/wandb/run-20240723_080451-puix1h96</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ayush-thakur/isic_lesions_24/runs/puix1h96' target=\"_blank\">prime-haze-747</a></strong> to <a href='https://wandb.ai/ayush-thakur/isic_lesions_24' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ayush-thakur/isic_lesions_24' target=\"_blank\">https://wandb.ai/ayush-thakur/isic_lesions_24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ayush-thakur/isic_lesions_24/runs/puix1h96' target=\"_blank\">https://wandb.ai/ayush-thakur/isic_lesions_24/runs/puix1h96</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch Loss: 0.9938347339630127\n",
      "Train Batch Loss: 0.29654261469841003\n",
      "Train Batch Loss: 0.07255585491657257\n",
      "Train Batch Loss: 0.024559348821640015\n",
      "Train Batch Loss: 0.016417246311903\n",
      "Train Batch Loss: 0.013899106532335281\n",
      "Train Batch Loss: 0.018576525151729584\n",
      "Train Batch Loss: 0.015132731758058071\n",
      "Train Batch Loss: 0.015436557121574879\n",
      "Train Batch Loss: 0.01929567940533161\n",
      "Train Batch Loss: 0.0200906191021204\n",
      "Train Batch Loss: 0.014607090502977371\n",
      "Train Batch Loss: 0.014009620994329453\n",
      "Train Batch Loss: 0.012998992577195168\n",
      "Train Batch Loss: 0.015194405801594257\n",
      "Train Batch Loss: 0.014454683288931847\n",
      "Train Batch Loss: 0.01786598190665245\n",
      "Train Batch Loss: 0.016805382445454597\n",
      "Train Batch Loss: 0.01873870939016342\n",
      "Train Batch Loss: 0.017156973481178284\n",
      "Train Batch Loss: 0.015766769647598267\n",
      "Train Batch Loss: 0.015033743344247341\n",
      "Train Batch Loss: 0.016078319400548935\n",
      "Train Batch Loss: 0.015765275806188583\n",
      "Train Batch Loss: 0.014872428961098194\n",
      "Train Batch Loss: 0.014920013956725597\n",
      "Train Batch Loss: 0.014410954900085926\n",
      "Train Batch Loss: 0.017955781891942024\n",
      "Train Batch Loss: 0.015797358006238937\n",
      "Train Batch Loss: 0.0143160130828619\n",
      "Train Batch Loss: 0.012835323810577393\n",
      "Train Batch Loss: 0.0153695372864604\n",
      "Train Batch Loss: 0.016377519816160202\n",
      "Train Batch Loss: 0.012825982645154\n",
      "Train Batch Loss: 0.013675812631845474\n",
      "Train Batch Loss: 0.014380600303411484\n",
      "Train Batch Loss: 0.01513539720326662\n",
      "Train Batch Loss: 0.01838872954249382\n",
      "Train Batch Loss: 0.012989628128707409\n",
      "Train Batch Loss: 0.01842571422457695\n",
      "Train Batch Loss: 0.015787333250045776\n",
      "Train Batch Loss: 0.013787692412734032\n",
      "Train Batch Loss: 0.02005111426115036\n",
      "Train Batch Loss: 0.015564661473035812\n",
      "Train Batch Loss: 0.014675738289952278\n",
      "Train Batch Loss: 0.01330573670566082\n",
      "Train Batch Loss: 0.02083863876760006\n",
      "Train Batch Loss: 0.013010689988732338\n",
      "Train Batch Loss: 0.012743829749524593\n",
      "Train Batch Loss: 0.016820888966321945\n",
      "Train Batch Loss: 0.015652360394597054\n",
      "Train Batch Loss: 0.015161433257162571\n",
      "Train Batch Loss: 0.012270605191588402\n",
      "Train Batch Loss: 0.026328884065151215\n",
      "Train Batch Loss: 0.01800122670829296\n",
      "Train Batch Loss: 0.015070982277393341\n",
      "Train Batch Loss: 0.01402239128947258\n",
      "Train Batch Loss: 0.01433548890054226\n",
      "Train Batch Loss: 0.015271252021193504\n",
      "Train Batch Loss: 0.012438539415597916\n",
      "Train Batch Loss: 0.019886888563632965\n",
      "Train Batch Loss: 0.015583089552819729\n",
      "Train Batch Loss: 0.015352839604020119\n",
      "Train Batch Loss: 0.014101872220635414\n",
      "Train Batch Loss: 0.018124140799045563\n",
      "Train Batch Loss: 0.01441681943833828\n",
      "Train Batch Loss: 0.015401712618768215\n",
      "Train Batch Loss: 0.01290852576494217\n",
      "Train Batch Loss: 0.017310841009020805\n",
      "Train Batch Loss: 0.01462375745177269\n",
      "Train Batch Loss: 0.012318423949182034\n",
      "Train Batch Loss: 0.019122932106256485\n",
      "Train Batch Loss: 0.017034661024808884\n",
      "Train Batch Loss: 0.01563636213541031\n",
      "Train Batch Loss: 0.014285675249993801\n",
      "Train Batch Loss: 0.013628936372697353\n",
      "Train Batch Loss: 0.017824823036789894\n",
      "Train Batch Loss: 0.014312071725726128\n",
      "Train Batch Loss: 0.015331648290157318\n",
      "Train Batch Loss: 0.018665291368961334\n",
      "Train Batch Loss: 0.015372239053249359\n",
      "Train Batch Loss: 0.01701912097632885\n",
      "Train Batch Loss: 0.015067482367157936\n",
      "Train Batch Loss: 0.015035688877105713\n",
      "Train Batch Loss: 0.01696106605231762\n",
      "Train Batch Loss: 0.012519627809524536\n",
      "Train Batch Loss: 0.012176640331745148\n",
      "Train Batch Loss: 0.018277190625667572\n",
      "Train Batch Loss: 0.015731651335954666\n",
      "Train Batch Loss: 0.014119650237262249\n",
      "Train Batch Loss: 0.01732579991221428\n",
      "Train Batch Loss: 0.016317788511514664\n",
      "Train Batch Loss: 0.013051960617303848\n",
      "Train Batch Loss: 0.020734569057822227\n",
      "Train Batch Loss: 0.01670161634683609\n",
      "Train Batch Loss: 0.014086375012993813\n",
      "Train Batch Loss: 0.01368673425167799\n",
      "Train Batch Loss: 0.015241183340549469\n",
      "Train Batch Loss: 0.018104327842593193\n",
      "Train Batch Loss: 0.01542647648602724\n",
      "Train Batch Loss: 0.01898089051246643\n",
      "Train Batch Loss: 0.015273259952664375\n",
      "Train Batch Loss: 0.014813730493187904\n",
      "Train Batch Loss: 0.017340540885925293\n",
      "Train Batch Loss: 0.020224206149578094\n",
      "Train Batch Loss: 0.015590647235512733\n",
      "Train Batch Loss: 0.01680601015686989\n",
      "Train Batch Loss: 0.014085988514125347\n",
      "Train Batch Loss: 0.015515749342739582\n",
      "Train Batch Loss: 0.01673494465649128\n",
      "Train Batch Loss: 0.021258294582366943\n",
      "Train Batch Loss: 0.015901833772659302\n",
      "Train Batch Loss: 0.01768472231924534\n",
      "Train Batch Loss: 0.017230302095413208\n",
      "Train Batch Loss: 0.013490980491042137\n",
      "Train Batch Loss: 0.013304576277732849\n",
      "Train Batch Loss: 0.016991455107927322\n",
      "Train Batch Loss: 0.01906454563140869\n",
      "Train Batch Loss: 0.018678929656744003\n",
      "Train Batch Loss: 0.01645423285663128\n",
      "Train Batch Loss: 0.012946738861501217\n",
      "Train Batch Loss: 0.017109639942646027\n",
      "Train Batch Loss: 0.01671268604695797\n",
      "Train Batch Loss: 0.01994428038597107\n",
      "Train Batch Loss: 0.014445474371314049\n",
      "Train Batch Loss: 0.014678086154162884\n",
      "Train Batch Loss: 0.015645617619156837\n",
      "Train Batch Loss: 0.01237812265753746\n",
      "Train Batch Loss: 0.01532265730202198\n",
      "Train Batch Loss: 0.018239930272102356\n",
      "Train Batch Loss: 0.013385817408561707\n",
      "Train Batch Loss: 0.017802853137254715\n",
      "Train Batch Loss: 0.019611645489931107\n",
      "Train Batch Loss: 0.014459334313869476\n",
      "Train Batch Loss: 0.015677617862820625\n",
      "Train Batch Loss: 0.01733982004225254\n",
      "Train Batch Loss: 0.020015813410282135\n",
      "Train Batch Loss: 0.012562707997858524\n",
      "Train Batch Loss: 0.014737613499164581\n",
      "Train Batch Loss: 0.016067856922745705\n",
      "Train Batch Loss: 0.012222719378769398\n",
      "Train Batch Loss: 0.017591288313269615\n",
      "Train Batch Loss: 0.015424727462232113\n",
      "Train Batch Loss: 0.013699269853532314\n",
      "Train Batch Loss: 0.01641169749200344\n",
      "Train Batch Loss: 0.016858946532011032\n",
      "Train Batch Loss: 0.016299143433570862\n",
      "Train Batch Loss: 0.016005344688892365\n",
      "Train Batch Loss: 0.01795535907149315\n",
      "Train Batch Loss: 0.020231762900948524\n",
      "Train Batch Loss: 0.017253104597330093\n",
      "Train Batch Loss: 0.014960741624236107\n",
      "Train Batch Loss: 0.014813161455094814\n",
      "Train Batch Loss: 0.01637568324804306\n",
      "Train Batch Loss: 0.014669293537735939\n",
      "Train Batch Loss: 0.013881606049835682\n",
      "Train Batch Loss: 0.017218150198459625\n",
      "Train Batch Loss: 0.017232447862625122\n",
      "Train Batch Loss: 0.016709983348846436\n",
      "Train Batch Loss: 0.013162809424102306\n",
      "Train Batch Loss: 0.016837386414408684\n",
      "Train Batch Loss: 0.01392417773604393\n",
      "Train Batch Loss: 0.020168770104646683\n",
      "Train Batch Loss: 0.020730096846818924\n",
      "Train Batch Loss: 0.015679573640227318\n",
      "Train Batch Loss: 0.01304933987557888\n",
      "Train Batch Loss: 0.013998184353113174\n",
      "Train Batch Loss: 0.019684243947267532\n",
      "Train Batch Loss: 0.016270898282527924\n",
      "Train Batch Loss: 0.020510610193014145\n",
      "Train Batch Loss: 0.015393816865980625\n",
      "Train Batch Loss: 0.011148310266435146\n",
      "Train Batch Loss: 0.015673160552978516\n",
      "Train Batch Loss: 0.016378678381443024\n",
      "Train Batch Loss: 0.016313279047608376\n",
      "Train Batch Loss: 0.017772480845451355\n",
      "Train Batch Loss: 0.015861783176660538\n",
      "Train Batch Loss: 0.017055228352546692\n",
      "Train Batch Loss: 0.019140709191560745\n",
      "Train Batch Loss: 0.024329673498868942\n",
      "Train Batch Loss: 0.021357042714953423\n",
      "Train Batch Loss: 0.014505219645798206\n",
      "Train Batch Loss: 0.013511239551007748\n",
      "Train Batch Loss: 0.01354534737765789\n",
      "Train Batch Loss: 0.016198765486478806\n",
      "Train Batch Loss: 0.014560230076313019\n",
      "Train Batch Loss: 0.01703065261244774\n",
      "Train Batch Loss: 0.015335057862102985\n",
      "Train Batch Loss: 0.018110554665327072\n",
      "Train Batch Loss: 0.01525728590786457\n",
      "Train Batch Loss: 0.017681339755654335\n",
      "Train Batch Loss: 0.019416511058807373\n",
      "Train Batch Loss: 0.018179215490818024\n",
      "Train Batch Loss: 0.011922801844775677\n",
      "Train Batch Loss: 0.017283961176872253\n",
      "Train Batch Loss: 0.0153860654681921\n",
      "Train Batch Loss: 0.01614314317703247\n",
      "Train Batch Loss: 0.01834634318947792\n",
      "Train Batch Loss: 0.01575048454105854\n",
      "Train Batch Loss: 0.01697421446442604\n",
      "Train Batch Loss: 0.01632808707654476\n",
      "Train Batch Loss: 0.013883380219340324\n",
      "Train Batch Loss: 0.0169389545917511\n",
      "Train Batch Loss: 0.018413156270980835\n",
      "Train Batch Loss: 0.016977360472083092\n",
      "Train Batch Loss: 0.01233980618417263\n",
      "Train Batch Loss: 0.01950233429670334\n",
      "Train Batch Loss: 0.018775828182697296\n",
      "Train Batch Loss: 0.014878151938319206\n",
      "Train Batch Loss: 0.01706014946103096\n",
      "Train Batch Loss: 0.0252276211977005\n",
      "Train Batch Loss: 0.015084600076079369\n",
      "Train Batch Loss: 0.01871899515390396\n",
      "Train Batch Loss: 0.014608068391680717\n",
      "Train Batch Loss: 0.01841418817639351\n",
      "Train Batch Loss: 0.01680031418800354\n",
      "Train Batch Loss: 0.017572812736034393\n",
      "Train Batch Loss: 0.01264311745762825\n",
      "Train Batch Loss: 0.01473281066864729\n",
      "Train Batch Loss: 0.01735006645321846\n",
      "Train Batch Loss: 0.015942048281431198\n",
      "Train Batch Loss: 0.015331664122641087\n",
      "Train Batch Loss: 0.014536475762724876\n",
      "Train Batch Loss: 0.015361668542027473\n",
      "Train Batch Loss: 0.01279835682362318\n",
      "Train Batch Loss: 0.016968579962849617\n",
      "Train Batch Loss: 0.01414601318538189\n",
      "Train Batch Loss: 0.015423135831952095\n",
      "Train Batch Loss: 0.011261101812124252\n",
      "Train Batch Loss: 0.018121793866157532\n",
      "Train Batch Loss: 0.014621160924434662\n",
      "Train Batch Loss: 0.024097461253404617\n",
      "Train Batch Loss: 0.020832335576415062\n",
      "Train Batch Loss: 0.0204889215528965\n",
      "Train Batch Loss: 0.016788646578788757\n",
      "Train Batch Loss: 0.014644448645412922\n",
      "Train Batch Loss: 0.018683435395359993\n",
      "Train Batch Loss: 0.017172742635011673\n",
      "Train Batch Loss: 0.011535841971635818\n",
      "Train Batch Loss: 0.012125836685299873\n",
      "Train Batch Loss: 0.01725838892161846\n",
      "Train Batch Loss: 0.015410283580422401\n",
      "Train Batch Loss: 0.01609448343515396\n",
      "Train Batch Loss: 0.014765528030693531\n",
      "Train Batch Loss: 0.016214173287153244\n",
      "Train Batch Loss: 0.01827865280210972\n",
      "Train Batch Loss: 0.017638444900512695\n",
      "Train Batch Loss: 0.016808461397886276\n",
      "Train Batch Loss: 0.016448024660348892\n",
      "Train Batch Loss: 0.01527818851172924\n",
      "valid Batch Loss: 0.019430551677942276\n",
      "valid Batch Loss: 0.016749581322073936\n",
      "valid Batch Loss: 0.02009584940969944\n",
      "valid Batch Loss: 0.01863912120461464\n",
      "valid Batch Loss: 0.011007419787347317\n",
      "valid Batch Loss: 0.019457025453448296\n",
      "valid Batch Loss: 0.011879116296768188\n",
      "valid Batch Loss: 0.016617849469184875\n",
      "valid Batch Loss: 0.015461750328540802\n",
      "valid Batch Loss: 0.016617923974990845\n",
      "valid Batch Loss: 0.017431655898690224\n",
      "valid Batch Loss: 0.015502996742725372\n",
      "valid Batch Loss: 0.014022458344697952\n",
      "valid Batch Loss: 0.014342330396175385\n",
      "valid Batch Loss: 0.017071425914764404\n",
      "valid Batch Loss: 0.01589319482445717\n",
      "valid Batch Loss: 0.021613575518131256\n",
      "valid Batch Loss: 0.012234564870595932\n",
      "valid Batch Loss: 0.019431309774518013\n",
      "valid Batch Loss: 0.014736578799784184\n",
      "valid Batch Loss: 0.016057582572102547\n",
      "valid Batch Loss: 0.015455000102519989\n",
      "valid Batch Loss: 0.01389281079173088\n",
      "valid Batch Loss: 0.017002679407596588\n",
      "valid Batch Loss: 0.012208409607410431\n",
      "valid Batch Loss: 0.016792815178632736\n",
      "valid Batch Loss: 0.015682954341173172\n",
      "valid Batch Loss: 0.017329487949609756\n",
      "valid Batch Loss: 0.016778327524662018\n",
      "valid Batch Loss: 0.015104847028851509\n",
      "valid Batch Loss: 0.014089616015553474\n",
      "valid Batch Loss: 0.01551582757383585\n",
      "valid Batch Loss: 0.01885579526424408\n",
      "valid Batch Loss: 0.0126870758831501\n",
      "valid Batch Loss: 0.017648305743932724\n",
      "valid Batch Loss: 0.016566339880228043\n",
      "valid Batch Loss: 0.019504882395267487\n",
      "valid Batch Loss: 0.019537411630153656\n",
      "valid Batch Loss: 0.01806892268359661\n",
      "valid Batch Loss: 0.0150008425116539\n",
      "valid Batch Loss: 0.01376931183040142\n",
      "valid Batch Loss: 0.012868908233940601\n",
      "valid Batch Loss: 0.014506414532661438\n",
      "valid Batch Loss: 0.017632927745580673\n",
      "valid Batch Loss: 0.012328614480793476\n",
      "valid Batch Loss: 0.018591338768601418\n",
      "valid Batch Loss: 0.015662413090467453\n",
      "valid Batch Loss: 0.018567588180303574\n",
      "valid Batch Loss: 0.016989650204777718\n",
      "valid Batch Loss: 0.02074233442544937\n",
      "valid Batch Loss: 0.015365460887551308\n",
      "valid Batch Loss: 0.01616411656141281\n",
      "valid Batch Loss: 0.018664665520191193\n",
      "valid Batch Loss: 0.015504530631005764\n",
      "valid Batch Loss: 0.020983733236789703\n",
      "valid Batch Loss: 0.015489265322685242\n",
      "valid Batch Loss: 0.013834312558174133\n",
      "valid Batch Loss: 0.013935348950326443\n",
      "valid Batch Loss: 0.015231145545840263\n",
      "valid Batch Loss: 0.014443756081163883\n",
      "valid Batch Loss: 0.01475625578314066\n",
      "valid Batch Loss: 0.013453809544444084\n",
      "Epoch: 0 | Train Loss: 0.07928711920976639 | Valid Loss: 0.016421154141426086\n",
      "\n",
      "\u001b[34mValidation Loss Improved (inf ---> 0.016421154141426086)\u001b[0m\n",
      "\u001b[34mModel Saved\u001b[0m\n",
      "Train Batch Loss: 0.02236887626349926\n",
      "Train Batch Loss: 0.02041718177497387\n",
      "Train Batch Loss: 0.018550900742411613\n",
      "Train Batch Loss: 0.01742880418896675\n",
      "Train Batch Loss: 0.020448584109544754\n",
      "Train Batch Loss: 0.015529105439782143\n",
      "Train Batch Loss: 0.019903166219592094\n",
      "Train Batch Loss: 0.015628211200237274\n",
      "Train Batch Loss: 0.016350075602531433\n",
      "Train Batch Loss: 0.014642532914876938\n",
      "Train Batch Loss: 0.01667642593383789\n",
      "Train Batch Loss: 0.02088601514697075\n",
      "Train Batch Loss: 0.01537984050810337\n",
      "Train Batch Loss: 0.01611895114183426\n",
      "Train Batch Loss: 0.01480121910572052\n",
      "Train Batch Loss: 0.01851969212293625\n",
      "Train Batch Loss: 0.013375228270888329\n",
      "Train Batch Loss: 0.015355056151747704\n",
      "Train Batch Loss: 0.016317717730998993\n",
      "Train Batch Loss: 0.01821359246969223\n",
      "Train Batch Loss: 0.019505955278873444\n",
      "Train Batch Loss: 0.014157155528664589\n",
      "Train Batch Loss: 0.022970478981733322\n",
      "Train Batch Loss: 0.025481920689344406\n",
      "Train Batch Loss: 0.024131258949637413\n",
      "Train Batch Loss: 0.01364624872803688\n",
      "Train Batch Loss: 0.01476489007472992\n",
      "Train Batch Loss: 0.017386864870786667\n",
      "Train Batch Loss: 0.017924141138792038\n",
      "Train Batch Loss: 0.016210954636335373\n",
      "Train Batch Loss: 0.012997657060623169\n",
      "Train Batch Loss: 0.015079355798661709\n",
      "Train Batch Loss: 0.01777632161974907\n",
      "Train Batch Loss: 0.013516618870198727\n",
      "Train Batch Loss: 0.015980681404471397\n",
      "Train Batch Loss: 0.014644146896898746\n",
      "Train Batch Loss: 0.01536398846656084\n",
      "Train Batch Loss: 0.015005995519459248\n",
      "Train Batch Loss: 0.012856487184762955\n",
      "Train Batch Loss: 0.01822410151362419\n",
      "Train Batch Loss: 0.014230303466320038\n",
      "Train Batch Loss: 0.018123000860214233\n",
      "Train Batch Loss: 0.013465587981045246\n",
      "Train Batch Loss: 0.018324850127100945\n",
      "Train Batch Loss: 0.01741693913936615\n",
      "Train Batch Loss: 0.0172401275485754\n",
      "Train Batch Loss: 0.014306839555501938\n",
      "Train Batch Loss: 0.01756436936557293\n",
      "Train Batch Loss: 0.02045261859893799\n",
      "Train Batch Loss: 0.013351758942008018\n",
      "Train Batch Loss: 0.015184042043983936\n",
      "Train Batch Loss: 0.016496408730745316\n",
      "Train Batch Loss: 0.018142953515052795\n",
      "Train Batch Loss: 0.013867278583347797\n",
      "Train Batch Loss: 0.016771674156188965\n",
      "Train Batch Loss: 0.01626209169626236\n",
      "Train Batch Loss: 0.014687618240714073\n",
      "Train Batch Loss: 0.0195385180413723\n",
      "Train Batch Loss: 0.018142346292734146\n",
      "Train Batch Loss: 0.014507720246911049\n",
      "Train Batch Loss: 0.015492845326662064\n",
      "Train Batch Loss: 0.014780786819756031\n",
      "Train Batch Loss: 0.014264233410358429\n",
      "Train Batch Loss: 0.013931140303611755\n",
      "Train Batch Loss: 0.01825350522994995\n",
      "Train Batch Loss: 0.018843386322259903\n",
      "Train Batch Loss: 0.015575738623738289\n",
      "Train Batch Loss: 0.022435132414102554\n",
      "Train Batch Loss: 0.01557762362062931\n",
      "Train Batch Loss: 0.016351690515875816\n",
      "Train Batch Loss: 0.016691341996192932\n",
      "Train Batch Loss: 0.020138446241617203\n",
      "Train Batch Loss: 0.01906193047761917\n",
      "Train Batch Loss: 0.02233288437128067\n",
      "Train Batch Loss: 0.016079019755125046\n",
      "Train Batch Loss: 0.014937152154743671\n",
      "Train Batch Loss: 0.015487363561987877\n",
      "Train Batch Loss: 0.019055340439081192\n",
      "Train Batch Loss: 0.016016611829400063\n",
      "Train Batch Loss: 0.016833048313856125\n",
      "Train Batch Loss: 0.014208284206688404\n",
      "Train Batch Loss: 0.017220495268702507\n",
      "Train Batch Loss: 0.017103929072618484\n",
      "Train Batch Loss: 0.017565269023180008\n",
      "Train Batch Loss: 0.015547323040664196\n",
      "Train Batch Loss: 0.01668403670191765\n",
      "Train Batch Loss: 0.014812426641583443\n",
      "Train Batch Loss: 0.015439040958881378\n",
      "Train Batch Loss: 0.018563997000455856\n",
      "Train Batch Loss: 0.018308699131011963\n",
      "Train Batch Loss: 0.013488752767443657\n",
      "Train Batch Loss: 0.013684830628335476\n",
      "Train Batch Loss: 0.016354918479919434\n",
      "Train Batch Loss: 0.019548185169696808\n",
      "Train Batch Loss: 0.01484405156224966\n",
      "Train Batch Loss: 0.013572124764323235\n",
      "Train Batch Loss: 0.011956891044974327\n",
      "Train Batch Loss: 0.012580272741615772\n",
      "Train Batch Loss: 0.02814031019806862\n",
      "Train Batch Loss: 0.016034645959734917\n",
      "Train Batch Loss: 0.017845548689365387\n",
      "Train Batch Loss: 0.017636926844716072\n",
      "Train Batch Loss: 0.013603736646473408\n",
      "Train Batch Loss: 0.016864031553268433\n",
      "Train Batch Loss: 0.01904997229576111\n",
      "Train Batch Loss: 0.014192751608788967\n",
      "Train Batch Loss: 0.021101169288158417\n",
      "Train Batch Loss: 0.01733962446451187\n",
      "Train Batch Loss: 0.014353742823004723\n",
      "Train Batch Loss: 0.020550094544887543\n",
      "Train Batch Loss: 0.015206973999738693\n",
      "Train Batch Loss: 0.019353192299604416\n",
      "Train Batch Loss: 0.015536466613411903\n",
      "Train Batch Loss: 0.016407176852226257\n",
      "Train Batch Loss: 0.013654010370373726\n",
      "Train Batch Loss: 0.018040068447589874\n",
      "Train Batch Loss: 0.01891981065273285\n",
      "Train Batch Loss: 0.014031784608960152\n",
      "Train Batch Loss: 0.018541226163506508\n",
      "Train Batch Loss: 0.015486021526157856\n",
      "Train Batch Loss: 0.017096154391765594\n",
      "Train Batch Loss: 0.017992980778217316\n",
      "Train Batch Loss: 0.016462935134768486\n",
      "Train Batch Loss: 0.017385799437761307\n",
      "Train Batch Loss: 0.01154346764087677\n",
      "Train Batch Loss: 0.019780360162258148\n",
      "Train Batch Loss: 0.01869194582104683\n",
      "Train Batch Loss: 0.019120018929243088\n",
      "Train Batch Loss: 0.016766391694545746\n",
      "Train Batch Loss: 0.013892179355025291\n",
      "Train Batch Loss: 0.01627982035279274\n",
      "Train Batch Loss: 0.01844913139939308\n",
      "Train Batch Loss: 0.014362076297402382\n",
      "Train Batch Loss: 0.01782514713704586\n",
      "Train Batch Loss: 0.018645234405994415\n",
      "Train Batch Loss: 0.015909582376480103\n",
      "Train Batch Loss: 0.0239717997610569\n",
      "Train Batch Loss: 0.019762307405471802\n",
      "Train Batch Loss: 0.015193799510598183\n",
      "Train Batch Loss: 0.017000772058963776\n",
      "Train Batch Loss: 0.017556121572852135\n",
      "Train Batch Loss: 0.024699106812477112\n",
      "Train Batch Loss: 0.012804381549358368\n",
      "Train Batch Loss: 0.017651723697781563\n",
      "Train Batch Loss: 0.014495310373604298\n",
      "Train Batch Loss: 0.015127941966056824\n",
      "Train Batch Loss: 0.014938696287572384\n",
      "Train Batch Loss: 0.019744716584682465\n",
      "Train Batch Loss: 0.015016120858490467\n",
      "Train Batch Loss: 0.02525368146598339\n",
      "Train Batch Loss: 0.018285509198904037\n",
      "Train Batch Loss: 0.02007392793893814\n",
      "Train Batch Loss: 0.019241221249103546\n",
      "Train Batch Loss: 0.015030523762106895\n",
      "Train Batch Loss: 0.013031539507210255\n",
      "Train Batch Loss: 0.013899199664592743\n",
      "Train Batch Loss: 0.015113327652215958\n",
      "Train Batch Loss: 0.018708743155002594\n",
      "Train Batch Loss: 0.013142506591975689\n",
      "Train Batch Loss: 0.015109196305274963\n",
      "Train Batch Loss: 0.017461547628045082\n",
      "Train Batch Loss: 0.01395399123430252\n",
      "Train Batch Loss: 0.014636363834142685\n",
      "Train Batch Loss: 0.014191520400345325\n",
      "Train Batch Loss: 0.016035404056310654\n",
      "Train Batch Loss: 0.017036672681570053\n",
      "Train Batch Loss: 0.017873113974928856\n",
      "Train Batch Loss: 0.01824955642223358\n",
      "Train Batch Loss: 0.014848100021481514\n",
      "Train Batch Loss: 0.01819514110684395\n",
      "Train Batch Loss: 0.02231055498123169\n",
      "Train Batch Loss: 0.017073357477784157\n",
      "Train Batch Loss: 0.01549664605408907\n",
      "Train Batch Loss: 0.020432692021131516\n",
      "Train Batch Loss: 0.011443780735135078\n",
      "Train Batch Loss: 0.015413601882755756\n",
      "Train Batch Loss: 0.017622414976358414\n",
      "Train Batch Loss: 0.01710326224565506\n",
      "Train Batch Loss: 0.012641912326216698\n",
      "Train Batch Loss: 0.014859789051115513\n",
      "Train Batch Loss: 0.016348598524928093\n",
      "Train Batch Loss: 0.01849633827805519\n",
      "Train Batch Loss: 0.014166256412863731\n",
      "Train Batch Loss: 0.014568749815225601\n",
      "Train Batch Loss: 0.013148099184036255\n",
      "Train Batch Loss: 0.017465585842728615\n",
      "Train Batch Loss: 0.01784098707139492\n",
      "Train Batch Loss: 0.015953024849295616\n",
      "Train Batch Loss: 0.015077002346515656\n",
      "Train Batch Loss: 0.017151344567537308\n",
      "Train Batch Loss: 0.036289118230342865\n",
      "Train Batch Loss: 0.023574424907565117\n",
      "Train Batch Loss: 0.015661926940083504\n",
      "Train Batch Loss: 0.01830480620265007\n",
      "Train Batch Loss: 0.019862312823534012\n",
      "Train Batch Loss: 0.02183631993830204\n",
      "Train Batch Loss: 0.016678040847182274\n",
      "Train Batch Loss: 0.011627213098108768\n",
      "Train Batch Loss: 0.016339322552084923\n",
      "Train Batch Loss: 0.016001973301172256\n",
      "Train Batch Loss: 0.02407684177160263\n",
      "Train Batch Loss: 0.01790604740381241\n",
      "Train Batch Loss: 0.01865595206618309\n",
      "Train Batch Loss: 0.01721637137234211\n",
      "Train Batch Loss: 0.016838500276207924\n",
      "Train Batch Loss: 0.013602424412965775\n",
      "Train Batch Loss: 0.014766741544008255\n",
      "Train Batch Loss: 0.017062894999980927\n",
      "Train Batch Loss: 0.017760468646883965\n",
      "Train Batch Loss: 0.018720438703894615\n",
      "Train Batch Loss: 0.019710853695869446\n",
      "Train Batch Loss: 0.028631791472434998\n",
      "Train Batch Loss: 0.013854135759174824\n",
      "Train Batch Loss: 0.017894700169563293\n",
      "Train Batch Loss: 0.013798684813082218\n",
      "Train Batch Loss: 0.01743258163332939\n",
      "Train Batch Loss: 0.018293457105755806\n",
      "Train Batch Loss: 0.019461121410131454\n",
      "Train Batch Loss: 0.014905733987689018\n",
      "Train Batch Loss: 0.0181825440376997\n",
      "Train Batch Loss: 0.017672311514616013\n",
      "Train Batch Loss: 0.016319194808602333\n",
      "Train Batch Loss: 0.021556705236434937\n",
      "Train Batch Loss: 0.01735055260360241\n",
      "Train Batch Loss: 0.0249225702136755\n",
      "Train Batch Loss: 0.01894635707139969\n",
      "Train Batch Loss: 0.013874625787138939\n",
      "Train Batch Loss: 0.01666630432009697\n",
      "Train Batch Loss: 0.01625649444758892\n",
      "Train Batch Loss: 0.015762576833367348\n",
      "Train Batch Loss: 0.018146688118577003\n",
      "Train Batch Loss: 0.020680751651525497\n",
      "Train Batch Loss: 0.02579234354197979\n",
      "Train Batch Loss: 0.019077612087130547\n",
      "Train Batch Loss: 0.019730012863874435\n",
      "Train Batch Loss: 0.018167048692703247\n",
      "Train Batch Loss: 0.015440067276358604\n",
      "Train Batch Loss: 0.018007228150963783\n",
      "Train Batch Loss: 0.013865356333553791\n",
      "Train Batch Loss: 0.015925303101539612\n",
      "Train Batch Loss: 0.01751263625919819\n",
      "Train Batch Loss: 0.01411181129515171\n",
      "Train Batch Loss: 0.018873197957873344\n",
      "Train Batch Loss: 0.01931733638048172\n",
      "Train Batch Loss: 0.013098416849970818\n",
      "Train Batch Loss: 0.018105242401361465\n",
      "Train Batch Loss: 0.011392539367079735\n",
      "Train Batch Loss: 0.014850078150629997\n",
      "Train Batch Loss: 0.01586856320500374\n",
      "Train Batch Loss: 0.015597558580338955\n",
      "valid Batch Loss: 0.01875969208776951\n",
      "valid Batch Loss: 0.01674891635775566\n",
      "valid Batch Loss: 0.019283732399344444\n",
      "valid Batch Loss: 0.01825074665248394\n",
      "valid Batch Loss: 0.012223439291119576\n",
      "valid Batch Loss: 0.01818579062819481\n",
      "valid Batch Loss: 0.012698695063591003\n",
      "valid Batch Loss: 0.016481947153806686\n",
      "valid Batch Loss: 0.015747874975204468\n",
      "valid Batch Loss: 0.016556713730096817\n",
      "valid Batch Loss: 0.017355263233184814\n",
      "valid Batch Loss: 0.015412596054375172\n",
      "valid Batch Loss: 0.013703567907214165\n",
      "valid Batch Loss: 0.014858249574899673\n",
      "valid Batch Loss: 0.016755448654294014\n",
      "valid Batch Loss: 0.015498965978622437\n",
      "valid Batch Loss: 0.021809717640280724\n",
      "valid Batch Loss: 0.012996545061469078\n",
      "valid Batch Loss: 0.019976668059825897\n",
      "valid Batch Loss: 0.014517640694975853\n",
      "valid Batch Loss: 0.015919258818030357\n",
      "valid Batch Loss: 0.01513549592345953\n",
      "valid Batch Loss: 0.01364409551024437\n",
      "valid Batch Loss: 0.016643762588500977\n",
      "valid Batch Loss: 0.012328816577792168\n",
      "valid Batch Loss: 0.016534043475985527\n",
      "valid Batch Loss: 0.015940511599183083\n",
      "valid Batch Loss: 0.017394889146089554\n",
      "valid Batch Loss: 0.017092835158109665\n",
      "valid Batch Loss: 0.01525992900133133\n",
      "valid Batch Loss: 0.014853769913315773\n",
      "valid Batch Loss: 0.014779013581573963\n",
      "valid Batch Loss: 0.018046751618385315\n",
      "valid Batch Loss: 0.012721136212348938\n",
      "valid Batch Loss: 0.017657529562711716\n",
      "valid Batch Loss: 0.01706685498356819\n",
      "valid Batch Loss: 0.01834983006119728\n",
      "valid Batch Loss: 0.018821049481630325\n",
      "valid Batch Loss: 0.018183745443820953\n",
      "valid Batch Loss: 0.014992672950029373\n",
      "valid Batch Loss: 0.014283777214586735\n",
      "valid Batch Loss: 0.013574142009019852\n",
      "valid Batch Loss: 0.014836492016911507\n",
      "valid Batch Loss: 0.017693128436803818\n",
      "valid Batch Loss: 0.01316971518099308\n",
      "valid Batch Loss: 0.017336562275886536\n",
      "valid Batch Loss: 0.015758946537971497\n",
      "valid Batch Loss: 0.018102914094924927\n",
      "valid Batch Loss: 0.017659194767475128\n",
      "valid Batch Loss: 0.01925203949213028\n",
      "valid Batch Loss: 0.015464797616004944\n",
      "valid Batch Loss: 0.016234003007411957\n",
      "valid Batch Loss: 0.018317170441150665\n",
      "valid Batch Loss: 0.015465601347386837\n",
      "valid Batch Loss: 0.02027450129389763\n",
      "valid Batch Loss: 0.014969785697758198\n",
      "valid Batch Loss: 0.014170350506901741\n",
      "valid Batch Loss: 0.013327708467841148\n",
      "valid Batch Loss: 0.015030743554234505\n",
      "valid Batch Loss: 0.014786237850785255\n",
      "valid Batch Loss: 0.014537476003170013\n",
      "valid Batch Loss: 0.01401001401245594\n",
      "Epoch: 1 | Train Loss: 0.01707148551940918 | Valid Loss: 0.016295844689011574\n",
      "\n",
      "\u001b[34mValidation Loss Improved (0.016421154141426086 ---> 0.016295844689011574)\u001b[0m\n",
      "\u001b[34mModel Saved\u001b[0m\n",
      "Train Batch Loss: 0.017015136778354645\n",
      "Train Batch Loss: 0.015858877450227737\n",
      "Train Batch Loss: 0.013055134564638138\n",
      "Train Batch Loss: 0.014070034958422184\n",
      "Train Batch Loss: 0.016326898708939552\n",
      "Train Batch Loss: 0.016620056703686714\n",
      "Train Batch Loss: 0.015765110030770302\n",
      "Train Batch Loss: 0.019414156675338745\n",
      "Train Batch Loss: 0.017640598118305206\n",
      "Train Batch Loss: 0.0174407996237278\n",
      "Train Batch Loss: 0.015837542712688446\n",
      "Train Batch Loss: 0.017497468739748\n",
      "Train Batch Loss: 0.01763657107949257\n",
      "Train Batch Loss: 0.013117817230522633\n",
      "Train Batch Loss: 0.013546296395361423\n",
      "Train Batch Loss: 0.01666758954524994\n",
      "Train Batch Loss: 0.016680235043168068\n",
      "Train Batch Loss: 0.015389343723654747\n",
      "Train Batch Loss: 0.024035470560193062\n",
      "Train Batch Loss: 0.01584351435303688\n",
      "Train Batch Loss: 0.01443568430840969\n",
      "Train Batch Loss: 0.01457655057311058\n",
      "Train Batch Loss: 0.013648908585309982\n",
      "Train Batch Loss: 0.017290398478507996\n",
      "Train Batch Loss: 0.01671113446354866\n",
      "Train Batch Loss: 0.020079366862773895\n",
      "Train Batch Loss: 0.016001541167497635\n",
      "Train Batch Loss: 0.014489680528640747\n",
      "Train Batch Loss: 0.01717906817793846\n",
      "Train Batch Loss: 0.018418846651911736\n",
      "Train Batch Loss: 0.010284412652254105\n",
      "Train Batch Loss: 0.015763254836201668\n",
      "Train Batch Loss: 0.017841031774878502\n",
      "Train Batch Loss: 0.017034558579325676\n",
      "Train Batch Loss: 0.017597882077097893\n",
      "Train Batch Loss: 0.014136945828795433\n",
      "Train Batch Loss: 0.016345568001270294\n",
      "Train Batch Loss: 0.014427952468395233\n",
      "Train Batch Loss: 0.01689198426902294\n",
      "Train Batch Loss: 0.015720004215836525\n",
      "Train Batch Loss: 0.01860211417078972\n",
      "Train Batch Loss: 0.016726113855838776\n",
      "Train Batch Loss: 0.014460353180766106\n",
      "Train Batch Loss: 0.0167060699313879\n",
      "Train Batch Loss: 0.01689586415886879\n",
      "Train Batch Loss: 0.011715198867022991\n",
      "Train Batch Loss: 0.01707066223025322\n",
      "Train Batch Loss: 0.015617022290825844\n",
      "Train Batch Loss: 0.01902519352734089\n",
      "Train Batch Loss: 0.016653593629598618\n",
      "Train Batch Loss: 0.015485982410609722\n",
      "Train Batch Loss: 0.016894064843654633\n",
      "Train Batch Loss: 0.014602772891521454\n",
      "Train Batch Loss: 0.012081494554877281\n",
      "Train Batch Loss: 0.016570061445236206\n",
      "Train Batch Loss: 0.014608565717935562\n",
      "Train Batch Loss: 0.015273230150341988\n",
      "Train Batch Loss: 0.020639218389987946\n",
      "Train Batch Loss: 0.012286334298551083\n",
      "Train Batch Loss: 0.013708950951695442\n",
      "Train Batch Loss: 0.014992216601967812\n",
      "Train Batch Loss: 0.01750141941010952\n",
      "Train Batch Loss: 0.014607222750782967\n",
      "Train Batch Loss: 0.011406785808503628\n",
      "Train Batch Loss: 0.015673723071813583\n",
      "Train Batch Loss: 0.014993702992796898\n",
      "Train Batch Loss: 0.013025153428316116\n",
      "Train Batch Loss: 0.018259156495332718\n",
      "Train Batch Loss: 0.01720082014799118\n",
      "Train Batch Loss: 0.013026752509176731\n",
      "Train Batch Loss: 0.012747340835630894\n",
      "Train Batch Loss: 0.02189137227833271\n",
      "Train Batch Loss: 0.015246454626321793\n",
      "Train Batch Loss: 0.016334015876054764\n",
      "Train Batch Loss: 0.014742711558938026\n",
      "Train Batch Loss: 0.015338028781116009\n",
      "Train Batch Loss: 0.012558849528431892\n",
      "Train Batch Loss: 0.014390825293958187\n",
      "Train Batch Loss: 0.023277603089809418\n",
      "Train Batch Loss: 0.012861339375376701\n",
      "Train Batch Loss: 0.015734568238258362\n",
      "Train Batch Loss: 0.016271859407424927\n",
      "Train Batch Loss: 0.015612220391631126\n",
      "Train Batch Loss: 0.013661812990903854\n",
      "Train Batch Loss: 0.015248676761984825\n",
      "Train Batch Loss: 0.017807744443416595\n",
      "Train Batch Loss: 0.014260376803576946\n",
      "Train Batch Loss: 0.014406556263566017\n",
      "Train Batch Loss: 0.014544072560966015\n",
      "Train Batch Loss: 0.015924174338579178\n",
      "Train Batch Loss: 0.015636758878827095\n",
      "Train Batch Loss: 0.013848984614014626\n",
      "Train Batch Loss: 0.015012679621577263\n",
      "Train Batch Loss: 0.02278311550617218\n",
      "Train Batch Loss: 0.017738180235028267\n",
      "Train Batch Loss: 0.015564761124551296\n",
      "Train Batch Loss: 0.018490638583898544\n",
      "Train Batch Loss: 0.014207568019628525\n",
      "Train Batch Loss: 0.015151390805840492\n",
      "Train Batch Loss: 0.014973186887800694\n",
      "Train Batch Loss: 0.01794048771262169\n",
      "Train Batch Loss: 0.01697561889886856\n",
      "Train Batch Loss: 0.015467232093214989\n",
      "Train Batch Loss: 0.019409624859690666\n",
      "Train Batch Loss: 0.01683088019490242\n",
      "Train Batch Loss: 0.01706762984395027\n",
      "Train Batch Loss: 0.017343638464808464\n",
      "Train Batch Loss: 0.01329312939196825\n",
      "Train Batch Loss: 0.016340035945177078\n",
      "Train Batch Loss: 0.016418153420090675\n",
      "Train Batch Loss: 0.015691466629505157\n",
      "Train Batch Loss: 0.019479235634207726\n",
      "Train Batch Loss: 0.014389083720743656\n",
      "Train Batch Loss: 0.017430780455470085\n",
      "Train Batch Loss: 0.017273934558033943\n",
      "Train Batch Loss: 0.017160844057798386\n",
      "Train Batch Loss: 0.02230694517493248\n",
      "Train Batch Loss: 0.014530275948345661\n",
      "Train Batch Loss: 0.016550865024328232\n",
      "Train Batch Loss: 0.013551278971135616\n",
      "Train Batch Loss: 0.016063904389739037\n",
      "Train Batch Loss: 0.017636818811297417\n",
      "Train Batch Loss: 0.016141097992658615\n",
      "Train Batch Loss: 0.021623266860842705\n",
      "Train Batch Loss: 0.016281556338071823\n",
      "Train Batch Loss: 0.016990546137094498\n",
      "Train Batch Loss: 0.018163232132792473\n",
      "Train Batch Loss: 0.014929402619600296\n",
      "Train Batch Loss: 0.016000589355826378\n",
      "Train Batch Loss: 0.014861847274005413\n",
      "Train Batch Loss: 0.019631249830126762\n",
      "Train Batch Loss: 0.017284594476222992\n",
      "Train Batch Loss: 0.0191135723143816\n",
      "Train Batch Loss: 0.015925586223602295\n",
      "Train Batch Loss: 0.017496390268206596\n",
      "Train Batch Loss: 0.016742456704378128\n",
      "Train Batch Loss: 0.019903067499399185\n",
      "Train Batch Loss: 0.02303396165370941\n",
      "Train Batch Loss: 0.018236888572573662\n",
      "Train Batch Loss: 0.013775831088423729\n",
      "Train Batch Loss: 0.02167743444442749\n",
      "Train Batch Loss: 0.01745918020606041\n",
      "Train Batch Loss: 0.014634983614087105\n",
      "Train Batch Loss: 0.013431845232844353\n",
      "Train Batch Loss: 0.016701843589544296\n",
      "Train Batch Loss: 0.01513077411800623\n",
      "Train Batch Loss: 0.021901242434978485\n",
      "Train Batch Loss: 0.016253255307674408\n",
      "Train Batch Loss: 0.019648412242531776\n",
      "Train Batch Loss: 0.013784756883978844\n",
      "Train Batch Loss: 0.01840764656662941\n",
      "Train Batch Loss: 0.016216900199651718\n",
      "Train Batch Loss: 0.015557519160211086\n",
      "Train Batch Loss: 0.015740130096673965\n",
      "Train Batch Loss: 0.023236222565174103\n",
      "Train Batch Loss: 0.016167977824807167\n",
      "Train Batch Loss: 0.016818154603242874\n",
      "Train Batch Loss: 0.017726149410009384\n",
      "Train Batch Loss: 0.01672477088868618\n",
      "Train Batch Loss: 0.018418211489915848\n",
      "Train Batch Loss: 0.013691185042262077\n",
      "Train Batch Loss: 0.01766144670546055\n",
      "Train Batch Loss: 0.014609809033572674\n",
      "Train Batch Loss: 0.01453146431595087\n",
      "Train Batch Loss: 0.01648431271314621\n",
      "Train Batch Loss: 0.020952053368091583\n",
      "Train Batch Loss: 0.01600859872996807\n",
      "Train Batch Loss: 0.018794625997543335\n",
      "Train Batch Loss: 0.013576967641711235\n",
      "Train Batch Loss: 0.014730509370565414\n",
      "Train Batch Loss: 0.016938019543886185\n",
      "Train Batch Loss: 0.014960046857595444\n",
      "Train Batch Loss: 0.012210693210363388\n",
      "Train Batch Loss: 0.01762155443429947\n",
      "Train Batch Loss: 0.019552048295736313\n",
      "Train Batch Loss: 0.0171014666557312\n",
      "Train Batch Loss: 0.01540418341755867\n",
      "Train Batch Loss: 0.015787135809659958\n",
      "Train Batch Loss: 0.0201474167406559\n",
      "Train Batch Loss: 0.01995660923421383\n",
      "Train Batch Loss: 0.01983485370874405\n",
      "Train Batch Loss: 0.013337299227714539\n",
      "Train Batch Loss: 0.012493185698986053\n",
      "Train Batch Loss: 0.015861758962273598\n",
      "Train Batch Loss: 0.016807250678539276\n",
      "Train Batch Loss: 0.012908218428492546\n",
      "Train Batch Loss: 0.01345094945281744\n",
      "Train Batch Loss: 0.013747207820415497\n",
      "Train Batch Loss: 0.012776615098118782\n",
      "Train Batch Loss: 0.014043407514691353\n",
      "Train Batch Loss: 0.01405941043049097\n",
      "Train Batch Loss: 0.0178823359310627\n",
      "Train Batch Loss: 0.011653590016067028\n",
      "Train Batch Loss: 0.019768336787819862\n",
      "Train Batch Loss: 0.02764434739947319\n",
      "Train Batch Loss: 0.016236962750554085\n",
      "Train Batch Loss: 0.008796488866209984\n",
      "Train Batch Loss: 0.01655234396457672\n",
      "Train Batch Loss: 0.01278337836265564\n",
      "Train Batch Loss: 0.01681622862815857\n",
      "Train Batch Loss: 0.020418766885995865\n",
      "Train Batch Loss: 0.016471017152071\n",
      "Train Batch Loss: 0.017821073532104492\n",
      "Train Batch Loss: 0.018365513533353806\n",
      "Train Batch Loss: 0.016559578478336334\n",
      "Train Batch Loss: 0.019960742443799973\n",
      "Train Batch Loss: 0.015517406165599823\n",
      "Train Batch Loss: 0.014376203529536724\n",
      "Train Batch Loss: 0.013669307343661785\n",
      "Train Batch Loss: 0.018375463783740997\n",
      "Train Batch Loss: 0.015490714460611343\n",
      "Train Batch Loss: 0.01580897346138954\n",
      "Train Batch Loss: 0.0180318895727396\n",
      "Train Batch Loss: 0.01996120810508728\n",
      "Train Batch Loss: 0.01512391772121191\n",
      "Train Batch Loss: 0.015332480892539024\n",
      "Train Batch Loss: 0.017472609877586365\n",
      "Train Batch Loss: 0.015916582196950912\n",
      "Train Batch Loss: 0.013827474787831306\n",
      "Train Batch Loss: 0.01085661444813013\n",
      "Train Batch Loss: 0.01368076540529728\n",
      "Train Batch Loss: 0.014065848663449287\n",
      "Train Batch Loss: 0.01692279800772667\n",
      "Train Batch Loss: 0.015168830752372742\n",
      "Train Batch Loss: 0.016531849279999733\n",
      "Train Batch Loss: 0.013018039986491203\n",
      "Train Batch Loss: 0.01936849020421505\n",
      "Train Batch Loss: 0.013907101936638355\n",
      "Train Batch Loss: 0.018679169937968254\n",
      "Train Batch Loss: 0.013440633192658424\n",
      "Train Batch Loss: 0.015402693301439285\n",
      "Train Batch Loss: 0.012606306001543999\n",
      "Train Batch Loss: 0.016595449298620224\n",
      "Train Batch Loss: 0.01342648733407259\n",
      "Train Batch Loss: 0.013109455816447735\n",
      "Train Batch Loss: 0.014692382887005806\n",
      "Train Batch Loss: 0.016482766717672348\n",
      "Train Batch Loss: 0.015370170585811138\n",
      "Train Batch Loss: 0.012948775663971901\n",
      "Train Batch Loss: 0.013222623616456985\n",
      "Train Batch Loss: 0.018340669572353363\n",
      "Train Batch Loss: 0.011518939398229122\n",
      "Train Batch Loss: 0.01877635344862938\n",
      "Train Batch Loss: 0.014280709438025951\n",
      "Train Batch Loss: 0.016926968470215797\n",
      "Train Batch Loss: 0.015104743652045727\n",
      "Train Batch Loss: 0.015658974647521973\n",
      "Train Batch Loss: 0.011489062570035458\n",
      "Train Batch Loss: 0.012153731659054756\n",
      "Train Batch Loss: 0.0173041932284832\n",
      "valid Batch Loss: 0.018991289660334587\n",
      "valid Batch Loss: 0.01748863235116005\n",
      "valid Batch Loss: 0.01940828375518322\n",
      "valid Batch Loss: 0.0186966210603714\n",
      "valid Batch Loss: 0.013885734602808952\n",
      "valid Batch Loss: 0.01796276494860649\n",
      "valid Batch Loss: 0.01406033430248499\n",
      "valid Batch Loss: 0.017119314521551132\n",
      "valid Batch Loss: 0.01670510694384575\n",
      "valid Batch Loss: 0.017250891774892807\n",
      "valid Batch Loss: 0.01803778111934662\n",
      "valid Batch Loss: 0.016084585338830948\n",
      "valid Batch Loss: 0.014202103018760681\n",
      "valid Batch Loss: 0.015989739447832108\n",
      "valid Batch Loss: 0.01725633814930916\n",
      "valid Batch Loss: 0.015940679237246513\n",
      "valid Batch Loss: 0.02269844524562359\n",
      "valid Batch Loss: 0.014314631000161171\n",
      "valid Batch Loss: 0.021130532026290894\n",
      "valid Batch Loss: 0.015091928653419018\n",
      "valid Batch Loss: 0.016554836183786392\n",
      "valid Batch Loss: 0.015633808448910713\n",
      "valid Batch Loss: 0.014195483177900314\n",
      "valid Batch Loss: 0.017112083733081818\n",
      "valid Batch Loss: 0.013160470873117447\n",
      "valid Batch Loss: 0.017078371718525887\n",
      "valid Batch Loss: 0.016875924542546272\n",
      "valid Batch Loss: 0.018184738233685493\n",
      "valid Batch Loss: 0.018071481958031654\n",
      "valid Batch Loss: 0.01611774042248726\n",
      "valid Batch Loss: 0.0161735936999321\n",
      "valid Batch Loss: 0.014960813336074352\n",
      "valid Batch Loss: 0.01817364990711212\n",
      "valid Batch Loss: 0.013487249612808228\n",
      "valid Batch Loss: 0.018404878675937653\n",
      "valid Batch Loss: 0.018186558037996292\n",
      "valid Batch Loss: 0.01821471005678177\n",
      "valid Batch Loss: 0.01901828870177269\n",
      "valid Batch Loss: 0.019011180847883224\n",
      "valid Batch Loss: 0.015726853162050247\n",
      "valid Batch Loss: 0.015414333902299404\n",
      "valid Batch Loss: 0.014849172905087471\n",
      "valid Batch Loss: 0.015826981514692307\n",
      "valid Batch Loss: 0.018479369580745697\n",
      "valid Batch Loss: 0.01454758457839489\n",
      "valid Batch Loss: 0.017125649377703667\n",
      "valid Batch Loss: 0.01657247357070446\n",
      "valid Batch Loss: 0.018491122871637344\n",
      "valid Batch Loss: 0.01890726201236248\n",
      "valid Batch Loss: 0.018862653523683548\n",
      "valid Batch Loss: 0.016280408948659897\n",
      "valid Batch Loss: 0.017027664929628372\n",
      "valid Batch Loss: 0.018794115632772446\n",
      "valid Batch Loss: 0.01617652177810669\n",
      "valid Batch Loss: 0.02047734335064888\n",
      "valid Batch Loss: 0.015316260047256947\n",
      "valid Batch Loss: 0.015165524557232857\n",
      "valid Batch Loss: 0.01360778883099556\n",
      "valid Batch Loss: 0.015618974342942238\n",
      "valid Batch Loss: 0.015786394476890564\n",
      "valid Batch Loss: 0.015111839398741722\n",
      "valid Batch Loss: 0.015171956270933151\n",
      "Epoch: 2 | Train Loss: 0.01627008244395256 | Valid Loss: 0.016940422356128693\n",
      "\n",
      "Train Batch Loss: 0.01821069046854973\n",
      "Train Batch Loss: 0.015268530696630478\n",
      "Train Batch Loss: 0.014019523747265339\n",
      "Train Batch Loss: 0.015559948980808258\n",
      "Train Batch Loss: 0.016132866963744164\n",
      "Train Batch Loss: 0.015731018036603928\n",
      "Train Batch Loss: 0.01889810897409916\n",
      "Train Batch Loss: 0.01640555076301098\n",
      "Train Batch Loss: 0.016209708526730537\n",
      "Train Batch Loss: 0.01683688350021839\n",
      "Train Batch Loss: 0.01615927740931511\n",
      "Train Batch Loss: 0.01671658456325531\n",
      "Train Batch Loss: 0.02304942160844803\n",
      "Train Batch Loss: 0.014643333852291107\n",
      "Train Batch Loss: 0.015781328082084656\n",
      "Train Batch Loss: 0.015171335078775883\n",
      "Train Batch Loss: 0.015642693266272545\n",
      "Train Batch Loss: 0.014012373052537441\n",
      "Train Batch Loss: 0.01498344261199236\n",
      "Train Batch Loss: 0.015252173878252506\n",
      "Train Batch Loss: 0.017240172252058983\n",
      "Train Batch Loss: 0.016275763511657715\n",
      "Train Batch Loss: 0.016400203108787537\n",
      "Train Batch Loss: 0.013766015879809856\n",
      "Train Batch Loss: 0.016473323106765747\n",
      "Train Batch Loss: 0.015091200359165668\n",
      "Train Batch Loss: 0.01635342836380005\n",
      "Train Batch Loss: 0.015859253704547882\n",
      "Train Batch Loss: 0.015746809542179108\n",
      "Train Batch Loss: 0.01553838886320591\n",
      "Train Batch Loss: 0.015318846330046654\n",
      "Train Batch Loss: 0.017410818487405777\n",
      "Train Batch Loss: 0.015872718766331673\n",
      "Train Batch Loss: 0.012391669675707817\n",
      "Train Batch Loss: 0.015738939866423607\n",
      "Train Batch Loss: 0.013721629045903683\n",
      "Train Batch Loss: 0.010691295377910137\n",
      "Train Batch Loss: 0.016847167164087296\n",
      "Train Batch Loss: 0.01630876585841179\n",
      "Train Batch Loss: 0.012818542309105396\n",
      "Train Batch Loss: 0.014020205475389957\n",
      "Train Batch Loss: 0.014694418758153915\n",
      "Train Batch Loss: 0.016787463799118996\n",
      "Train Batch Loss: 0.016190366819500923\n",
      "Train Batch Loss: 0.01440762635320425\n",
      "Train Batch Loss: 0.017493821680545807\n",
      "Train Batch Loss: 0.014767673797905445\n",
      "Train Batch Loss: 0.015148158185184002\n",
      "Train Batch Loss: 0.014407054521143436\n",
      "Train Batch Loss: 0.011260749772191048\n",
      "Train Batch Loss: 0.014328585006296635\n",
      "Train Batch Loss: 0.018740424886345863\n",
      "Train Batch Loss: 0.01734376698732376\n",
      "Train Batch Loss: 0.015911715105175972\n",
      "Train Batch Loss: 0.016780052334070206\n",
      "Train Batch Loss: 0.018000368028879166\n",
      "Train Batch Loss: 0.013938365504145622\n",
      "Train Batch Loss: 0.017513029277324677\n",
      "Train Batch Loss: 0.01519288681447506\n",
      "Train Batch Loss: 0.016087505966424942\n",
      "Train Batch Loss: 0.015796784311532974\n",
      "Train Batch Loss: 0.013188157230615616\n",
      "Train Batch Loss: 0.01645655184984207\n",
      "Train Batch Loss: 0.013870746828615665\n",
      "Train Batch Loss: 0.01478757243603468\n",
      "Train Batch Loss: 0.01759345456957817\n",
      "Train Batch Loss: 0.011963093653321266\n",
      "Train Batch Loss: 0.013360993936657906\n",
      "Train Batch Loss: 0.014260649681091309\n",
      "Train Batch Loss: 0.013782230205833912\n",
      "Train Batch Loss: 0.01873418502509594\n",
      "Train Batch Loss: 0.012709937989711761\n",
      "Train Batch Loss: 0.016745246946811676\n",
      "Train Batch Loss: 0.012036088854074478\n",
      "Train Batch Loss: 0.02081073820590973\n",
      "Train Batch Loss: 0.014618959277868271\n",
      "Train Batch Loss: 0.013498835265636444\n",
      "Train Batch Loss: 0.015549415722489357\n",
      "Train Batch Loss: 0.016611864790320396\n",
      "Train Batch Loss: 0.016956031322479248\n",
      "Train Batch Loss: 0.013890441507101059\n",
      "Train Batch Loss: 0.01715519279241562\n",
      "Train Batch Loss: 0.017579499632120132\n",
      "Train Batch Loss: 0.014298699796199799\n",
      "Train Batch Loss: 0.012217836454510689\n",
      "Train Batch Loss: 0.016669005155563354\n",
      "Train Batch Loss: 0.017561212182044983\n",
      "Train Batch Loss: 0.017817122861742973\n",
      "Train Batch Loss: 0.018254872411489487\n",
      "Train Batch Loss: 0.011211398988962173\n",
      "Train Batch Loss: 0.014365291222929955\n",
      "Train Batch Loss: 0.017205065116286278\n",
      "Train Batch Loss: 0.020780974999070168\n",
      "Train Batch Loss: 0.014112342149019241\n",
      "Train Batch Loss: 0.012636428698897362\n",
      "Train Batch Loss: 0.016558580100536346\n",
      "Train Batch Loss: 0.01562909036874771\n",
      "Train Batch Loss: 0.015317402780056\n",
      "Train Batch Loss: 0.014429002068936825\n",
      "Train Batch Loss: 0.017727050930261612\n",
      "Train Batch Loss: 0.015246089547872543\n",
      "Train Batch Loss: 0.015857364982366562\n",
      "Train Batch Loss: 0.016114264726638794\n",
      "Train Batch Loss: 0.014543638564646244\n",
      "Train Batch Loss: 0.016526414081454277\n",
      "Train Batch Loss: 0.014871945604681969\n",
      "Train Batch Loss: 0.014113401062786579\n",
      "Train Batch Loss: 0.016048936173319817\n",
      "Train Batch Loss: 0.013458449393510818\n",
      "Train Batch Loss: 0.01860334724187851\n",
      "Train Batch Loss: 0.011549262329936028\n",
      "Train Batch Loss: 0.013028089888393879\n",
      "Train Batch Loss: 0.017297251150012016\n",
      "Train Batch Loss: 0.01856616884469986\n",
      "Train Batch Loss: 0.016808990389108658\n",
      "Train Batch Loss: 0.013126116245985031\n",
      "Train Batch Loss: 0.013252723962068558\n",
      "Train Batch Loss: 0.01092654187232256\n",
      "Train Batch Loss: 0.020502464845776558\n",
      "Train Batch Loss: 0.012798444367945194\n",
      "Train Batch Loss: 0.015502030029892921\n",
      "Train Batch Loss: 0.01744503527879715\n",
      "Train Batch Loss: 0.015658695250749588\n",
      "Train Batch Loss: 0.012967189773917198\n",
      "Train Batch Loss: 0.01357290893793106\n",
      "Train Batch Loss: 0.015401107259094715\n",
      "Train Batch Loss: 0.019091306254267693\n",
      "Train Batch Loss: 0.017345264554023743\n",
      "Train Batch Loss: 0.016203774139285088\n",
      "Train Batch Loss: 0.016181014478206635\n",
      "Train Batch Loss: 0.014263135381042957\n",
      "Train Batch Loss: 0.015911627560853958\n",
      "Train Batch Loss: 0.01680903322994709\n",
      "Train Batch Loss: 0.015394315123558044\n",
      "Train Batch Loss: 0.017627879977226257\n",
      "Train Batch Loss: 0.013528985902667046\n",
      "Train Batch Loss: 0.017854545265436172\n",
      "Train Batch Loss: 0.016497503966093063\n",
      "Train Batch Loss: 0.017376093193888664\n",
      "Train Batch Loss: 0.013989567756652832\n",
      "Train Batch Loss: 0.013681581243872643\n",
      "Train Batch Loss: 0.01898052915930748\n",
      "Train Batch Loss: 0.014691414311528206\n",
      "Train Batch Loss: 0.016028739511966705\n",
      "Train Batch Loss: 0.01637210324406624\n",
      "Train Batch Loss: 0.014933774247765541\n",
      "Train Batch Loss: 0.014667893759906292\n",
      "Train Batch Loss: 0.016004662960767746\n",
      "Train Batch Loss: 0.01866142451763153\n",
      "Train Batch Loss: 0.015606831759214401\n",
      "Train Batch Loss: 0.014418690465390682\n",
      "Train Batch Loss: 0.015278958715498447\n",
      "Train Batch Loss: 0.014631257392466068\n",
      "Train Batch Loss: 0.01931728608906269\n",
      "Train Batch Loss: 0.017440462484955788\n",
      "Train Batch Loss: 0.015519749373197556\n",
      "Train Batch Loss: 0.015483603812754154\n",
      "Train Batch Loss: 0.016603823751211166\n",
      "Train Batch Loss: 0.012267760932445526\n",
      "Train Batch Loss: 0.013255663216114044\n",
      "Train Batch Loss: 0.012401819229125977\n",
      "Train Batch Loss: 0.017492171376943588\n",
      "Train Batch Loss: 0.014787327498197556\n",
      "Train Batch Loss: 0.012883223593235016\n",
      "Train Batch Loss: 0.016553722321987152\n",
      "Train Batch Loss: 0.016350070014595985\n",
      "Train Batch Loss: 0.015524336136877537\n",
      "Train Batch Loss: 0.01524405274540186\n",
      "Train Batch Loss: 0.01373567245900631\n",
      "Train Batch Loss: 0.017466044053435326\n",
      "Train Batch Loss: 0.015428947284817696\n",
      "Train Batch Loss: 0.017438530921936035\n",
      "Train Batch Loss: 0.014226796105504036\n",
      "Train Batch Loss: 0.014243828132748604\n",
      "Train Batch Loss: 0.016537852585315704\n",
      "Train Batch Loss: 0.014043678529560566\n",
      "Train Batch Loss: 0.014823631383478642\n",
      "Train Batch Loss: 0.015686307102441788\n",
      "Train Batch Loss: 0.015831880271434784\n",
      "Train Batch Loss: 0.0175468772649765\n",
      "Train Batch Loss: 0.01628829725086689\n",
      "Train Batch Loss: 0.018666885793209076\n",
      "Train Batch Loss: 0.013742674142122269\n",
      "Train Batch Loss: 0.015527107752859592\n",
      "Train Batch Loss: 0.01574849523603916\n",
      "Train Batch Loss: 0.01600594073534012\n",
      "Train Batch Loss: 0.014734528958797455\n",
      "Train Batch Loss: 0.01795027405023575\n",
      "Train Batch Loss: 0.0185367651283741\n",
      "Train Batch Loss: 0.013895630836486816\n",
      "Train Batch Loss: 0.016821324825286865\n",
      "Train Batch Loss: 0.01969202421605587\n",
      "Train Batch Loss: 0.018020374700427055\n",
      "Train Batch Loss: 0.01649503968656063\n",
      "Train Batch Loss: 0.01545767392963171\n",
      "Train Batch Loss: 0.01535811834037304\n",
      "Train Batch Loss: 0.01590312272310257\n",
      "Train Batch Loss: 0.019331898540258408\n",
      "Train Batch Loss: 0.015075741335749626\n",
      "Train Batch Loss: 0.015668237581849098\n",
      "Train Batch Loss: 0.014599576592445374\n",
      "Train Batch Loss: 0.01500522717833519\n",
      "Train Batch Loss: 0.01781543903052807\n",
      "Train Batch Loss: 0.016988925635814667\n",
      "Train Batch Loss: 0.018515892326831818\n",
      "Train Batch Loss: 0.013849075883626938\n",
      "Train Batch Loss: 0.01590588502585888\n",
      "Train Batch Loss: 0.014709481969475746\n",
      "Train Batch Loss: 0.017967544496059418\n",
      "Train Batch Loss: 0.016512662172317505\n",
      "Train Batch Loss: 0.01821209117770195\n",
      "Train Batch Loss: 0.012334004044532776\n",
      "Train Batch Loss: 0.015418530441820621\n",
      "Train Batch Loss: 0.014937655068933964\n",
      "Train Batch Loss: 0.013608591631054878\n",
      "Train Batch Loss: 0.014976570382714272\n",
      "Train Batch Loss: 0.01868310570716858\n",
      "Train Batch Loss: 0.01575464941561222\n",
      "Train Batch Loss: 0.01580369472503662\n",
      "Train Batch Loss: 0.012058519758284092\n",
      "Train Batch Loss: 0.016803106293082237\n",
      "Train Batch Loss: 0.015656229108572006\n",
      "Train Batch Loss: 0.011716825887560844\n",
      "Train Batch Loss: 0.015728004276752472\n",
      "Train Batch Loss: 0.015885453671216965\n",
      "Train Batch Loss: 0.015638157725334167\n",
      "Train Batch Loss: 0.013374745845794678\n",
      "Train Batch Loss: 0.013110747560858727\n",
      "Train Batch Loss: 0.02074282430112362\n",
      "Train Batch Loss: 0.013792658224701881\n",
      "Train Batch Loss: 0.01751289889216423\n",
      "Train Batch Loss: 0.013560355640947819\n",
      "Train Batch Loss: 0.013851781375706196\n",
      "Train Batch Loss: 0.016872847452759743\n",
      "Train Batch Loss: 0.014946256764233112\n",
      "Train Batch Loss: 0.01652628928422928\n",
      "Train Batch Loss: 0.012906482443213463\n",
      "Train Batch Loss: 0.01850254274904728\n",
      "Train Batch Loss: 0.017239084467291832\n",
      "Train Batch Loss: 0.02098965086042881\n",
      "Train Batch Loss: 0.017638007178902626\n",
      "Train Batch Loss: 0.01639075018465519\n",
      "Train Batch Loss: 0.015540584921836853\n",
      "Train Batch Loss: 0.015221713110804558\n",
      "Train Batch Loss: 0.015187812969088554\n",
      "Train Batch Loss: 0.01349322684109211\n",
      "Train Batch Loss: 0.020545167848467827\n",
      "Train Batch Loss: 0.016958152875304222\n",
      "Train Batch Loss: 0.015924831852316856\n",
      "Train Batch Loss: 0.014940612018108368\n",
      "valid Batch Loss: 0.018766172230243683\n",
      "valid Batch Loss: 0.016725260764360428\n",
      "valid Batch Loss: 0.019296396523714066\n",
      "valid Batch Loss: 0.018244562670588493\n",
      "valid Batch Loss: 0.012145301327109337\n",
      "valid Batch Loss: 0.018219003453850746\n",
      "valid Batch Loss: 0.012638356536626816\n",
      "valid Batch Loss: 0.016464415937662125\n",
      "valid Batch Loss: 0.015711333602666855\n",
      "valid Batch Loss: 0.016535812988877296\n",
      "valid Batch Loss: 0.017334995791316032\n",
      "valid Batch Loss: 0.01539287343621254\n",
      "valid Batch Loss: 0.013694202527403831\n",
      "valid Batch Loss: 0.01481148973107338\n",
      "valid Batch Loss: 0.016745951026678085\n",
      "valid Batch Loss: 0.015492982231080532\n",
      "valid Batch Loss: 0.0217773225158453\n",
      "valid Batch Loss: 0.012938747182488441\n",
      "valid Batch Loss: 0.019928663969039917\n",
      "valid Batch Loss: 0.014503829181194305\n",
      "valid Batch Loss: 0.015901710838079453\n",
      "valid Batch Loss: 0.015126117505133152\n",
      "valid Batch Loss: 0.013631688430905342\n",
      "valid Batch Loss: 0.01663617230951786\n",
      "valid Batch Loss: 0.012299751862883568\n",
      "valid Batch Loss: 0.016521986573934555\n",
      "valid Batch Loss: 0.015905359759926796\n",
      "valid Batch Loss: 0.017368324100971222\n",
      "valid Batch Loss: 0.01705506630241871\n",
      "valid Batch Loss: 0.015229439362883568\n",
      "valid Batch Loss: 0.014795852825045586\n",
      "valid Batch Loss: 0.014788363128900528\n",
      "valid Batch Loss: 0.018059343099594116\n",
      "valid Batch Loss: 0.012695984914898872\n",
      "valid Batch Loss: 0.01763346791267395\n",
      "valid Batch Loss: 0.017020750790834427\n",
      "valid Batch Loss: 0.018377913162112236\n",
      "valid Batch Loss: 0.01882953941822052\n",
      "valid Batch Loss: 0.018154967576265335\n",
      "valid Batch Loss: 0.014969312585890293\n",
      "valid Batch Loss: 0.01423700526356697\n",
      "valid Batch Loss: 0.013518921099603176\n",
      "valid Batch Loss: 0.014798036776483059\n",
      "valid Batch Loss: 0.017666783183813095\n",
      "valid Batch Loss: 0.013108412735164165\n",
      "valid Batch Loss: 0.01736917532980442\n",
      "valid Batch Loss: 0.015730885788798332\n",
      "valid Batch Loss: 0.018100034445524216\n",
      "valid Batch Loss: 0.017605550587177277\n",
      "valid Batch Loss: 0.019295144826173782\n",
      "valid Batch Loss: 0.015436707995831966\n",
      "valid Batch Loss: 0.01620711013674736\n",
      "valid Batch Loss: 0.018309179693460464\n",
      "valid Batch Loss: 0.015443531796336174\n",
      "valid Batch Loss: 0.02028266340494156\n",
      "valid Batch Loss: 0.014969391748309135\n",
      "valid Batch Loss: 0.014131614938378334\n",
      "valid Batch Loss: 0.01333128847181797\n",
      "valid Batch Loss: 0.015016110613942146\n",
      "valid Batch Loss: 0.01474723406136036\n",
      "valid Batch Loss: 0.014523647725582123\n",
      "valid Batch Loss: 0.013961471617221832\n",
      "Epoch: 3 | Train Loss: 0.015760241076350212 | Valid Loss: 0.016277803108096123\n",
      "\n",
      "\u001b[34mValidation Loss Improved (0.016295844689011574 ---> 0.016277803108096123)\u001b[0m\n",
      "\u001b[34mModel Saved\u001b[0m\n",
      "Train Batch Loss: 0.014688490889966488\n",
      "Train Batch Loss: 0.01380463782697916\n",
      "Train Batch Loss: 0.01971989870071411\n",
      "Train Batch Loss: 0.01086936704814434\n",
      "Train Batch Loss: 0.012301861308515072\n",
      "Train Batch Loss: 0.01372403185814619\n",
      "Train Batch Loss: 0.018119458109140396\n",
      "Train Batch Loss: 0.01685698889195919\n",
      "Train Batch Loss: 0.016455214470624924\n",
      "Train Batch Loss: 0.013910144567489624\n",
      "Train Batch Loss: 0.01866334117949009\n",
      "Train Batch Loss: 0.016544772312045097\n",
      "Train Batch Loss: 0.01572827249765396\n",
      "Train Batch Loss: 0.01518573984503746\n",
      "Train Batch Loss: 0.01503885816782713\n",
      "Train Batch Loss: 0.01514890044927597\n",
      "Train Batch Loss: 0.015166929922997952\n",
      "Train Batch Loss: 0.016355115920305252\n",
      "Train Batch Loss: 0.016846023499965668\n",
      "Train Batch Loss: 0.014342067763209343\n",
      "Train Batch Loss: 0.018988296389579773\n",
      "Train Batch Loss: 0.014283237978816032\n",
      "Train Batch Loss: 0.017462050542235374\n",
      "Train Batch Loss: 0.014892554841935635\n",
      "Train Batch Loss: 0.01951942779123783\n",
      "Train Batch Loss: 0.014460991136729717\n",
      "Train Batch Loss: 0.012320850044488907\n",
      "Train Batch Loss: 0.017944756895303726\n",
      "Train Batch Loss: 0.011948876082897186\n",
      "Train Batch Loss: 0.014106502756476402\n",
      "Train Batch Loss: 0.014587598852813244\n",
      "Train Batch Loss: 0.01563161052763462\n",
      "Train Batch Loss: 0.014917517080903053\n",
      "Train Batch Loss: 0.018225081264972687\n",
      "Train Batch Loss: 0.015617703087627888\n",
      "Train Batch Loss: 0.016521219164133072\n",
      "Train Batch Loss: 0.012960666790604591\n",
      "Train Batch Loss: 0.017176823690533638\n",
      "Train Batch Loss: 0.013269401155412197\n",
      "Train Batch Loss: 0.01328875683248043\n",
      "Train Batch Loss: 0.014599600806832314\n",
      "Train Batch Loss: 0.01726447604596615\n",
      "Train Batch Loss: 0.014624852687120438\n",
      "Train Batch Loss: 0.016220493242144585\n",
      "Train Batch Loss: 0.015249602496623993\n",
      "Train Batch Loss: 0.017513900995254517\n",
      "Train Batch Loss: 0.01472699549049139\n",
      "Train Batch Loss: 0.01480708085000515\n",
      "Train Batch Loss: 0.015240909531712532\n",
      "Train Batch Loss: 0.01360043790191412\n",
      "Train Batch Loss: 0.013012399896979332\n",
      "Train Batch Loss: 0.017722994089126587\n",
      "Train Batch Loss: 0.023746822029352188\n",
      "Train Batch Loss: 0.016128739342093468\n",
      "Train Batch Loss: 0.02126636728644371\n",
      "Train Batch Loss: 0.016068803146481514\n",
      "Train Batch Loss: 0.012641685083508492\n",
      "Train Batch Loss: 0.01475509162992239\n",
      "Train Batch Loss: 0.014581716619431973\n",
      "Train Batch Loss: 0.013572278432548046\n",
      "Train Batch Loss: 0.02034338004887104\n",
      "Train Batch Loss: 0.016187487170100212\n",
      "Train Batch Loss: 0.015598737634718418\n",
      "Train Batch Loss: 0.011489449068903923\n",
      "Train Batch Loss: 0.013731535524129868\n",
      "Train Batch Loss: 0.013398263603448868\n",
      "Train Batch Loss: 0.013024815358221531\n",
      "Train Batch Loss: 0.013183162547647953\n",
      "Train Batch Loss: 0.01401691697537899\n",
      "Train Batch Loss: 0.018316145986318588\n",
      "Train Batch Loss: 0.0137161985039711\n",
      "Train Batch Loss: 0.016096211969852448\n",
      "Train Batch Loss: 0.016371455043554306\n",
      "Train Batch Loss: 0.015768608078360558\n",
      "Train Batch Loss: 0.01777249574661255\n",
      "Train Batch Loss: 0.0138753242790699\n",
      "Train Batch Loss: 0.021975986659526825\n",
      "Train Batch Loss: 0.01586073450744152\n",
      "Train Batch Loss: 0.013670778833329678\n",
      "Train Batch Loss: 0.016834314912557602\n",
      "Train Batch Loss: 0.014408262446522713\n",
      "Train Batch Loss: 0.013374540023505688\n",
      "Train Batch Loss: 0.012669499963521957\n",
      "Train Batch Loss: 0.014064732939004898\n",
      "Train Batch Loss: 0.016981780529022217\n",
      "Train Batch Loss: 0.020465891808271408\n",
      "Train Batch Loss: 0.02001015841960907\n",
      "Train Batch Loss: 0.017552727833390236\n",
      "Train Batch Loss: 0.016980856657028198\n",
      "Train Batch Loss: 0.01668493263423443\n",
      "Train Batch Loss: 0.016624221578240395\n",
      "Train Batch Loss: 0.013702824711799622\n",
      "Train Batch Loss: 0.015183644369244576\n",
      "Train Batch Loss: 0.01107648853212595\n",
      "Train Batch Loss: 0.018492761999368668\n",
      "Train Batch Loss: 0.021810214966535568\n",
      "Train Batch Loss: 0.01281654555350542\n",
      "Train Batch Loss: 0.018250703811645508\n",
      "Train Batch Loss: 0.01654539629817009\n",
      "Train Batch Loss: 0.0134825324639678\n",
      "Train Batch Loss: 0.016449129208922386\n",
      "Train Batch Loss: 0.014949184842407703\n",
      "Train Batch Loss: 0.015579219907522202\n",
      "Train Batch Loss: 0.014711197465658188\n",
      "Train Batch Loss: 0.015725959092378616\n",
      "Train Batch Loss: 0.018858276307582855\n",
      "Train Batch Loss: 0.014927222393453121\n",
      "Train Batch Loss: 0.01638493314385414\n",
      "Train Batch Loss: 0.014695107005536556\n",
      "Train Batch Loss: 0.015005533583462238\n",
      "Train Batch Loss: 0.01453150250017643\n",
      "Train Batch Loss: 0.01770542934536934\n",
      "Train Batch Loss: 0.018046867102384567\n",
      "Train Batch Loss: 0.015279112383723259\n",
      "Train Batch Loss: 0.014902734197676182\n",
      "Train Batch Loss: 0.01341201737523079\n",
      "Train Batch Loss: 0.018415428698062897\n",
      "Train Batch Loss: 0.0154456477612257\n",
      "Train Batch Loss: 0.015818487852811813\n",
      "Train Batch Loss: 0.016144469380378723\n",
      "Train Batch Loss: 0.01497557945549488\n",
      "Train Batch Loss: 0.014392435550689697\n",
      "Train Batch Loss: 0.014974590390920639\n",
      "Train Batch Loss: 0.013399608433246613\n",
      "Train Batch Loss: 0.015287467278540134\n",
      "Train Batch Loss: 0.014763211831450462\n",
      "Train Batch Loss: 0.01644519716501236\n",
      "Train Batch Loss: 0.014373603276908398\n",
      "Train Batch Loss: 0.013206744566559792\n",
      "Train Batch Loss: 0.016115253791213036\n",
      "Train Batch Loss: 0.015414919704198837\n",
      "Train Batch Loss: 0.017488831654191017\n",
      "Train Batch Loss: 0.015526827424764633\n",
      "Train Batch Loss: 0.01608847640454769\n",
      "Train Batch Loss: 0.016947846859693527\n",
      "Train Batch Loss: 0.014468549750745296\n",
      "Train Batch Loss: 0.017746150493621826\n",
      "Train Batch Loss: 0.013775080442428589\n",
      "Train Batch Loss: 0.01621532253921032\n",
      "Train Batch Loss: 0.01438601128757\n",
      "Train Batch Loss: 0.017602453008294106\n",
      "Train Batch Loss: 0.01628572680056095\n",
      "Train Batch Loss: 0.015915866941213608\n",
      "Train Batch Loss: 0.012420826591551304\n",
      "Train Batch Loss: 0.014351997524499893\n",
      "Train Batch Loss: 0.018694046884775162\n",
      "Train Batch Loss: 0.014626933261752129\n",
      "Train Batch Loss: 0.013745501637458801\n",
      "Train Batch Loss: 0.013819046318531036\n",
      "Train Batch Loss: 0.01645790785551071\n",
      "Train Batch Loss: 0.017582561820745468\n",
      "Train Batch Loss: 0.01677865907549858\n",
      "Train Batch Loss: 0.01341719925403595\n",
      "Train Batch Loss: 0.014540362171828747\n",
      "Train Batch Loss: 0.013631748035550117\n",
      "Train Batch Loss: 0.016013747081160545\n",
      "Train Batch Loss: 0.01584411785006523\n",
      "Train Batch Loss: 0.019480165094137192\n",
      "Train Batch Loss: 0.014802379533648491\n",
      "Train Batch Loss: 0.016907241195440292\n",
      "Train Batch Loss: 0.013106371276080608\n",
      "Train Batch Loss: 0.014155695214867592\n",
      "Train Batch Loss: 0.01539329532533884\n",
      "Train Batch Loss: 0.015590790659189224\n",
      "Train Batch Loss: 0.018172364681959152\n",
      "Train Batch Loss: 0.020273219794034958\n",
      "Train Batch Loss: 0.015302013605833054\n",
      "Train Batch Loss: 0.015880383551120758\n",
      "Train Batch Loss: 0.018929794430732727\n",
      "Train Batch Loss: 0.01599173992872238\n",
      "Train Batch Loss: 0.011985870078206062\n",
      "Train Batch Loss: 0.014077378436923027\n",
      "Train Batch Loss: 0.016726821660995483\n",
      "Train Batch Loss: 0.014580812305212021\n",
      "Train Batch Loss: 0.014290284365415573\n",
      "Train Batch Loss: 0.01807548850774765\n",
      "Train Batch Loss: 0.016199342906475067\n",
      "Train Batch Loss: 0.017129885032773018\n",
      "Train Batch Loss: 0.01430580671876669\n",
      "Train Batch Loss: 0.01571740210056305\n",
      "Train Batch Loss: 0.015293935313820839\n",
      "Train Batch Loss: 0.01539329532533884\n",
      "Train Batch Loss: 0.0128836864605546\n",
      "Train Batch Loss: 0.016920937225222588\n",
      "Train Batch Loss: 0.012782949954271317\n",
      "Train Batch Loss: 0.01918948069214821\n",
      "Train Batch Loss: 0.015612279064953327\n",
      "Train Batch Loss: 0.014455925673246384\n",
      "Train Batch Loss: 0.01779283955693245\n",
      "Train Batch Loss: 0.011501471512019634\n",
      "Train Batch Loss: 0.014386296272277832\n",
      "Train Batch Loss: 0.01422484964132309\n",
      "Train Batch Loss: 0.01592404767870903\n",
      "Train Batch Loss: 0.017965681850910187\n",
      "Train Batch Loss: 0.013295045122504234\n",
      "Train Batch Loss: 0.017478054389357567\n",
      "Train Batch Loss: 0.014618813060224056\n",
      "Train Batch Loss: 0.016543246805667877\n",
      "Train Batch Loss: 0.013146279379725456\n",
      "Train Batch Loss: 0.014000640250742435\n",
      "Train Batch Loss: 0.015616974793374538\n",
      "Train Batch Loss: 0.01850999891757965\n",
      "Train Batch Loss: 0.019406013190746307\n",
      "Train Batch Loss: 0.018362464383244514\n",
      "Train Batch Loss: 0.01292581669986248\n",
      "Train Batch Loss: 0.014642376452684402\n",
      "Train Batch Loss: 0.01608697511255741\n",
      "Train Batch Loss: 0.0148696294054389\n",
      "Train Batch Loss: 0.017754290252923965\n",
      "Train Batch Loss: 0.01639951393008232\n",
      "Train Batch Loss: 0.014562174677848816\n",
      "Train Batch Loss: 0.015375671908259392\n",
      "Train Batch Loss: 0.012050360441207886\n",
      "Train Batch Loss: 0.01627415418624878\n",
      "Train Batch Loss: 0.013284359127283096\n",
      "Train Batch Loss: 0.014837933704257011\n",
      "Train Batch Loss: 0.01395117212086916\n",
      "Train Batch Loss: 0.017728285863995552\n",
      "Train Batch Loss: 0.01261783204972744\n",
      "Train Batch Loss: 0.013740483671426773\n",
      "Train Batch Loss: 0.01923828199505806\n",
      "Train Batch Loss: 0.0158708393573761\n",
      "Train Batch Loss: 0.011776858009397984\n",
      "Train Batch Loss: 0.014925382100045681\n",
      "Train Batch Loss: 0.014828938990831375\n",
      "Train Batch Loss: 0.013827471062541008\n",
      "Train Batch Loss: 0.016293209046125412\n",
      "Train Batch Loss: 0.014898110181093216\n",
      "Train Batch Loss: 0.011007262393832207\n",
      "Train Batch Loss: 0.015154758468270302\n",
      "Train Batch Loss: 0.01627921685576439\n",
      "Train Batch Loss: 0.016983376815915108\n",
      "Train Batch Loss: 0.014118684455752373\n",
      "Train Batch Loss: 0.013967286795377731\n",
      "Train Batch Loss: 0.016889868304133415\n",
      "Train Batch Loss: 0.01469432096928358\n",
      "Train Batch Loss: 0.01713256537914276\n",
      "Train Batch Loss: 0.011455575004220009\n",
      "Train Batch Loss: 0.014628896489739418\n",
      "Train Batch Loss: 0.0157950259745121\n",
      "Train Batch Loss: 0.019932283088564873\n",
      "Train Batch Loss: 0.018736708909273148\n",
      "Train Batch Loss: 0.018194817006587982\n",
      "Train Batch Loss: 0.011969789862632751\n",
      "Train Batch Loss: 0.017474379390478134\n",
      "Train Batch Loss: 0.017743902280926704\n",
      "Train Batch Loss: 0.013811483047902584\n",
      "Train Batch Loss: 0.017794540151953697\n",
      "Train Batch Loss: 0.011403389275074005\n",
      "Train Batch Loss: 0.015219378285109997\n",
      "valid Batch Loss: 0.019404467195272446\n",
      "valid Batch Loss: 0.016738243401050568\n",
      "valid Batch Loss: 0.020066233351826668\n",
      "valid Batch Loss: 0.018619202077388763\n",
      "valid Batch Loss: 0.011023206636309624\n",
      "valid Batch Loss: 0.019416501745581627\n",
      "valid Batch Loss: 0.0118862334638834\n",
      "valid Batch Loss: 0.016603413969278336\n",
      "valid Batch Loss: 0.01545661874115467\n",
      "valid Batch Loss: 0.016604896634817123\n",
      "valid Batch Loss: 0.01741836965084076\n",
      "valid Batch Loss: 0.015489107929170132\n",
      "valid Batch Loss: 0.014003980904817581\n",
      "valid Batch Loss: 0.014342512004077435\n",
      "valid Batch Loss: 0.01705286279320717\n",
      "valid Batch Loss: 0.01587267778813839\n",
      "valid Batch Loss: 0.02160707116127014\n",
      "valid Batch Loss: 0.012240168638527393\n",
      "valid Batch Loss: 0.019432207569479942\n",
      "valid Batch Loss: 0.014720441773533821\n",
      "valid Batch Loss: 0.016042765229940414\n",
      "valid Batch Loss: 0.015436114743351936\n",
      "valid Batch Loss: 0.013876567594707012\n",
      "valid Batch Loss: 0.016983121633529663\n",
      "valid Batch Loss: 0.012199647724628448\n",
      "valid Batch Loss: 0.016775429248809814\n",
      "valid Batch Loss: 0.01567765884101391\n",
      "valid Batch Loss: 0.0173196904361248\n",
      "valid Batch Loss: 0.016774147748947144\n",
      "valid Batch Loss: 0.015097295865416527\n",
      "valid Batch Loss: 0.014095093123614788\n",
      "valid Batch Loss: 0.015487848781049252\n",
      "valid Batch Loss: 0.018826400861144066\n",
      "valid Batch Loss: 0.012676585465669632\n",
      "valid Batch Loss: 0.017637114971876144\n",
      "valid Batch Loss: 0.016566243022680283\n",
      "valid Batch Loss: 0.019467322155833244\n",
      "valid Batch Loss: 0.019510064274072647\n",
      "valid Batch Loss: 0.01806008815765381\n",
      "valid Batch Loss: 0.014989081770181656\n",
      "valid Batch Loss: 0.013769088312983513\n",
      "valid Batch Loss: 0.012873366475105286\n",
      "valid Batch Loss: 0.014502501115202904\n",
      "valid Batch Loss: 0.01762259751558304\n",
      "valid Batch Loss: 0.01233634538948536\n",
      "valid Batch Loss: 0.018551908433437347\n",
      "valid Batch Loss: 0.01565299555659294\n",
      "valid Batch Loss: 0.0185454823076725\n",
      "valid Batch Loss: 0.016993124037981033\n",
      "valid Batch Loss: 0.020697401836514473\n",
      "valid Batch Loss: 0.015356376767158508\n",
      "valid Batch Loss: 0.016153646633028984\n",
      "valid Batch Loss: 0.01864563673734665\n",
      "valid Batch Loss: 0.015491819009184837\n",
      "valid Batch Loss: 0.02095627970993519\n",
      "valid Batch Loss: 0.015466256998479366\n",
      "valid Batch Loss: 0.013830304145812988\n",
      "valid Batch Loss: 0.013909848406910896\n",
      "valid Batch Loss: 0.015215572901070118\n",
      "valid Batch Loss: 0.01443982683122158\n",
      "valid Batch Loss: 0.014740150421857834\n",
      "valid Batch Loss: 0.01345502957701683\n",
      "Epoch: 4 | Train Loss: 0.015570519492030144 | Valid Loss: 0.016406917944550514\n",
      "\n",
      "Train Batch Loss: 0.015456387773156166\n",
      "Train Batch Loss: 0.013359427452087402\n",
      "Train Batch Loss: 0.017020145431160927\n",
      "Train Batch Loss: 0.015394343063235283\n",
      "Train Batch Loss: 0.016094673424959183\n",
      "Train Batch Loss: 0.013448910787701607\n",
      "Train Batch Loss: 0.014448929578065872\n",
      "Train Batch Loss: 0.017419185489416122\n",
      "Train Batch Loss: 0.01325199380517006\n",
      "Train Batch Loss: 0.014725147746503353\n",
      "Train Batch Loss: 0.01771915890276432\n",
      "Train Batch Loss: 0.013483328744769096\n",
      "Train Batch Loss: 0.01718837395310402\n",
      "Train Batch Loss: 0.018151219934225082\n",
      "Train Batch Loss: 0.017279192805290222\n",
      "Train Batch Loss: 0.0165909081697464\n",
      "Train Batch Loss: 0.018479209393262863\n",
      "Train Batch Loss: 0.012919742614030838\n",
      "Train Batch Loss: 0.016303904354572296\n",
      "Train Batch Loss: 0.015354645438492298\n",
      "Train Batch Loss: 0.015636850148439407\n",
      "Train Batch Loss: 0.014087125658988953\n",
      "Train Batch Loss: 0.014252370223402977\n",
      "Train Batch Loss: 0.01651088520884514\n",
      "Train Batch Loss: 0.0160513985902071\n",
      "Train Batch Loss: 0.014128079637885094\n",
      "Train Batch Loss: 0.014644291251897812\n",
      "Train Batch Loss: 0.012966455891728401\n",
      "Train Batch Loss: 0.016426656395196915\n",
      "Train Batch Loss: 0.013957456685602665\n",
      "Train Batch Loss: 0.016110669821500778\n",
      "Train Batch Loss: 0.01409652829170227\n",
      "Train Batch Loss: 0.01306566596031189\n",
      "Train Batch Loss: 0.013990124687552452\n",
      "Train Batch Loss: 0.01507893018424511\n",
      "Train Batch Loss: 0.0128618273884058\n",
      "Train Batch Loss: 0.012220924720168114\n",
      "Train Batch Loss: 0.013332800008356571\n",
      "Train Batch Loss: 0.01645955629646778\n",
      "Train Batch Loss: 0.01648741401731968\n",
      "Train Batch Loss: 0.014681130647659302\n",
      "Train Batch Loss: 0.014872558414936066\n",
      "Train Batch Loss: 0.015620987862348557\n",
      "Train Batch Loss: 0.014464608393609524\n",
      "Train Batch Loss: 0.013299302197992802\n",
      "Train Batch Loss: 0.014351099729537964\n",
      "Train Batch Loss: 0.014516295865178108\n",
      "Train Batch Loss: 0.014258760958909988\n",
      "Train Batch Loss: 0.017102155834436417\n",
      "Train Batch Loss: 0.015035998076200485\n",
      "Train Batch Loss: 0.01687467470765114\n",
      "Train Batch Loss: 0.011666892096400261\n",
      "Train Batch Loss: 0.016067203134298325\n",
      "Train Batch Loss: 0.015673067420721054\n",
      "Train Batch Loss: 0.017090298235416412\n",
      "Train Batch Loss: 0.013934699818491936\n",
      "Train Batch Loss: 0.014994441531598568\n",
      "Train Batch Loss: 0.01779959909617901\n",
      "Train Batch Loss: 0.015827380120754242\n",
      "Train Batch Loss: 0.014378547668457031\n",
      "Train Batch Loss: 0.015633800998330116\n",
      "Train Batch Loss: 0.014158547855913639\n",
      "Train Batch Loss: 0.012464401312172413\n",
      "Train Batch Loss: 0.01235665287822485\n",
      "Train Batch Loss: 0.014831462875008583\n",
      "Train Batch Loss: 0.015584773384034634\n",
      "Train Batch Loss: 0.015274593606591225\n",
      "Train Batch Loss: 0.018781887367367744\n",
      "Train Batch Loss: 0.013459022156894207\n",
      "Train Batch Loss: 0.013334903866052628\n",
      "Train Batch Loss: 0.015254903584718704\n",
      "Train Batch Loss: 0.01919824630022049\n",
      "Train Batch Loss: 0.01674872636795044\n",
      "Train Batch Loss: 0.01503063179552555\n",
      "Train Batch Loss: 0.014956570230424404\n",
      "Train Batch Loss: 0.017932835966348648\n",
      "Train Batch Loss: 0.01306309923529625\n",
      "Train Batch Loss: 0.014127498492598534\n",
      "Train Batch Loss: 0.020112141966819763\n",
      "Train Batch Loss: 0.015254000201821327\n",
      "Train Batch Loss: 0.014039364643394947\n",
      "Train Batch Loss: 0.017888791859149933\n",
      "Train Batch Loss: 0.01502808928489685\n",
      "Train Batch Loss: 0.018839148804545403\n",
      "Train Batch Loss: 0.0160722304135561\n",
      "Train Batch Loss: 0.01463684905320406\n",
      "Train Batch Loss: 0.013757911510765553\n",
      "Train Batch Loss: 0.013529260642826557\n",
      "Train Batch Loss: 0.013841572217643261\n",
      "Train Batch Loss: 0.0165218785405159\n",
      "Train Batch Loss: 0.01591557078063488\n",
      "Train Batch Loss: 0.013258125633001328\n",
      "Train Batch Loss: 0.01224524900317192\n",
      "Train Batch Loss: 0.015336914919316769\n",
      "Train Batch Loss: 0.013939696364104748\n",
      "Train Batch Loss: 0.01559252105653286\n",
      "Train Batch Loss: 0.016075827181339264\n",
      "Train Batch Loss: 0.019163236021995544\n",
      "Train Batch Loss: 0.0173316802829504\n",
      "Train Batch Loss: 0.013071749359369278\n",
      "Train Batch Loss: 0.01733967289328575\n",
      "Train Batch Loss: 0.016764145344495773\n",
      "Train Batch Loss: 0.011897586286067963\n",
      "Train Batch Loss: 0.018384311348199844\n",
      "Train Batch Loss: 0.01799210160970688\n",
      "Train Batch Loss: 0.016156796365976334\n",
      "Train Batch Loss: 0.017476659268140793\n",
      "Train Batch Loss: 0.01780814677476883\n",
      "Train Batch Loss: 0.01503942720592022\n",
      "Train Batch Loss: 0.01792389526963234\n",
      "Train Batch Loss: 0.016434447839856148\n",
      "Train Batch Loss: 0.01424165815114975\n",
      "Train Batch Loss: 0.014075507409870625\n",
      "Train Batch Loss: 0.013620099984109402\n",
      "Train Batch Loss: 0.014072074554860592\n",
      "Train Batch Loss: 0.01609630510210991\n",
      "Train Batch Loss: 0.019856035709381104\n",
      "Train Batch Loss: 0.017883585765957832\n",
      "Train Batch Loss: 0.01626063697040081\n",
      "Train Batch Loss: 0.013965863734483719\n",
      "Train Batch Loss: 0.013262595981359482\n",
      "Train Batch Loss: 0.015557223930954933\n",
      "Train Batch Loss: 0.013935031369328499\n",
      "Train Batch Loss: 0.015847960487008095\n",
      "Train Batch Loss: 0.01222265139222145\n",
      "Train Batch Loss: 0.021965324878692627\n",
      "Train Batch Loss: 0.015988148748874664\n",
      "Train Batch Loss: 0.018221406266093254\n",
      "Train Batch Loss: 0.013285210356116295\n",
      "Train Batch Loss: 0.014680996537208557\n",
      "Train Batch Loss: 0.012791305780410767\n",
      "Train Batch Loss: 0.01320427656173706\n",
      "Train Batch Loss: 0.020358797162771225\n",
      "Train Batch Loss: 0.014733223244547844\n",
      "Train Batch Loss: 0.016208142042160034\n",
      "Train Batch Loss: 0.015085099264979362\n",
      "Train Batch Loss: 0.015812912955880165\n",
      "Train Batch Loss: 0.01161857321858406\n",
      "Train Batch Loss: 0.01657157950103283\n",
      "Train Batch Loss: 0.014006023295223713\n",
      "Train Batch Loss: 0.011530864983797073\n",
      "Train Batch Loss: 0.017082612961530685\n",
      "Train Batch Loss: 0.016673117876052856\n",
      "Train Batch Loss: 0.014264164492487907\n",
      "Train Batch Loss: 0.016399869695305824\n",
      "Train Batch Loss: 0.013496477156877518\n",
      "Train Batch Loss: 0.015814080834388733\n",
      "Train Batch Loss: 0.014222795143723488\n",
      "Train Batch Loss: 0.016273172572255135\n",
      "Train Batch Loss: 0.013719507493078709\n",
      "Train Batch Loss: 0.01889783889055252\n",
      "Train Batch Loss: 0.014174606651067734\n",
      "Train Batch Loss: 0.01773730479180813\n",
      "Train Batch Loss: 0.014666406437754631\n",
      "Train Batch Loss: 0.01543176919221878\n",
      "Train Batch Loss: 0.0183602012693882\n",
      "Train Batch Loss: 0.020321641117334366\n",
      "Train Batch Loss: 0.016275888308882713\n",
      "Train Batch Loss: 0.016295865178108215\n",
      "Train Batch Loss: 0.013280564919114113\n",
      "Train Batch Loss: 0.01654168777167797\n",
      "Train Batch Loss: 0.013590514659881592\n",
      "Train Batch Loss: 0.01267106644809246\n",
      "Train Batch Loss: 0.011249251663684845\n",
      "Train Batch Loss: 0.015961891040205956\n",
      "Train Batch Loss: 0.0142897954210639\n",
      "Train Batch Loss: 0.012981748208403587\n",
      "Train Batch Loss: 0.01718360185623169\n",
      "Train Batch Loss: 0.0138704227283597\n",
      "Train Batch Loss: 0.014810888096690178\n",
      "Train Batch Loss: 0.014721755869686604\n",
      "Train Batch Loss: 0.01545985508710146\n",
      "Train Batch Loss: 0.012189175933599472\n",
      "Train Batch Loss: 0.015719281509518623\n",
      "Train Batch Loss: 0.010436004027724266\n",
      "Train Batch Loss: 0.015174629166722298\n",
      "Train Batch Loss: 0.014745287597179413\n",
      "Train Batch Loss: 0.01288174744695425\n",
      "Train Batch Loss: 0.019163701683282852\n",
      "Train Batch Loss: 0.01538262888789177\n",
      "Train Batch Loss: 0.01621420867741108\n",
      "Train Batch Loss: 0.014888711273670197\n",
      "Train Batch Loss: 0.015050559304654598\n",
      "Train Batch Loss: 0.017571184784173965\n",
      "Train Batch Loss: 0.017547108232975006\n",
      "Train Batch Loss: 0.012950126081705093\n",
      "Train Batch Loss: 0.015687143430113792\n",
      "Train Batch Loss: 0.012064272537827492\n",
      "Train Batch Loss: 0.01738547533750534\n",
      "Train Batch Loss: 0.01496943086385727\n",
      "Train Batch Loss: 0.017943406477570534\n",
      "Train Batch Loss: 0.01696576178073883\n",
      "Train Batch Loss: 0.018267789855599403\n",
      "Train Batch Loss: 0.018628127872943878\n",
      "Train Batch Loss: 0.012797145172953606\n",
      "Train Batch Loss: 0.014746086671948433\n",
      "Train Batch Loss: 0.015486630611121655\n",
      "Train Batch Loss: 0.01565481722354889\n",
      "Train Batch Loss: 0.01713709346950054\n",
      "Train Batch Loss: 0.01832694187760353\n",
      "Train Batch Loss: 0.019019637256860733\n",
      "Train Batch Loss: 0.016840890049934387\n",
      "Train Batch Loss: 0.01479762140661478\n",
      "Train Batch Loss: 0.014617128297686577\n",
      "Train Batch Loss: 0.014959131367504597\n",
      "Train Batch Loss: 0.01767371967434883\n",
      "Train Batch Loss: 0.015977198258042336\n",
      "Train Batch Loss: 0.013997608795762062\n",
      "Train Batch Loss: 0.012711459770798683\n",
      "Train Batch Loss: 0.012792137451469898\n",
      "Train Batch Loss: 0.016471486538648605\n",
      "Train Batch Loss: 0.013224304653704166\n",
      "Train Batch Loss: 0.012854023836553097\n",
      "Train Batch Loss: 0.016498487442731857\n",
      "Train Batch Loss: 0.015832191333174706\n",
      "Train Batch Loss: 0.01613008603453636\n",
      "Train Batch Loss: 0.01773025095462799\n",
      "Train Batch Loss: 0.015196430496871471\n",
      "Train Batch Loss: 0.017248909920454025\n",
      "Train Batch Loss: 0.013030282221734524\n",
      "Train Batch Loss: 0.01489386148750782\n",
      "Train Batch Loss: 0.01906474493443966\n",
      "Train Batch Loss: 0.013961026445031166\n",
      "Train Batch Loss: 0.014084556140005589\n",
      "Train Batch Loss: 0.014663053676486015\n",
      "Train Batch Loss: 0.015344887971878052\n",
      "Train Batch Loss: 0.0165395550429821\n",
      "Train Batch Loss: 0.019628796726465225\n",
      "Train Batch Loss: 0.016281917691230774\n",
      "Train Batch Loss: 0.01530009601265192\n",
      "Train Batch Loss: 0.015400880947709084\n",
      "Train Batch Loss: 0.014768124558031559\n",
      "Train Batch Loss: 0.022119149565696716\n",
      "Train Batch Loss: 0.013188378885388374\n",
      "Train Batch Loss: 0.01618896797299385\n",
      "Train Batch Loss: 0.013865816406905651\n",
      "Train Batch Loss: 0.011939858086407185\n",
      "Train Batch Loss: 0.017769064754247665\n",
      "Train Batch Loss: 0.015150784514844418\n",
      "Train Batch Loss: 0.015940919518470764\n",
      "Train Batch Loss: 0.019548654556274414\n",
      "Train Batch Loss: 0.019296444952487946\n",
      "Train Batch Loss: 0.017887819558382034\n",
      "Train Batch Loss: 0.018842939287424088\n",
      "Train Batch Loss: 0.021687783300876617\n",
      "Train Batch Loss: 0.01611916534602642\n",
      "Train Batch Loss: 0.012022895738482475\n",
      "Train Batch Loss: 0.014779741875827312\n",
      "Train Batch Loss: 0.017174510285258293\n",
      "Train Batch Loss: 0.01387966051697731\n",
      "valid Batch Loss: 0.018898962065577507\n",
      "valid Batch Loss: 0.0166154894977808\n",
      "valid Batch Loss: 0.019480213522911072\n",
      "valid Batch Loss: 0.018275082111358643\n",
      "valid Batch Loss: 0.011595234274864197\n",
      "valid Batch Loss: 0.018568705767393112\n",
      "valid Batch Loss: 0.012231800705194473\n",
      "valid Batch Loss: 0.016403527930378914\n",
      "valid Batch Loss: 0.015497717075049877\n",
      "valid Batch Loss: 0.016447804868221283\n",
      "valid Batch Loss: 0.01725253090262413\n",
      "valid Batch Loss: 0.015315394848585129\n",
      "valid Batch Loss: 0.013699561357498169\n",
      "valid Batch Loss: 0.014514769427478313\n",
      "valid Batch Loss: 0.016750195994973183\n",
      "valid Batch Loss: 0.01552545465528965\n",
      "valid Batch Loss: 0.021596478298306465\n",
      "valid Batch Loss: 0.012552961707115173\n",
      "valid Batch Loss: 0.01962127722799778\n",
      "valid Batch Loss: 0.014473049901425838\n",
      "valid Batch Loss: 0.015841618180274963\n",
      "valid Batch Loss: 0.015131554566323757\n",
      "valid Batch Loss: 0.01361188292503357\n",
      "valid Batch Loss: 0.016655948013067245\n",
      "valid Batch Loss: 0.012146113440394402\n",
      "valid Batch Loss: 0.016505490988492966\n",
      "valid Batch Loss: 0.01570223458111286\n",
      "valid Batch Loss: 0.01723465323448181\n",
      "valid Batch Loss: 0.016831301152706146\n",
      "valid Batch Loss: 0.015063371509313583\n",
      "valid Batch Loss: 0.014409217983484268\n",
      "valid Batch Loss: 0.014944889582693577\n",
      "valid Batch Loss: 0.018242081627249718\n",
      "valid Batch Loss: 0.012573656626045704\n",
      "valid Batch Loss: 0.017520077526569366\n",
      "valid Batch Loss: 0.016729645431041718\n",
      "valid Batch Loss: 0.018685687333345413\n",
      "valid Batch Loss: 0.01897870935499668\n",
      "valid Batch Loss: 0.01800335757434368\n",
      "valid Batch Loss: 0.014862177893519402\n",
      "valid Batch Loss: 0.013940691947937012\n",
      "valid Batch Loss: 0.013153706677258015\n",
      "valid Batch Loss: 0.014568593353033066\n",
      "valid Batch Loss: 0.017534837126731873\n",
      "valid Batch Loss: 0.012694128789007664\n",
      "valid Batch Loss: 0.01771315559744835\n",
      "valid Batch Loss: 0.015585864894092083\n",
      "valid Batch Loss: 0.01815801113843918\n",
      "valid Batch Loss: 0.01725318282842636\n",
      "valid Batch Loss: 0.019724272191524506\n",
      "valid Batch Loss: 0.015290744602680206\n",
      "valid Batch Loss: 0.016071557998657227\n",
      "valid Batch Loss: 0.01832488551735878\n",
      "valid Batch Loss: 0.01534745842218399\n",
      "valid Batch Loss: 0.020429149270057678\n",
      "valid Batch Loss: 0.015047326683998108\n",
      "valid Batch Loss: 0.013899927958846092\n",
      "valid Batch Loss: 0.013440899550914764\n",
      "valid Batch Loss: 0.01497868075966835\n",
      "valid Batch Loss: 0.014513181522488594\n",
      "valid Batch Loss: 0.014492826536297798\n",
      "valid Batch Loss: 0.013650210574269295\n",
      "Epoch: 5 | Train Loss: 0.015546685084700584 | Valid Loss: 0.016213195398449898\n",
      "\n",
      "\u001b[34mValidation Loss Improved (0.016277803108096123 ---> 0.016213195398449898)\u001b[0m\n",
      "\u001b[34mModel Saved\u001b[0m\n",
      "Train Batch Loss: 0.011933466419577599\n",
      "Train Batch Loss: 0.015027114190161228\n",
      "Train Batch Loss: 0.019162282347679138\n",
      "Train Batch Loss: 0.015031782910227776\n",
      "Train Batch Loss: 0.014435254037380219\n",
      "Train Batch Loss: 0.013909942470490932\n",
      "Train Batch Loss: 0.01523885503411293\n",
      "Train Batch Loss: 0.01695205643773079\n",
      "Train Batch Loss: 0.012473306618630886\n",
      "Train Batch Loss: 0.015037045814096928\n",
      "Train Batch Loss: 0.012652948498725891\n",
      "Train Batch Loss: 0.019192969426512718\n",
      "Train Batch Loss: 0.014999995939433575\n",
      "Train Batch Loss: 0.02228696458041668\n",
      "Train Batch Loss: 0.014685415662825108\n",
      "Train Batch Loss: 0.015324070118367672\n",
      "Train Batch Loss: 0.013707594014704227\n",
      "Train Batch Loss: 0.016081392765045166\n",
      "Train Batch Loss: 0.012387670576572418\n",
      "Train Batch Loss: 0.014278973452746868\n",
      "Train Batch Loss: 0.015502836555242538\n",
      "Train Batch Loss: 0.01521242968738079\n",
      "Train Batch Loss: 0.016550084576010704\n",
      "Train Batch Loss: 0.016279809176921844\n",
      "Train Batch Loss: 0.018902231007814407\n",
      "Train Batch Loss: 0.013995014131069183\n",
      "Train Batch Loss: 0.012954790145158768\n",
      "Train Batch Loss: 0.013766730204224586\n",
      "Train Batch Loss: 0.014505173079669476\n",
      "Train Batch Loss: 0.016402896493673325\n",
      "Train Batch Loss: 0.014116371050477028\n",
      "Train Batch Loss: 0.012607157230377197\n",
      "Train Batch Loss: 0.016056396067142487\n",
      "Train Batch Loss: 0.014596972614526749\n",
      "Train Batch Loss: 0.012538867071270943\n",
      "Train Batch Loss: 0.014408971183001995\n",
      "Train Batch Loss: 0.015285669825971127\n",
      "Train Batch Loss: 0.018138311803340912\n",
      "Train Batch Loss: 0.014212317764759064\n",
      "Train Batch Loss: 0.01994163915514946\n",
      "Train Batch Loss: 0.013747410848736763\n",
      "Train Batch Loss: 0.013459712266921997\n",
      "Train Batch Loss: 0.013195307925343513\n",
      "Train Batch Loss: 0.014260713942348957\n",
      "Train Batch Loss: 0.01788238249719143\n",
      "Train Batch Loss: 0.015356345102190971\n",
      "Train Batch Loss: 0.013700339943170547\n",
      "Train Batch Loss: 0.01860520988702774\n",
      "Train Batch Loss: 0.011946868151426315\n",
      "Train Batch Loss: 0.01606816053390503\n",
      "Train Batch Loss: 0.014773628674447536\n",
      "Train Batch Loss: 0.014726238325238228\n",
      "Train Batch Loss: 0.01274956576526165\n",
      "Train Batch Loss: 0.012869393453001976\n",
      "Train Batch Loss: 0.019818536937236786\n",
      "Train Batch Loss: 0.014513516798615456\n",
      "Train Batch Loss: 0.014090136624872684\n",
      "Train Batch Loss: 0.01731095276772976\n",
      "Train Batch Loss: 0.014817067421972752\n",
      "Train Batch Loss: 0.01895679160952568\n",
      "Train Batch Loss: 0.016977496445178986\n",
      "Train Batch Loss: 0.017041802406311035\n",
      "Train Batch Loss: 0.01643337681889534\n",
      "Train Batch Loss: 0.01735416240990162\n",
      "Train Batch Loss: 0.017056405544281006\n",
      "Train Batch Loss: 0.015663158148527145\n",
      "Train Batch Loss: 0.012579290196299553\n",
      "Train Batch Loss: 0.014874394983053207\n",
      "Train Batch Loss: 0.013126244768500328\n",
      "Train Batch Loss: 0.012475441209971905\n",
      "Train Batch Loss: 0.01681261882185936\n",
      "Train Batch Loss: 0.011859266087412834\n",
      "Train Batch Loss: 0.01572292670607567\n",
      "Train Batch Loss: 0.0182284377515316\n",
      "Train Batch Loss: 0.015939336270093918\n",
      "Train Batch Loss: 0.013077063485980034\n",
      "Train Batch Loss: 0.01785394176840782\n",
      "Train Batch Loss: 0.016394786536693573\n",
      "Train Batch Loss: 0.015982039272785187\n",
      "Train Batch Loss: 0.01402309350669384\n",
      "Train Batch Loss: 0.012973139993846416\n",
      "Train Batch Loss: 0.016103345900774002\n",
      "Train Batch Loss: 0.015030581504106522\n",
      "Train Batch Loss: 0.01308441162109375\n",
      "Train Batch Loss: 0.014171408489346504\n",
      "Train Batch Loss: 0.014079295098781586\n",
      "Train Batch Loss: 0.013761147856712341\n",
      "Train Batch Loss: 0.01435129065066576\n",
      "Train Batch Loss: 0.012097029015421867\n",
      "Train Batch Loss: 0.016677023842930794\n",
      "Train Batch Loss: 0.020251384004950523\n",
      "Train Batch Loss: 0.012357018887996674\n",
      "Train Batch Loss: 0.013366629369556904\n",
      "Train Batch Loss: 0.016320262104272842\n",
      "Train Batch Loss: 0.017646217718720436\n",
      "Train Batch Loss: 0.014534024521708488\n",
      "Train Batch Loss: 0.017884301021695137\n",
      "Train Batch Loss: 0.013670574873685837\n",
      "Train Batch Loss: 0.015312379226088524\n",
      "Train Batch Loss: 0.018613245338201523\n",
      "Train Batch Loss: 0.014443473890423775\n",
      "Train Batch Loss: 0.013138051144778728\n",
      "Train Batch Loss: 0.013325700536370277\n",
      "Train Batch Loss: 0.01477107871323824\n",
      "Train Batch Loss: 0.016699858009815216\n",
      "Train Batch Loss: 0.017128102481365204\n",
      "Train Batch Loss: 0.013341096229851246\n",
      "Train Batch Loss: 0.015706341713666916\n",
      "Train Batch Loss: 0.017478419467806816\n",
      "Train Batch Loss: 0.018806451931595802\n",
      "Train Batch Loss: 0.015542974695563316\n",
      "Train Batch Loss: 0.015975136309862137\n",
      "Train Batch Loss: 0.013161025941371918\n",
      "Train Batch Loss: 0.014294138178229332\n",
      "Train Batch Loss: 0.015902075916528702\n",
      "Train Batch Loss: 0.015957264229655266\n",
      "Train Batch Loss: 0.017897896468639374\n",
      "Train Batch Loss: 0.012496654875576496\n",
      "Train Batch Loss: 0.014164936728775501\n",
      "Train Batch Loss: 0.01348030474036932\n",
      "Train Batch Loss: 0.0166470929980278\n",
      "Train Batch Loss: 0.020241957157850266\n",
      "Train Batch Loss: 0.01504642702639103\n",
      "Train Batch Loss: 0.01283217966556549\n",
      "Train Batch Loss: 0.019012998789548874\n",
      "Train Batch Loss: 0.014180936850607395\n",
      "Train Batch Loss: 0.015345713123679161\n",
      "Train Batch Loss: 0.01572064682841301\n",
      "Train Batch Loss: 0.014964592643082142\n",
      "Train Batch Loss: 0.015385854989290237\n",
      "Train Batch Loss: 0.018106993287801743\n",
      "Train Batch Loss: 0.011891892179846764\n",
      "Train Batch Loss: 0.01377007458359003\n",
      "Train Batch Loss: 0.014134909026324749\n",
      "Train Batch Loss: 0.019102878868579865\n",
      "Train Batch Loss: 0.014716512523591518\n",
      "Train Batch Loss: 0.018766818568110466\n",
      "Train Batch Loss: 0.016424095258116722\n",
      "Train Batch Loss: 0.01826484315097332\n",
      "Train Batch Loss: 0.012068163603544235\n",
      "Train Batch Loss: 0.016921084374189377\n",
      "Train Batch Loss: 0.015085901133716106\n",
      "Train Batch Loss: 0.016123253852128983\n",
      "Train Batch Loss: 0.017599456012248993\n",
      "Train Batch Loss: 0.019122786819934845\n",
      "Train Batch Loss: 0.013018200173974037\n",
      "Train Batch Loss: 0.01371927559375763\n",
      "Train Batch Loss: 0.013741458766162395\n",
      "Train Batch Loss: 0.016287021338939667\n",
      "Train Batch Loss: 0.016554832458496094\n",
      "Train Batch Loss: 0.014952370896935463\n",
      "Train Batch Loss: 0.012859033420681953\n",
      "Train Batch Loss: 0.012871179729700089\n",
      "Train Batch Loss: 0.012249383144080639\n",
      "Train Batch Loss: 0.01422712579369545\n",
      "Train Batch Loss: 0.01247535552829504\n",
      "Train Batch Loss: 0.01830291375517845\n",
      "Train Batch Loss: 0.014361243695020676\n",
      "Train Batch Loss: 0.020912673324346542\n",
      "Train Batch Loss: 0.0168489757925272\n",
      "Train Batch Loss: 0.015272807329893112\n",
      "Train Batch Loss: 0.01835636794567108\n",
      "Train Batch Loss: 0.017268970608711243\n",
      "Train Batch Loss: 0.01854776218533516\n",
      "Train Batch Loss: 0.015903346240520477\n",
      "Train Batch Loss: 0.012306819669902325\n",
      "Train Batch Loss: 0.017950255423784256\n",
      "Train Batch Loss: 0.015104658901691437\n",
      "Train Batch Loss: 0.014975003898143768\n",
      "Train Batch Loss: 0.016378069296479225\n",
      "Train Batch Loss: 0.017983093857765198\n",
      "Train Batch Loss: 0.010288357734680176\n",
      "Train Batch Loss: 0.011470324359834194\n",
      "Train Batch Loss: 0.015471696853637695\n",
      "Train Batch Loss: 0.014434239827096462\n",
      "Train Batch Loss: 0.016131557524204254\n",
      "Train Batch Loss: 0.015326257795095444\n",
      "Train Batch Loss: 0.016787845641374588\n",
      "Train Batch Loss: 0.014673852361738682\n",
      "Train Batch Loss: 0.01301519013941288\n",
      "Train Batch Loss: 0.01933266595005989\n",
      "Train Batch Loss: 0.016390442848205566\n",
      "Train Batch Loss: 0.013238601386547089\n",
      "Train Batch Loss: 0.01777273789048195\n",
      "Train Batch Loss: 0.01552730891853571\n",
      "Train Batch Loss: 0.013680864125490189\n",
      "Train Batch Loss: 0.011702291667461395\n",
      "Train Batch Loss: 0.01243598572909832\n",
      "Train Batch Loss: 0.01736140064895153\n",
      "Train Batch Loss: 0.01754719391465187\n",
      "Train Batch Loss: 0.015927854925394058\n",
      "Train Batch Loss: 0.015620887279510498\n",
      "Train Batch Loss: 0.01758713461458683\n",
      "Train Batch Loss: 0.01706576906144619\n",
      "Train Batch Loss: 0.01589723490178585\n",
      "Train Batch Loss: 0.01591791957616806\n",
      "Train Batch Loss: 0.019004441797733307\n",
      "Train Batch Loss: 0.012002050876617432\n",
      "Train Batch Loss: 0.015373487025499344\n",
      "Train Batch Loss: 0.014194346964359283\n",
      "Train Batch Loss: 0.01353703998029232\n",
      "Train Batch Loss: 0.013424982316792011\n",
      "Train Batch Loss: 0.02081570029258728\n",
      "Train Batch Loss: 0.014431828632950783\n",
      "Train Batch Loss: 0.017313074320554733\n",
      "Train Batch Loss: 0.015165913850069046\n",
      "Train Batch Loss: 0.01420813612639904\n",
      "Train Batch Loss: 0.016016952693462372\n",
      "Train Batch Loss: 0.015506034716963768\n",
      "Train Batch Loss: 0.01835581287741661\n",
      "Train Batch Loss: 0.019893933087587357\n",
      "Train Batch Loss: 0.011772152036428452\n",
      "Train Batch Loss: 0.01737077906727791\n",
      "Train Batch Loss: 0.015688836574554443\n",
      "Train Batch Loss: 0.018736764788627625\n",
      "Train Batch Loss: 0.01834913343191147\n",
      "Train Batch Loss: 0.0171834584325552\n",
      "Train Batch Loss: 0.014705386012792587\n",
      "Train Batch Loss: 0.014836154878139496\n",
      "Train Batch Loss: 0.01610001176595688\n",
      "Train Batch Loss: 0.017657313495874405\n",
      "Train Batch Loss: 0.012976180762052536\n",
      "Train Batch Loss: 0.012655874714255333\n",
      "Train Batch Loss: 0.01377798244357109\n",
      "Train Batch Loss: 0.016518203541636467\n",
      "Train Batch Loss: 0.014430077746510506\n",
      "Train Batch Loss: 0.01408856175839901\n",
      "Train Batch Loss: 0.016731102019548416\n",
      "Train Batch Loss: 0.01834920421242714\n",
      "Train Batch Loss: 0.015912670642137527\n",
      "Train Batch Loss: 0.012080991640686989\n",
      "Train Batch Loss: 0.010976387187838554\n",
      "Train Batch Loss: 0.016191579401493073\n",
      "Train Batch Loss: 0.014663592912256718\n",
      "Train Batch Loss: 0.013460719957947731\n",
      "Train Batch Loss: 0.015240056440234184\n",
      "Train Batch Loss: 0.01554412767291069\n",
      "Train Batch Loss: 0.015076329931616783\n",
      "Train Batch Loss: 0.01789757050573826\n",
      "Train Batch Loss: 0.016161181032657623\n",
      "Train Batch Loss: 0.015447346493601799\n",
      "Train Batch Loss: 0.012004876509308815\n",
      "Train Batch Loss: 0.01824507862329483\n",
      "Train Batch Loss: 0.013962659984827042\n",
      "Train Batch Loss: 0.0170738585293293\n",
      "Train Batch Loss: 0.018093332648277283\n",
      "Train Batch Loss: 0.013836955651640892\n",
      "Train Batch Loss: 0.013528959825634956\n",
      "Train Batch Loss: 0.014678758569061756\n",
      "Train Batch Loss: 0.017926793545484543\n",
      "valid Batch Loss: 0.01896221935749054\n",
      "valid Batch Loss: 0.016610827296972275\n",
      "valid Batch Loss: 0.019557759165763855\n",
      "valid Batch Loss: 0.0183096993714571\n",
      "valid Batch Loss: 0.011467283591628075\n",
      "valid Batch Loss: 0.018692702054977417\n",
      "valid Batch Loss: 0.01214403472840786\n",
      "valid Batch Loss: 0.016412552446126938\n",
      "valid Batch Loss: 0.015463978052139282\n",
      "valid Batch Loss: 0.016449235379695892\n",
      "valid Batch Loss: 0.017255514860153198\n",
      "valid Batch Loss: 0.01531977392733097\n",
      "valid Batch Loss: 0.01372713502496481\n",
      "valid Batch Loss: 0.014457755722105503\n",
      "valid Batch Loss: 0.016777459532022476\n",
      "valid Batch Loss: 0.015560622327029705\n",
      "valid Batch Loss: 0.021571911871433258\n",
      "valid Batch Loss: 0.012471010908484459\n",
      "valid Batch Loss: 0.0195612795650959\n",
      "valid Batch Loss: 0.014490503817796707\n",
      "valid Batch Loss: 0.015850866213440895\n",
      "valid Batch Loss: 0.015159155242145061\n",
      "valid Batch Loss: 0.013632405549287796\n",
      "valid Batch Loss: 0.016687560826539993\n",
      "valid Batch Loss: 0.012129167094826698\n",
      "valid Batch Loss: 0.016526944935321808\n",
      "valid Batch Loss: 0.01567142829298973\n",
      "valid Batch Loss: 0.01722329668700695\n",
      "valid Batch Loss: 0.016794711351394653\n",
      "valid Batch Loss: 0.015042947605252266\n",
      "valid Batch Loss: 0.014327030628919601\n",
      "valid Batch Loss: 0.01501479372382164\n",
      "valid Batch Loss: 0.018319323658943176\n",
      "valid Batch Loss: 0.012565476819872856\n",
      "valid Batch Loss: 0.01751440204679966\n",
      "valid Batch Loss: 0.01667420193552971\n",
      "valid Batch Loss: 0.018797943368554115\n",
      "valid Batch Loss: 0.01904655434191227\n",
      "valid Batch Loss: 0.017986979335546494\n",
      "valid Batch Loss: 0.014858252368867397\n",
      "valid Batch Loss: 0.013883796520531178\n",
      "valid Batch Loss: 0.013077514246106148\n",
      "valid Batch Loss: 0.014530418440699577\n",
      "valid Batch Loss: 0.017523963004350662\n",
      "valid Batch Loss: 0.01260419562458992\n",
      "valid Batch Loss: 0.017835550010204315\n",
      "valid Batch Loss: 0.015571331605315208\n",
      "valid Batch Loss: 0.01820031926035881\n",
      "valid Batch Loss: 0.017180589959025383\n",
      "valid Batch Loss: 0.019870508462190628\n",
      "valid Batch Loss: 0.015275945886969566\n",
      "valid Batch Loss: 0.01605967804789543\n",
      "valid Batch Loss: 0.0183553546667099\n",
      "valid Batch Loss: 0.015346630476415157\n",
      "valid Batch Loss: 0.020496245473623276\n",
      "valid Batch Loss: 0.015095220878720284\n",
      "valid Batch Loss: 0.013861125335097313\n",
      "valid Batch Loss: 0.013497669249773026\n",
      "valid Batch Loss: 0.014994269236922264\n",
      "valid Batch Loss: 0.014473720453679562\n",
      "valid Batch Loss: 0.014510270208120346\n",
      "valid Batch Loss: 0.013589130714535713\n",
      "Epoch: 6 | Train Loss: 0.015555078163743019 | Valid Loss: 0.016221152618527412\n",
      "\n",
      "Train Batch Loss: 0.01599474623799324\n",
      "Train Batch Loss: 0.0186648890376091\n",
      "Train Batch Loss: 0.015245215967297554\n",
      "Train Batch Loss: 0.01640898548066616\n",
      "Train Batch Loss: 0.011014798656105995\n",
      "Train Batch Loss: 0.011252600699663162\n",
      "Train Batch Loss: 0.010808652266860008\n",
      "Train Batch Loss: 0.01676161028444767\n",
      "Train Batch Loss: 0.01623164862394333\n",
      "Train Batch Loss: 0.015565110370516777\n",
      "Train Batch Loss: 0.01684270054101944\n",
      "Train Batch Loss: 0.015492139384150505\n",
      "Train Batch Loss: 0.013352402485907078\n",
      "Train Batch Loss: 0.01423913985490799\n",
      "Train Batch Loss: 0.010333993472158909\n",
      "Train Batch Loss: 0.016258880496025085\n",
      "Train Batch Loss: 0.015580076724290848\n",
      "Train Batch Loss: 0.011404779739677906\n",
      "Train Batch Loss: 0.016685983166098595\n",
      "Train Batch Loss: 0.015393014997243881\n",
      "Train Batch Loss: 0.014778674580156803\n",
      "Train Batch Loss: 0.019813593477010727\n",
      "Train Batch Loss: 0.015235263854265213\n",
      "Train Batch Loss: 0.017388159409165382\n",
      "Train Batch Loss: 0.019091036170721054\n",
      "Train Batch Loss: 0.014728503301739693\n",
      "Train Batch Loss: 0.014130661264061928\n",
      "Train Batch Loss: 0.018345948308706284\n",
      "Train Batch Loss: 0.01612326316535473\n",
      "Train Batch Loss: 0.01635683700442314\n",
      "Train Batch Loss: 0.012577055022120476\n",
      "Train Batch Loss: 0.01497410424053669\n",
      "Train Batch Loss: 0.01827535778284073\n",
      "Train Batch Loss: 0.014333883300423622\n",
      "Train Batch Loss: 0.016818970441818237\n",
      "Train Batch Loss: 0.011413347907364368\n",
      "Train Batch Loss: 0.016363073140382767\n",
      "Train Batch Loss: 0.01609668880701065\n",
      "Train Batch Loss: 0.017218321561813354\n",
      "Train Batch Loss: 0.016293443739414215\n",
      "Train Batch Loss: 0.014017474837601185\n",
      "Train Batch Loss: 0.019472625106573105\n",
      "Train Batch Loss: 0.01632203906774521\n",
      "Train Batch Loss: 0.015475066378712654\n",
      "Train Batch Loss: 0.01400848850607872\n",
      "Train Batch Loss: 0.014089173637330532\n",
      "Train Batch Loss: 0.013721399940550327\n",
      "Train Batch Loss: 0.013781119138002396\n",
      "Train Batch Loss: 0.013422885909676552\n",
      "Train Batch Loss: 0.015496882610023022\n",
      "Train Batch Loss: 0.019219353795051575\n",
      "Train Batch Loss: 0.01634489744901657\n",
      "Train Batch Loss: 0.01868145540356636\n",
      "Train Batch Loss: 0.01796429604291916\n",
      "Train Batch Loss: 0.01799752190709114\n",
      "Train Batch Loss: 0.013378845527768135\n",
      "Train Batch Loss: 0.015349199064075947\n",
      "Train Batch Loss: 0.01660991460084915\n",
      "Train Batch Loss: 0.01853298395872116\n",
      "Train Batch Loss: 0.018298786133527756\n",
      "Train Batch Loss: 0.013659841381013393\n",
      "Train Batch Loss: 0.017139170318841934\n",
      "Train Batch Loss: 0.014878285117447376\n",
      "Train Batch Loss: 0.01416197419166565\n",
      "Train Batch Loss: 0.017447685822844505\n",
      "Train Batch Loss: 0.015380797907710075\n",
      "Train Batch Loss: 0.014002666808664799\n",
      "Train Batch Loss: 0.017212335020303726\n",
      "Train Batch Loss: 0.012235532514750957\n",
      "Train Batch Loss: 0.015651602298021317\n",
      "Train Batch Loss: 0.015004448592662811\n",
      "Train Batch Loss: 0.014911187812685966\n",
      "Train Batch Loss: 0.013771265745162964\n",
      "Train Batch Loss: 0.019233018159866333\n",
      "Train Batch Loss: 0.013583232648670673\n",
      "Train Batch Loss: 0.014053966850042343\n",
      "Train Batch Loss: 0.014088387601077557\n",
      "Train Batch Loss: 0.012198219075798988\n",
      "Train Batch Loss: 0.013327525928616524\n",
      "Train Batch Loss: 0.014281537383794785\n",
      "Train Batch Loss: 0.019228503108024597\n",
      "Train Batch Loss: 0.014007450081408024\n",
      "Train Batch Loss: 0.013525878079235554\n",
      "Train Batch Loss: 0.013268680311739445\n",
      "Train Batch Loss: 0.01489801425486803\n",
      "Train Batch Loss: 0.01589398831129074\n",
      "Train Batch Loss: 0.011581185273826122\n",
      "Train Batch Loss: 0.01782042719423771\n",
      "Train Batch Loss: 0.014943314716219902\n",
      "Train Batch Loss: 0.014800046570599079\n",
      "Train Batch Loss: 0.016675889492034912\n",
      "Train Batch Loss: 0.014565459452569485\n",
      "Train Batch Loss: 0.014036914333701134\n",
      "Train Batch Loss: 0.015078755095601082\n",
      "Train Batch Loss: 0.015265156514942646\n",
      "Train Batch Loss: 0.013726390898227692\n",
      "Train Batch Loss: 0.01400566753000021\n",
      "Train Batch Loss: 0.014963443391025066\n",
      "Train Batch Loss: 0.017243634909391403\n",
      "Train Batch Loss: 0.018245065584778786\n",
      "Train Batch Loss: 0.014331402257084846\n",
      "Train Batch Loss: 0.013895783573389053\n",
      "Train Batch Loss: 0.017013542354106903\n",
      "Train Batch Loss: 0.013715321198105812\n",
      "Train Batch Loss: 0.017172956839203835\n",
      "Train Batch Loss: 0.014321445487439632\n",
      "Train Batch Loss: 0.01567968912422657\n",
      "Train Batch Loss: 0.015529357828199863\n",
      "Train Batch Loss: 0.01613030582666397\n",
      "Train Batch Loss: 0.015502110123634338\n",
      "Train Batch Loss: 0.015595799311995506\n",
      "Train Batch Loss: 0.01558186486363411\n",
      "Train Batch Loss: 0.017177600413560867\n",
      "Train Batch Loss: 0.017040131613612175\n",
      "Train Batch Loss: 0.01637190766632557\n",
      "Train Batch Loss: 0.018413294106721878\n",
      "Train Batch Loss: 0.015001844614744186\n",
      "Train Batch Loss: 0.0164573322981596\n",
      "Train Batch Loss: 0.016744717955589294\n",
      "Train Batch Loss: 0.0166219100356102\n",
      "Train Batch Loss: 0.016875751316547394\n",
      "Train Batch Loss: 0.01729741320014\n",
      "Train Batch Loss: 0.0137352729216218\n",
      "Train Batch Loss: 0.01247977465391159\n",
      "Train Batch Loss: 0.01515293400734663\n",
      "Train Batch Loss: 0.016349362209439278\n",
      "Train Batch Loss: 0.014324501156806946\n",
      "Train Batch Loss: 0.01326306164264679\n",
      "Train Batch Loss: 0.01400804053992033\n",
      "Train Batch Loss: 0.013268091715872288\n",
      "Train Batch Loss: 0.015412529930472374\n",
      "Train Batch Loss: 0.015629753470420837\n",
      "Train Batch Loss: 0.012262946926057339\n",
      "Train Batch Loss: 0.012267012149095535\n",
      "Train Batch Loss: 0.0176384374499321\n",
      "Train Batch Loss: 0.01411665603518486\n",
      "Train Batch Loss: 0.013973335735499859\n",
      "Train Batch Loss: 0.016445782035589218\n",
      "Train Batch Loss: 0.01261901669204235\n",
      "Train Batch Loss: 0.013796818442642689\n",
      "Train Batch Loss: 0.013598965480923653\n",
      "Train Batch Loss: 0.017961975187063217\n",
      "Train Batch Loss: 0.015046456828713417\n",
      "Train Batch Loss: 0.014489338733255863\n",
      "Train Batch Loss: 0.014700576663017273\n",
      "Train Batch Loss: 0.014277826994657516\n",
      "Train Batch Loss: 0.015891116112470627\n",
      "Train Batch Loss: 0.018532203510403633\n",
      "Train Batch Loss: 0.013064900413155556\n",
      "Train Batch Loss: 0.016097549349069595\n",
      "Train Batch Loss: 0.02100485749542713\n",
      "Train Batch Loss: 0.0162538830190897\n",
      "Train Batch Loss: 0.01577943004667759\n",
      "Train Batch Loss: 0.014597786590456963\n",
      "Train Batch Loss: 0.016552653163671494\n",
      "Train Batch Loss: 0.015333357267081738\n",
      "Train Batch Loss: 0.014819078147411346\n",
      "Train Batch Loss: 0.016760455444455147\n",
      "Train Batch Loss: 0.012071831151843071\n",
      "Train Batch Loss: 0.018197685480117798\n",
      "Train Batch Loss: 0.013273789547383785\n",
      "Train Batch Loss: 0.018792033195495605\n",
      "Train Batch Loss: 0.014308715239167213\n",
      "Train Batch Loss: 0.01759895123541355\n",
      "Train Batch Loss: 0.016511574387550354\n",
      "Train Batch Loss: 0.014942625537514687\n",
      "Train Batch Loss: 0.012983756139874458\n",
      "Train Batch Loss: 0.014921410009264946\n",
      "Train Batch Loss: 0.015105236321687698\n",
      "Train Batch Loss: 0.014429198578000069\n",
      "Train Batch Loss: 0.013276912271976471\n",
      "Train Batch Loss: 0.014029543846845627\n",
      "Train Batch Loss: 0.013020437210798264\n",
      "Train Batch Loss: 0.01817537471652031\n",
      "Train Batch Loss: 0.015480714850127697\n",
      "Train Batch Loss: 0.01520156767219305\n",
      "Train Batch Loss: 0.015959661453962326\n",
      "Train Batch Loss: 0.017600664868950844\n",
      "Train Batch Loss: 0.014846842736005783\n",
      "Train Batch Loss: 0.014675173908472061\n",
      "Train Batch Loss: 0.014490224421024323\n",
      "Train Batch Loss: 0.018452001735568047\n",
      "Train Batch Loss: 0.015908803790807724\n",
      "Train Batch Loss: 0.014846637845039368\n",
      "Train Batch Loss: 0.015061156824231148\n",
      "Train Batch Loss: 0.013027830980718136\n",
      "Train Batch Loss: 0.01760457642376423\n",
      "Train Batch Loss: 0.015001569874584675\n",
      "Train Batch Loss: 0.015819702297449112\n",
      "Train Batch Loss: 0.01415404211729765\n",
      "Train Batch Loss: 0.020062172785401344\n",
      "Train Batch Loss: 0.016019612550735474\n",
      "Train Batch Loss: 0.01535448431968689\n",
      "Train Batch Loss: 0.0159474927932024\n",
      "Train Batch Loss: 0.015530109405517578\n",
      "Train Batch Loss: 0.014558290131390095\n",
      "Train Batch Loss: 0.013701405376195908\n",
      "Train Batch Loss: 0.013426635414361954\n",
      "Train Batch Loss: 0.018186265602707863\n",
      "Train Batch Loss: 0.020721053704619408\n",
      "Train Batch Loss: 0.015076106414198875\n",
      "Train Batch Loss: 0.015036542899906635\n",
      "Train Batch Loss: 0.020133398473262787\n",
      "Train Batch Loss: 0.014794422313570976\n",
      "Train Batch Loss: 0.017317969352006912\n",
      "Train Batch Loss: 0.012826051563024521\n",
      "Train Batch Loss: 0.015191043727099895\n",
      "Train Batch Loss: 0.015735063701868057\n",
      "Train Batch Loss: 0.019254298880696297\n",
      "Train Batch Loss: 0.01393037661910057\n",
      "Train Batch Loss: 0.015991823747754097\n",
      "Train Batch Loss: 0.016181856393814087\n",
      "Train Batch Loss: 0.015019822865724564\n",
      "Train Batch Loss: 0.015134087763726711\n",
      "Train Batch Loss: 0.012467066757380962\n",
      "Train Batch Loss: 0.014451013877987862\n",
      "Train Batch Loss: 0.013075381517410278\n",
      "Train Batch Loss: 0.01458093523979187\n",
      "Train Batch Loss: 0.014766442589461803\n",
      "Train Batch Loss: 0.016338618472218513\n",
      "Train Batch Loss: 0.017075438052415848\n",
      "Train Batch Loss: 0.01428602822124958\n",
      "Train Batch Loss: 0.016681069508194923\n",
      "Train Batch Loss: 0.012431478127837181\n",
      "Train Batch Loss: 0.017416903749108315\n",
      "Train Batch Loss: 0.018254335969686508\n",
      "Train Batch Loss: 0.015623686835169792\n",
      "Train Batch Loss: 0.014932121150195599\n",
      "Train Batch Loss: 0.01571866124868393\n",
      "Train Batch Loss: 0.011827312409877777\n",
      "Train Batch Loss: 0.011101020500063896\n",
      "Train Batch Loss: 0.016272837296128273\n",
      "Train Batch Loss: 0.016888393089175224\n",
      "Train Batch Loss: 0.016926098614931107\n",
      "Train Batch Loss: 0.01403388287872076\n",
      "Train Batch Loss: 0.01351678092032671\n",
      "Train Batch Loss: 0.015673881396651268\n",
      "Train Batch Loss: 0.01732497103512287\n",
      "Train Batch Loss: 0.013568677939474583\n",
      "Train Batch Loss: 0.013081449083983898\n",
      "Train Batch Loss: 0.017643379047513008\n",
      "Train Batch Loss: 0.014774253591895103\n",
      "Train Batch Loss: 0.015201127156615257\n",
      "Train Batch Loss: 0.012679148465394974\n",
      "Train Batch Loss: 0.017696619033813477\n",
      "Train Batch Loss: 0.015980441123247147\n",
      "Train Batch Loss: 0.015587711706757545\n",
      "Train Batch Loss: 0.014585698023438454\n",
      "Train Batch Loss: 0.014096584171056747\n",
      "Train Batch Loss: 0.013813762925565243\n",
      "valid Batch Loss: 0.018790502101182938\n",
      "valid Batch Loss: 0.016677379608154297\n",
      "valid Batch Loss: 0.01933591440320015\n",
      "valid Batch Loss: 0.01823844388127327\n",
      "valid Batch Loss: 0.011966348625719547\n",
      "valid Batch Loss: 0.018307901918888092\n",
      "valid Batch Loss: 0.012502124533057213\n",
      "valid Batch Loss: 0.016431085765361786\n",
      "valid Batch Loss: 0.01563253253698349\n",
      "valid Batch Loss: 0.016494406387209892\n",
      "valid Batch Loss: 0.017295243218541145\n",
      "valid Batch Loss: 0.015354601666331291\n",
      "valid Batch Loss: 0.013680594973266125\n",
      "valid Batch Loss: 0.014707956463098526\n",
      "valid Batch Loss: 0.01673201285302639\n",
      "valid Batch Loss: 0.015487439930438995\n",
      "valid Batch Loss: 0.021708283573389053\n",
      "valid Batch Loss: 0.012808697298169136\n",
      "valid Batch Loss: 0.019821949303150177\n",
      "valid Batch Loss: 0.01447946485131979\n",
      "valid Batch Loss: 0.015868613496422768\n",
      "valid Batch Loss: 0.015112526714801788\n",
      "valid Batch Loss: 0.013610601425170898\n",
      "valid Batch Loss: 0.016626857221126556\n",
      "valid Batch Loss: 0.01223880983889103\n",
      "valid Batch Loss: 0.01650187186896801\n",
      "valid Batch Loss: 0.015829693526029587\n",
      "valid Batch Loss: 0.01731332764029503\n",
      "valid Batch Loss: 0.016973260790109634\n",
      "valid Batch Loss: 0.015164795331656933\n",
      "valid Batch Loss: 0.014665546827018261\n",
      "valid Batch Loss: 0.014819757081568241\n",
      "valid Batch Loss: 0.01809854432940483\n",
      "valid Batch Loss: 0.012644367292523384\n",
      "valid Batch Loss: 0.017584508284926414\n",
      "valid Batch Loss: 0.01691889390349388\n",
      "valid Batch Loss: 0.018454331904649734\n",
      "valid Batch Loss: 0.018858741968870163\n",
      "valid Batch Loss: 0.018094630911946297\n",
      "valid Batch Loss: 0.014922218397259712\n",
      "valid Batch Loss: 0.014133589342236519\n",
      "valid Batch Loss: 0.013394998386502266\n",
      "valid Batch Loss: 0.014714529737830162\n",
      "valid Batch Loss: 0.01761230081319809\n",
      "valid Batch Loss: 0.01296988781541586\n",
      "valid Batch Loss: 0.017456376925110817\n",
      "valid Batch Loss: 0.015672508627176285\n",
      "valid Batch Loss: 0.01810208149254322\n",
      "valid Batch Loss: 0.017485443502664566\n",
      "valid Batch Loss: 0.019407689571380615\n",
      "valid Batch Loss: 0.015378054231405258\n",
      "valid Batch Loss: 0.01615154556930065\n",
      "valid Batch Loss: 0.01829865202307701\n",
      "valid Batch Loss: 0.01539972797036171\n",
      "valid Batch Loss: 0.020311059430241585\n",
      "valid Batch Loss: 0.014977396465837955\n",
      "valid Batch Loss: 0.01404743641614914\n",
      "valid Batch Loss: 0.013348706066608429\n",
      "valid Batch Loss: 0.014989767223596573\n",
      "valid Batch Loss: 0.014662351459264755\n",
      "valid Batch Loss: 0.014499273151159286\n",
      "valid Batch Loss: 0.013853603042662144\n",
      "Epoch: 7 | Train Loss: 0.015555212274193764 | Valid Loss: 0.01624339260160923\n",
      "\n",
      "Train Batch Loss: 0.017686551436781883\n",
      "Train Batch Loss: 0.014462857507169247\n",
      "Train Batch Loss: 0.013006800785660744\n",
      "Train Batch Loss: 0.017893481999635696\n",
      "Train Batch Loss: 0.011823112145066261\n",
      "Train Batch Loss: 0.016202732920646667\n",
      "Train Batch Loss: 0.013133618980646133\n",
      "Train Batch Loss: 0.01453968696296215\n",
      "Train Batch Loss: 0.015168098732829094\n",
      "Train Batch Loss: 0.014052767306566238\n",
      "Train Batch Loss: 0.00989924743771553\n",
      "Train Batch Loss: 0.01866954192519188\n",
      "Train Batch Loss: 0.01682545617222786\n",
      "Train Batch Loss: 0.01195814460515976\n",
      "Train Batch Loss: 0.014637070707976818\n",
      "Train Batch Loss: 0.01781061291694641\n",
      "Train Batch Loss: 0.01914328709244728\n",
      "Train Batch Loss: 0.012474577873945236\n",
      "Train Batch Loss: 0.015221689827740192\n",
      "Train Batch Loss: 0.013804661110043526\n",
      "Train Batch Loss: 0.01589752361178398\n",
      "Train Batch Loss: 0.01533108577132225\n",
      "Train Batch Loss: 0.016461219638586044\n",
      "Train Batch Loss: 0.013683641329407692\n",
      "Train Batch Loss: 0.014425035566091537\n",
      "Train Batch Loss: 0.014854298904538155\n",
      "Train Batch Loss: 0.01268062274903059\n",
      "Train Batch Loss: 0.01711832359433174\n",
      "Train Batch Loss: 0.012754249386489391\n",
      "Train Batch Loss: 0.012777190655469894\n",
      "Train Batch Loss: 0.01625031791627407\n",
      "Train Batch Loss: 0.01942284032702446\n",
      "Train Batch Loss: 0.014869743958115578\n",
      "Train Batch Loss: 0.01772802323102951\n",
      "Train Batch Loss: 0.017094001173973083\n",
      "Train Batch Loss: 0.014715498313307762\n",
      "Train Batch Loss: 0.016885194927453995\n",
      "Train Batch Loss: 0.01593795418739319\n",
      "Train Batch Loss: 0.01630784198641777\n",
      "Train Batch Loss: 0.012564785778522491\n",
      "Train Batch Loss: 0.014332229271531105\n",
      "Train Batch Loss: 0.013945315033197403\n",
      "Train Batch Loss: 0.016147758811712265\n",
      "Train Batch Loss: 0.01476756390184164\n",
      "Train Batch Loss: 0.01607663184404373\n",
      "Train Batch Loss: 0.016259945929050446\n",
      "Train Batch Loss: 0.018481772392988205\n",
      "Train Batch Loss: 0.01595326140522957\n",
      "Train Batch Loss: 0.014394771307706833\n",
      "Train Batch Loss: 0.017655357718467712\n",
      "Train Batch Loss: 0.016552738845348358\n",
      "Train Batch Loss: 0.010284541174769402\n",
      "Train Batch Loss: 0.012791289016604424\n",
      "Train Batch Loss: 0.014922210946679115\n",
      "Train Batch Loss: 0.017959842458367348\n",
      "Train Batch Loss: 0.01753414049744606\n",
      "Train Batch Loss: 0.015191139653325081\n",
      "Train Batch Loss: 0.01546601951122284\n",
      "Train Batch Loss: 0.014684073626995087\n",
      "Train Batch Loss: 0.01590767875313759\n",
      "Train Batch Loss: 0.01663021370768547\n",
      "Train Batch Loss: 0.013005230575799942\n",
      "Train Batch Loss: 0.016776416450738907\n",
      "Train Batch Loss: 0.016301631927490234\n",
      "Train Batch Loss: 0.012620272114872932\n",
      "Train Batch Loss: 0.01908063143491745\n",
      "Train Batch Loss: 0.01780320145189762\n",
      "Train Batch Loss: 0.016848163679242134\n",
      "Train Batch Loss: 0.013019856065511703\n",
      "Train Batch Loss: 0.014895557425916195\n",
      "Train Batch Loss: 0.01358842570334673\n",
      "Train Batch Loss: 0.01810949295759201\n",
      "Train Batch Loss: 0.016559505835175514\n",
      "Train Batch Loss: 0.015595145523548126\n",
      "Train Batch Loss: 0.015619647689163685\n",
      "Train Batch Loss: 0.013737209141254425\n",
      "Train Batch Loss: 0.01743280701339245\n",
      "Train Batch Loss: 0.011869211681187153\n",
      "Train Batch Loss: 0.017843695357441902\n",
      "Train Batch Loss: 0.019509870558977127\n",
      "Train Batch Loss: 0.015286577865481377\n",
      "Train Batch Loss: 0.014597754925489426\n",
      "Train Batch Loss: 0.017435237765312195\n",
      "Train Batch Loss: 0.015571223571896553\n",
      "Train Batch Loss: 0.015797527506947517\n",
      "Train Batch Loss: 0.01606934890151024\n",
      "Train Batch Loss: 0.017165008932352066\n",
      "Train Batch Loss: 0.015337279066443443\n",
      "Train Batch Loss: 0.012746473774313927\n",
      "Train Batch Loss: 0.014074916020035744\n",
      "Train Batch Loss: 0.012131797149777412\n",
      "Train Batch Loss: 0.013710448518395424\n",
      "Train Batch Loss: 0.01808583363890648\n",
      "Train Batch Loss: 0.018759746104478836\n",
      "Train Batch Loss: 0.015208697877824306\n",
      "Train Batch Loss: 0.019326578825712204\n",
      "Train Batch Loss: 0.02369179204106331\n",
      "Train Batch Loss: 0.016693726181983948\n",
      "Train Batch Loss: 0.01783115416765213\n",
      "Train Batch Loss: 0.016696609556674957\n",
      "Train Batch Loss: 0.0197531059384346\n",
      "Train Batch Loss: 0.015759624540805817\n",
      "Train Batch Loss: 0.012384897097945213\n",
      "Train Batch Loss: 0.01938563771545887\n",
      "Train Batch Loss: 0.01207781583070755\n",
      "Train Batch Loss: 0.01843387819826603\n",
      "Train Batch Loss: 0.017600510269403458\n",
      "Train Batch Loss: 0.02040817216038704\n",
      "Train Batch Loss: 0.014777136966586113\n",
      "Train Batch Loss: 0.016785457730293274\n",
      "Train Batch Loss: 0.018235107883810997\n",
      "Train Batch Loss: 0.01426619291305542\n",
      "Train Batch Loss: 0.014557963237166405\n",
      "Train Batch Loss: 0.013564693741500378\n",
      "Train Batch Loss: 0.014086389914155006\n",
      "Train Batch Loss: 0.012146275490522385\n",
      "Train Batch Loss: 0.015543168410658836\n",
      "Train Batch Loss: 0.01612314209342003\n",
      "Train Batch Loss: 0.018628492951393127\n",
      "Train Batch Loss: 0.01440958958119154\n",
      "Train Batch Loss: 0.016748875379562378\n",
      "Train Batch Loss: 0.01567750610411167\n",
      "Train Batch Loss: 0.012933185324072838\n",
      "Train Batch Loss: 0.014198328368365765\n",
      "Train Batch Loss: 0.015003960579633713\n",
      "Train Batch Loss: 0.01817796565592289\n",
      "Train Batch Loss: 0.013475626707077026\n",
      "Train Batch Loss: 0.018172115087509155\n",
      "Train Batch Loss: 0.018170855939388275\n",
      "Train Batch Loss: 0.020154614001512527\n",
      "Train Batch Loss: 0.018472781404852867\n",
      "Train Batch Loss: 0.014734745025634766\n",
      "Train Batch Loss: 0.013958441093564034\n",
      "Train Batch Loss: 0.013125084340572357\n",
      "Train Batch Loss: 0.01351552177220583\n",
      "Train Batch Loss: 0.01716884784400463\n",
      "Train Batch Loss: 0.017515035346150398\n",
      "Train Batch Loss: 0.014241321012377739\n",
      "Train Batch Loss: 0.009574228897690773\n",
      "Train Batch Loss: 0.02043936774134636\n",
      "Train Batch Loss: 0.017622925341129303\n",
      "Train Batch Loss: 0.014250842854380608\n",
      "Train Batch Loss: 0.01456315629184246\n",
      "Train Batch Loss: 0.016434725373983383\n",
      "Train Batch Loss: 0.017620066180825233\n",
      "Train Batch Loss: 0.014018638990819454\n",
      "Train Batch Loss: 0.01842242106795311\n",
      "Train Batch Loss: 0.01778636872768402\n",
      "Train Batch Loss: 0.014839153736829758\n",
      "Train Batch Loss: 0.014309724792838097\n",
      "Train Batch Loss: 0.01677430421113968\n",
      "Train Batch Loss: 0.01547018252313137\n",
      "Train Batch Loss: 0.01517706923186779\n",
      "Train Batch Loss: 0.0180587749928236\n",
      "Train Batch Loss: 0.015805337578058243\n",
      "Train Batch Loss: 0.01578199677169323\n",
      "Train Batch Loss: 0.014633695594966412\n",
      "Train Batch Loss: 0.013755377382040024\n",
      "Train Batch Loss: 0.013149559497833252\n",
      "Train Batch Loss: 0.016012195497751236\n",
      "Train Batch Loss: 0.02014749124646187\n",
      "Train Batch Loss: 0.01679881662130356\n",
      "Train Batch Loss: 0.012306435033679008\n",
      "Train Batch Loss: 0.01410951092839241\n",
      "Train Batch Loss: 0.015771333128213882\n",
      "Train Batch Loss: 0.0140455961227417\n",
      "Train Batch Loss: 0.015203512273728848\n",
      "Train Batch Loss: 0.015889842063188553\n",
      "Train Batch Loss: 0.014106065034866333\n",
      "Train Batch Loss: 0.01883094012737274\n",
      "Train Batch Loss: 0.01704440265893936\n",
      "Train Batch Loss: 0.01746233180165291\n",
      "Train Batch Loss: 0.017992882058024406\n",
      "Train Batch Loss: 0.01437779888510704\n",
      "Train Batch Loss: 0.01625138521194458\n",
      "Train Batch Loss: 0.015300870873034\n",
      "Train Batch Loss: 0.01561327837407589\n",
      "Train Batch Loss: 0.01882389560341835\n",
      "Train Batch Loss: 0.013279273174703121\n",
      "Train Batch Loss: 0.015940431505441666\n",
      "Train Batch Loss: 0.01563752070069313\n",
      "Train Batch Loss: 0.014192603528499603\n",
      "Train Batch Loss: 0.012185807339847088\n",
      "Train Batch Loss: 0.013096373528242111\n",
      "Train Batch Loss: 0.015153635293245316\n",
      "Train Batch Loss: 0.015085027553141117\n",
      "Train Batch Loss: 0.015516316518187523\n",
      "Train Batch Loss: 0.01711359992623329\n",
      "Train Batch Loss: 0.016540702432394028\n",
      "Train Batch Loss: 0.019048236310482025\n",
      "Train Batch Loss: 0.019616540521383286\n",
      "Train Batch Loss: 0.011928379535675049\n",
      "Train Batch Loss: 0.012073080986738205\n",
      "Train Batch Loss: 0.011765902861952782\n",
      "Train Batch Loss: 0.015394739806652069\n",
      "Train Batch Loss: 0.016885792836546898\n",
      "Train Batch Loss: 0.012993881478905678\n",
      "Train Batch Loss: 0.015459258109331131\n",
      "Train Batch Loss: 0.014606247656047344\n",
      "Train Batch Loss: 0.015429047867655754\n",
      "Train Batch Loss: 0.014919102191925049\n",
      "Train Batch Loss: 0.015436694957315922\n",
      "Train Batch Loss: 0.017169594764709473\n",
      "Train Batch Loss: 0.017440855503082275\n",
      "Train Batch Loss: 0.01743525266647339\n",
      "Train Batch Loss: 0.01641799882054329\n",
      "Train Batch Loss: 0.014151480980217457\n",
      "Train Batch Loss: 0.01617448590695858\n",
      "Train Batch Loss: 0.01934110000729561\n",
      "Train Batch Loss: 0.013433210551738739\n",
      "Train Batch Loss: 0.015533524565398693\n",
      "Train Batch Loss: 0.015086131170392036\n",
      "Train Batch Loss: 0.012347716838121414\n",
      "Train Batch Loss: 0.017204660922288895\n",
      "Train Batch Loss: 0.011452313512563705\n",
      "Train Batch Loss: 0.016416169703006744\n",
      "Train Batch Loss: 0.016247041523456573\n",
      "Train Batch Loss: 0.015419966541230679\n",
      "Train Batch Loss: 0.010485002771019936\n",
      "Train Batch Loss: 0.02041306346654892\n",
      "Train Batch Loss: 0.014595440588891506\n",
      "Train Batch Loss: 0.014617531560361385\n",
      "Train Batch Loss: 0.014062890782952309\n",
      "Train Batch Loss: 0.01160910539329052\n",
      "Train Batch Loss: 0.015602120198309422\n",
      "Train Batch Loss: 0.012241804972290993\n",
      "Train Batch Loss: 0.013853165321052074\n",
      "Train Batch Loss: 0.017108850181102753\n",
      "Train Batch Loss: 0.015458839014172554\n",
      "Train Batch Loss: 0.015211683697998524\n",
      "Train Batch Loss: 0.01271723210811615\n",
      "Train Batch Loss: 0.01409978698939085\n",
      "Train Batch Loss: 0.012932650744915009\n",
      "Train Batch Loss: 0.01825794018805027\n",
      "Train Batch Loss: 0.019151058048009872\n",
      "Train Batch Loss: 0.015106948092579842\n",
      "Train Batch Loss: 0.013646778650581837\n",
      "Train Batch Loss: 0.014165865257382393\n",
      "Train Batch Loss: 0.014240405522286892\n",
      "Train Batch Loss: 0.016818374395370483\n",
      "Train Batch Loss: 0.016790110617876053\n",
      "Train Batch Loss: 0.018989577889442444\n",
      "Train Batch Loss: 0.016724813729524612\n",
      "Train Batch Loss: 0.016442079097032547\n",
      "Train Batch Loss: 0.012124378234148026\n",
      "Train Batch Loss: 0.019779367372393608\n",
      "Train Batch Loss: 0.019316788762807846\n",
      "Train Batch Loss: 0.015737714245915413\n",
      "Train Batch Loss: 0.014354405924677849\n",
      "Train Batch Loss: 0.016722507774829865\n",
      "valid Batch Loss: 0.01917041651904583\n",
      "valid Batch Loss: 0.01664913073182106\n",
      "valid Batch Loss: 0.019801698625087738\n",
      "valid Batch Loss: 0.018446266651153564\n",
      "valid Batch Loss: 0.01119719073176384\n",
      "valid Batch Loss: 0.019052835181355476\n",
      "valid Batch Loss: 0.011974461376667023\n",
      "valid Batch Loss: 0.016485093161463737\n",
      "valid Batch Loss: 0.015429551713168621\n",
      "valid Batch Loss: 0.016502782702445984\n",
      "valid Batch Loss: 0.017312943935394287\n",
      "valid Batch Loss: 0.01538070011883974\n",
      "valid Batch Loss: 0.013846077024936676\n",
      "valid Batch Loss: 0.014365112408995628\n",
      "valid Batch Loss: 0.01689562201499939\n",
      "valid Batch Loss: 0.015698565170168877\n",
      "valid Batch Loss: 0.0215604305267334\n",
      "valid Batch Loss: 0.012315986678004265\n",
      "valid Batch Loss: 0.019461173564195633\n",
      "valid Batch Loss: 0.0145841334015131\n",
      "valid Batch Loss: 0.015923967584967613\n",
      "valid Batch Loss: 0.015278158709406853\n",
      "valid Batch Loss: 0.013733710162341595\n",
      "valid Batch Loss: 0.016816603019833565\n",
      "valid Batch Loss: 0.012136747129261494\n",
      "valid Batch Loss: 0.01663058251142502\n",
      "valid Batch Loss: 0.015644336119294167\n",
      "valid Batch Loss: 0.017244860529899597\n",
      "valid Batch Loss: 0.01675315946340561\n",
      "valid Batch Loss: 0.015041818842291832\n",
      "valid Batch Loss: 0.014171415939927101\n",
      "valid Batch Loss: 0.015239616855978966\n",
      "valid Batch Loss: 0.018562505021691322\n",
      "valid Batch Loss: 0.012594982050359249\n",
      "valid Batch Loss: 0.017550170421600342\n",
      "valid Batch Loss: 0.016585486009716988\n",
      "valid Batch Loss: 0.0191287063062191\n",
      "valid Batch Loss: 0.019266225397586823\n",
      "valid Batch Loss: 0.01799597591161728\n",
      "valid Batch Loss: 0.014898399822413921\n",
      "valid Batch Loss: 0.013791441917419434\n",
      "valid Batch Loss: 0.012936895713210106\n",
      "valid Batch Loss: 0.01448490098118782\n",
      "valid Batch Loss: 0.01754673570394516\n",
      "valid Batch Loss: 0.01242920197546482\n",
      "valid Batch Loss: 0.018191669136285782\n",
      "valid Batch Loss: 0.015584945678710938\n",
      "valid Batch Loss: 0.01835612580180168\n",
      "valid Batch Loss: 0.017048977315425873\n",
      "valid Batch Loss: 0.02028627321124077\n",
      "valid Batch Loss: 0.01528889685869217\n",
      "valid Batch Loss: 0.01607993245124817\n",
      "valid Batch Loss: 0.01848154515028\n",
      "valid Batch Loss: 0.015394527465105057\n",
      "valid Batch Loss: 0.020714040845632553\n",
      "valid Batch Loss: 0.015264991670846939\n",
      "valid Batch Loss: 0.013814039528369904\n",
      "valid Batch Loss: 0.013689642772078514\n",
      "valid Batch Loss: 0.01508323848247528\n",
      "valid Batch Loss: 0.014424976892769337\n",
      "valid Batch Loss: 0.014603869989514351\n",
      "valid Batch Loss: 0.013486308977007866\n",
      "Epoch: 8 | Train Loss: 0.01553597766906023 | Valid Loss: 0.016290981322526932\n",
      "\n",
      "Train Batch Loss: 0.018356282263994217\n",
      "Train Batch Loss: 0.014684575609862804\n",
      "Train Batch Loss: 0.01857331022620201\n",
      "Train Batch Loss: 0.018419917672872543\n",
      "Train Batch Loss: 0.013240773230791092\n",
      "Train Batch Loss: 0.01898311823606491\n",
      "Train Batch Loss: 0.01771942526102066\n",
      "Train Batch Loss: 0.015831924974918365\n",
      "Train Batch Loss: 0.01636527106165886\n",
      "Train Batch Loss: 0.012281283736228943\n",
      "Train Batch Loss: 0.015992848202586174\n",
      "Train Batch Loss: 0.015659350901842117\n",
      "Train Batch Loss: 0.01598341390490532\n",
      "Train Batch Loss: 0.01725832372903824\n",
      "Train Batch Loss: 0.0159903597086668\n",
      "Train Batch Loss: 0.016145585104823112\n",
      "Train Batch Loss: 0.011627205647528172\n",
      "Train Batch Loss: 0.015183581039309502\n",
      "Train Batch Loss: 0.012533670291304588\n",
      "Train Batch Loss: 0.017961174249649048\n",
      "Train Batch Loss: 0.014745271764695644\n",
      "Train Batch Loss: 0.018364299088716507\n",
      "Train Batch Loss: 0.014225457794964314\n",
      "Train Batch Loss: 0.016723833978176117\n",
      "Train Batch Loss: 0.013684003613889217\n",
      "Train Batch Loss: 0.015022821724414825\n",
      "Train Batch Loss: 0.016450945287942886\n",
      "Train Batch Loss: 0.01513116154819727\n",
      "Train Batch Loss: 0.013684837147593498\n",
      "Train Batch Loss: 0.01567159779369831\n",
      "Train Batch Loss: 0.01702364906668663\n",
      "Train Batch Loss: 0.016284029930830002\n",
      "Train Batch Loss: 0.014399955049157143\n",
      "Train Batch Loss: 0.012132279574871063\n",
      "Train Batch Loss: 0.014463315717875957\n",
      "Train Batch Loss: 0.01827956736087799\n",
      "Train Batch Loss: 0.01757322996854782\n",
      "Train Batch Loss: 0.015681294724345207\n",
      "Train Batch Loss: 0.013170478865504265\n",
      "Train Batch Loss: 0.01796884834766388\n",
      "Train Batch Loss: 0.013911327347159386\n",
      "Train Batch Loss: 0.015303309075534344\n",
      "Train Batch Loss: 0.014979390427470207\n",
      "Train Batch Loss: 0.013356627896428108\n",
      "Train Batch Loss: 0.01785496063530445\n",
      "Train Batch Loss: 0.018415678292512894\n",
      "Train Batch Loss: 0.01735920086503029\n",
      "Train Batch Loss: 0.0170822124928236\n",
      "Train Batch Loss: 0.013836905360221863\n",
      "Train Batch Loss: 0.015183326788246632\n",
      "Train Batch Loss: 0.014922946691513062\n",
      "Train Batch Loss: 0.01753823831677437\n",
      "Train Batch Loss: 0.021447446197271347\n",
      "Train Batch Loss: 0.014379617758095264\n",
      "Train Batch Loss: 0.015501894056797028\n",
      "Train Batch Loss: 0.015223560854792595\n",
      "Train Batch Loss: 0.012755187228322029\n",
      "Train Batch Loss: 0.010178168304264545\n",
      "Train Batch Loss: 0.015274878591299057\n",
      "Train Batch Loss: 0.016130592674016953\n",
      "Train Batch Loss: 0.019170496612787247\n",
      "Train Batch Loss: 0.013379601761698723\n",
      "Train Batch Loss: 0.013528889045119286\n",
      "Train Batch Loss: 0.015751924365758896\n",
      "Train Batch Loss: 0.010933978483080864\n",
      "Train Batch Loss: 0.015250042080879211\n",
      "Train Batch Loss: 0.016572579741477966\n",
      "Train Batch Loss: 0.01421379204839468\n",
      "Train Batch Loss: 0.01642196625471115\n",
      "Train Batch Loss: 0.018675412982702255\n",
      "Train Batch Loss: 0.013892404735088348\n",
      "Train Batch Loss: 0.015914570540189743\n",
      "Train Batch Loss: 0.014488250948488712\n",
      "Train Batch Loss: 0.012781267985701561\n",
      "Train Batch Loss: 0.017872348427772522\n",
      "Train Batch Loss: 0.01650615595281124\n",
      "Train Batch Loss: 0.015406442806124687\n",
      "Train Batch Loss: 0.016472849994897842\n",
      "Train Batch Loss: 0.016510672867298126\n",
      "Train Batch Loss: 0.017223842442035675\n",
      "Train Batch Loss: 0.018505917862057686\n",
      "Train Batch Loss: 0.012327207252383232\n",
      "Train Batch Loss: 0.012734651565551758\n",
      "Train Batch Loss: 0.015567685477435589\n",
      "Train Batch Loss: 0.015298332087695599\n",
      "Train Batch Loss: 0.01667732745409012\n",
      "Train Batch Loss: 0.011819612234830856\n",
      "Train Batch Loss: 0.014947626739740372\n",
      "Train Batch Loss: 0.015821928158402443\n",
      "Train Batch Loss: 0.013599765487015247\n",
      "Train Batch Loss: 0.015063688158988953\n",
      "Train Batch Loss: 0.0169479101896286\n",
      "Train Batch Loss: 0.014942674897611141\n",
      "Train Batch Loss: 0.01536821760237217\n",
      "Train Batch Loss: 0.015398887917399406\n",
      "Train Batch Loss: 0.014924358576536179\n",
      "Train Batch Loss: 0.01562761887907982\n",
      "Train Batch Loss: 0.0158726517111063\n",
      "Train Batch Loss: 0.014866461977362633\n",
      "Train Batch Loss: 0.016097184270620346\n",
      "Train Batch Loss: 0.014122516848146915\n",
      "Train Batch Loss: 0.01229953020811081\n",
      "Train Batch Loss: 0.014059197157621384\n",
      "Train Batch Loss: 0.015194281935691833\n",
      "Train Batch Loss: 0.012390279211103916\n",
      "Train Batch Loss: 0.012841560877859592\n",
      "Train Batch Loss: 0.01698579639196396\n",
      "Train Batch Loss: 0.013299808837473392\n",
      "Train Batch Loss: 0.015316613018512726\n",
      "Train Batch Loss: 0.015100155025720596\n",
      "Train Batch Loss: 0.013553693890571594\n",
      "Train Batch Loss: 0.014565730467438698\n",
      "Train Batch Loss: 0.014381702989339828\n",
      "Train Batch Loss: 0.016334734857082367\n",
      "Train Batch Loss: 0.017762377858161926\n",
      "Train Batch Loss: 0.014027352444827557\n",
      "Train Batch Loss: 0.022565390914678574\n",
      "Train Batch Loss: 0.014679040759801865\n",
      "Train Batch Loss: 0.013986192643642426\n",
      "Train Batch Loss: 0.0189367663115263\n",
      "Train Batch Loss: 0.016477346420288086\n",
      "Train Batch Loss: 0.01906384713947773\n",
      "Train Batch Loss: 0.016961481422185898\n",
      "Train Batch Loss: 0.016861002892255783\n",
      "Train Batch Loss: 0.016702696681022644\n",
      "Train Batch Loss: 0.015756674110889435\n",
      "Train Batch Loss: 0.011493400670588017\n",
      "Train Batch Loss: 0.017578091472387314\n",
      "Train Batch Loss: 0.017199942842125893\n",
      "Train Batch Loss: 0.01527455449104309\n",
      "Train Batch Loss: 0.016813253983855247\n",
      "Train Batch Loss: 0.018245816230773926\n",
      "Train Batch Loss: 0.015505054965615273\n",
      "Train Batch Loss: 0.015737168490886688\n",
      "Train Batch Loss: 0.015399269759654999\n",
      "Train Batch Loss: 0.014300909824669361\n",
      "Train Batch Loss: 0.012882648035883904\n",
      "Train Batch Loss: 0.014216240495443344\n",
      "Train Batch Loss: 0.01589471846818924\n",
      "Train Batch Loss: 0.02071588858962059\n",
      "Train Batch Loss: 0.012679282575845718\n",
      "Train Batch Loss: 0.014346526935696602\n",
      "Train Batch Loss: 0.013631223700940609\n",
      "Train Batch Loss: 0.017172032967209816\n",
      "Train Batch Loss: 0.016797419637441635\n",
      "Train Batch Loss: 0.017637118697166443\n",
      "Train Batch Loss: 0.01487499289214611\n",
      "Train Batch Loss: 0.01737215742468834\n",
      "Train Batch Loss: 0.01606915146112442\n",
      "Train Batch Loss: 0.012361032888293266\n",
      "Train Batch Loss: 0.015307361260056496\n",
      "Train Batch Loss: 0.017729490995407104\n",
      "Train Batch Loss: 0.014074867591261864\n",
      "Train Batch Loss: 0.017362207174301147\n",
      "Train Batch Loss: 0.014892460778355598\n",
      "Train Batch Loss: 0.01695246621966362\n",
      "Train Batch Loss: 0.01511080376803875\n",
      "Train Batch Loss: 0.012436993420124054\n",
      "Train Batch Loss: 0.01715194061398506\n",
      "Train Batch Loss: 0.014382931403815746\n",
      "Train Batch Loss: 0.017193056643009186\n",
      "Train Batch Loss: 0.015641720965504646\n",
      "Train Batch Loss: 0.016034763306379318\n",
      "Train Batch Loss: 0.01771068572998047\n",
      "Train Batch Loss: 0.01888119801878929\n",
      "Train Batch Loss: 0.01664683222770691\n",
      "Train Batch Loss: 0.014176626689732075\n",
      "Train Batch Loss: 0.02165308967232704\n",
      "Train Batch Loss: 0.014230825006961823\n",
      "Train Batch Loss: 0.016413841396570206\n",
      "Train Batch Loss: 0.017703644931316376\n",
      "Train Batch Loss: 0.019482137635350227\n",
      "Train Batch Loss: 0.014035604894161224\n",
      "Train Batch Loss: 0.014243962243199348\n",
      "Train Batch Loss: 0.01616533100605011\n",
      "Train Batch Loss: 0.018320385366678238\n",
      "Train Batch Loss: 0.013247465714812279\n",
      "Train Batch Loss: 0.019636105746030807\n",
      "Train Batch Loss: 0.01220775954425335\n",
      "Train Batch Loss: 0.017576981335878372\n",
      "Train Batch Loss: 0.017333827912807465\n",
      "Train Batch Loss: 0.01745157316327095\n",
      "Train Batch Loss: 0.01729314774274826\n",
      "Train Batch Loss: 0.013416844420135021\n",
      "Train Batch Loss: 0.015634430572390556\n",
      "Train Batch Loss: 0.01930292323231697\n",
      "Train Batch Loss: 0.015824787318706512\n",
      "Train Batch Loss: 0.015964917838573456\n",
      "Train Batch Loss: 0.017142942175269127\n",
      "Train Batch Loss: 0.014044104143977165\n",
      "Train Batch Loss: 0.012590724043548107\n",
      "Train Batch Loss: 0.015209624543786049\n",
      "Train Batch Loss: 0.016117431223392487\n",
      "Train Batch Loss: 0.017740637063980103\n",
      "Train Batch Loss: 0.01900206133723259\n",
      "Train Batch Loss: 0.015824461355805397\n",
      "Train Batch Loss: 0.017536906525492668\n",
      "Train Batch Loss: 0.018853262066841125\n",
      "Train Batch Loss: 0.02055664360523224\n",
      "Train Batch Loss: 0.016595404595136642\n",
      "Train Batch Loss: 0.01147964783012867\n",
      "Train Batch Loss: 0.014540523290634155\n",
      "Train Batch Loss: 0.015984943136572838\n",
      "Train Batch Loss: 0.014458956196904182\n",
      "Train Batch Loss: 0.018044058233499527\n",
      "Train Batch Loss: 0.013424891978502274\n",
      "Train Batch Loss: 0.01689862087368965\n",
      "Train Batch Loss: 0.017193138599395752\n",
      "Train Batch Loss: 0.014166753739118576\n",
      "Train Batch Loss: 0.017348170280456543\n",
      "Train Batch Loss: 0.020905746147036552\n",
      "Train Batch Loss: 0.015548305585980415\n",
      "Train Batch Loss: 0.014667144045233727\n",
      "Train Batch Loss: 0.019468775019049644\n",
      "Train Batch Loss: 0.016407012939453125\n",
      "Train Batch Loss: 0.019928578287363052\n",
      "Train Batch Loss: 0.01260354183614254\n",
      "Train Batch Loss: 0.012939237989485264\n",
      "Train Batch Loss: 0.014757401309907436\n",
      "Train Batch Loss: 0.01554372999817133\n",
      "Train Batch Loss: 0.01676136627793312\n",
      "Train Batch Loss: 0.015989577397704124\n",
      "Train Batch Loss: 0.015294271521270275\n",
      "Train Batch Loss: 0.01719774305820465\n",
      "Train Batch Loss: 0.014909805729985237\n",
      "Train Batch Loss: 0.017189713194966316\n",
      "Train Batch Loss: 0.01636776514351368\n",
      "Train Batch Loss: 0.016286229714751244\n",
      "Train Batch Loss: 0.01662006415426731\n",
      "Train Batch Loss: 0.014760766178369522\n",
      "Train Batch Loss: 0.014956727623939514\n",
      "Train Batch Loss: 0.013757845386862755\n",
      "Train Batch Loss: 0.01798267476260662\n",
      "Train Batch Loss: 0.014870847575366497\n",
      "Train Batch Loss: 0.014738813042640686\n",
      "Train Batch Loss: 0.01774209551513195\n",
      "Train Batch Loss: 0.018129881471395493\n",
      "Train Batch Loss: 0.013154204934835434\n",
      "Train Batch Loss: 0.01812826283276081\n",
      "Train Batch Loss: 0.01614989899098873\n",
      "Train Batch Loss: 0.015683647245168686\n",
      "Train Batch Loss: 0.015425103716552258\n",
      "Train Batch Loss: 0.016690203920006752\n",
      "Train Batch Loss: 0.015215321443974972\n",
      "Train Batch Loss: 0.013282408006489277\n",
      "Train Batch Loss: 0.013518201187252998\n",
      "Train Batch Loss: 0.015521310269832611\n",
      "Train Batch Loss: 0.01400088332593441\n",
      "Train Batch Loss: 0.016325991600751877\n",
      "Train Batch Loss: 0.015818409621715546\n",
      "valid Batch Loss: 0.01942448318004608\n",
      "valid Batch Loss: 0.016747184097766876\n",
      "valid Batch Loss: 0.02008858695626259\n",
      "valid Batch Loss: 0.018634553998708725\n",
      "valid Batch Loss: 0.011012042872607708\n",
      "valid Batch Loss: 0.019446425139904022\n",
      "valid Batch Loss: 0.011881621554493904\n",
      "valid Batch Loss: 0.016614587977528572\n",
      "valid Batch Loss: 0.015460817143321037\n",
      "valid Batch Loss: 0.01661483757197857\n",
      "valid Batch Loss: 0.017428560182452202\n",
      "valid Batch Loss: 0.01549952756613493\n",
      "valid Batch Loss: 0.014018181711435318\n",
      "valid Batch Loss: 0.014342915266752243\n",
      "valid Batch Loss: 0.017067009583115578\n",
      "valid Batch Loss: 0.015888115391135216\n",
      "valid Batch Loss: 0.02161276713013649\n",
      "valid Batch Loss: 0.01223650760948658\n",
      "valid Batch Loss: 0.019432121887803078\n",
      "valid Batch Loss: 0.014732991345226765\n",
      "valid Batch Loss: 0.016053976491093636\n",
      "valid Batch Loss: 0.015450319275259972\n",
      "valid Batch Loss: 0.013889618217945099\n",
      "valid Batch Loss: 0.016997981816530228\n",
      "valid Batch Loss: 0.012206586077809334\n",
      "valid Batch Loss: 0.016788629814982414\n",
      "valid Batch Loss: 0.015682339668273926\n",
      "valid Batch Loss: 0.017327535897493362\n",
      "valid Batch Loss: 0.016777880489826202\n",
      "valid Batch Loss: 0.015103664249181747\n",
      "valid Batch Loss: 0.01409139297902584\n",
      "valid Batch Loss: 0.015508950687944889\n",
      "valid Batch Loss: 0.01884869486093521\n",
      "valid Batch Loss: 0.012684956192970276\n",
      "valid Batch Loss: 0.01764589734375477\n",
      "valid Batch Loss: 0.016566898673772812\n",
      "valid Batch Loss: 0.01949533075094223\n",
      "valid Batch Loss: 0.019530830904841423\n",
      "valid Batch Loss: 0.018067121505737305\n",
      "valid Batch Loss: 0.014998147264122963\n",
      "valid Batch Loss: 0.013769512996077538\n",
      "valid Batch Loss: 0.01287064328789711\n",
      "valid Batch Loss: 0.014505980536341667\n",
      "valid Batch Loss: 0.017630524933338165\n",
      "valid Batch Loss: 0.012331383302807808\n",
      "valid Batch Loss: 0.018581572920084\n",
      "valid Batch Loss: 0.015660328790545464\n",
      "valid Batch Loss: 0.018562082201242447\n",
      "valid Batch Loss: 0.016990995034575462\n",
      "valid Batch Loss: 0.020730948075652122\n",
      "valid Batch Loss: 0.015363668091595173\n",
      "valid Batch Loss: 0.016161412000656128\n",
      "valid Batch Loss: 0.018660303205251694\n",
      "valid Batch Loss: 0.015501389279961586\n",
      "valid Batch Loss: 0.020976927131414413\n",
      "valid Batch Loss: 0.015483771450817585\n",
      "valid Batch Loss: 0.01383367832750082\n",
      "valid Batch Loss: 0.013928811065852642\n",
      "valid Batch Loss: 0.015227816998958588\n",
      "valid Batch Loss: 0.01444309577345848\n",
      "valid Batch Loss: 0.014752700924873352\n",
      "valid Batch Loss: 0.013454768806695938\n",
      "Epoch: 9 | Train Loss: 0.015528947114944458 | Valid Loss: 0.016417905688285828\n",
      "\n",
      "\u001b[34mEarly stopping triggered after 4 epochs with no improvement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"isic_lesions_24\", job_type=\"pretrain\")\n",
    "\n",
    "best_epoch_auroc = -np.inf\n",
    "best_valid_loss = np.inf\n",
    "early_stopping_patience = 4\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    model_ft, epoch_loss = train_model(\n",
    "        model_ft, train_dataloader, criterion, optimizer_ft\n",
    "    )\n",
    "    model_ft, valid_loss = validate_model(\n",
    "        model_ft, valid_dataloader, criterion, optimizer_ft\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch} | Train Loss: {epoch_loss} | Valid Loss: {valid_loss}\\n\"\n",
    "    )\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"epoch_loss\": epoch_loss,\n",
    "            \"epoch_val_loss\": valid_loss,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # earlystopping dependent on validation loss\n",
    "    if best_valid_loss >= valid_loss:\n",
    "        print(f\"{b_}Validation Loss Improved ({best_valid_loss} ---> {valid_loss}){sr_}\")\n",
    "        \n",
    "        # checkpointing\n",
    "        best_model_wts = copy.deepcopy(model_ft.state_dict())\n",
    "        PATH = \"../models/pretrain_valid_loss{:.4f}_epoch{:.0f}.bin\".format(valid_loss, epoch)\n",
    "        torch.save(model_ft.state_dict(), PATH)\n",
    "        # Save a model file from the current directory\n",
    "        print(f\"{b_}Model Saved{sr_}\")\n",
    "        best_valid_loss = valid_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stopping_patience:\n",
    "        print(\n",
    "            f\"{b_}Early stopping triggered after {epochs_no_improve} epochs with no improvement.{sr_}\"\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
