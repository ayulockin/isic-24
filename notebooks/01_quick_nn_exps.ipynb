{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.12 (you have 1.4.11). Upgrade using: pip install --upgrade albumentations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.cpu_count())\n",
    "\n",
    "import copy\n",
    "import wandb\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torcheval.metrics.functional import binary_auroc\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from colorama import Fore, Style\n",
    "b_ = Fore.BLUE\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.set_float32_matmul_precision('highest')\n",
    "\n",
    "# Set the random seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1668685/1003576336.py:4: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_metadata_df = pd.read_csv(\"../data/stratified_5_fold_train_metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "def add_path(row):\n",
    "    return f\"../data/train-image/image/{row.isic_id}.jpg\"\n",
    "\n",
    "train_metadata_df = pd.read_csv(\"../data/stratified_5_fold_train_metadata.csv\")\n",
    "train_metadata_df[\"path\"] = train_metadata_df.apply(lambda row: add_path(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform=None, target_transform=None):\n",
    "        assert \"path\" in df.columns\n",
    "        assert \"target\" in df.columns\n",
    "\n",
    "        self.paths = df.path.tolist()\n",
    "        self.labels = df.target.tolist() # binary\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image = read_image(self.paths[idx]).to(torch.float32) / 255.0\n",
    "        label = self.labels[idx] / 1.0\n",
    "        if self.transform:\n",
    "            image = image.numpy().transpose((1,2,0))\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "    def get_class_samples(self, class_label):\n",
    "        indices = [i for i, label in enumerate(self.labels) if label == class_label]\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/pydantic/main.py:308: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `Union[float, json-or-python[json=list[float], python=list[float]]]` but got `tuple` - serialized value may not be as expected\n",
      "  Expected `Union[float, json-or-python[json=list[float], python=list[float]]]` but got `tuple` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "transforms_train = A.Compose([\n",
    "    A.Resize(124, 124),\n",
    "    A.Normalize(\n",
    "        # mean=(0.6962, 0.5209, 0.4193),\n",
    "        # std=(0.1395, 0.1320, 0.1240)\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "transforms_valid = A.Compose([\n",
    "    A.Resize(124, 124),\n",
    "    A.Normalize(\n",
    "        # mean=(0.6962, 0.5209, 0.4193),\n",
    "        # std=(0.1395, 0.1320, 0.1240)\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, train_step, scheduler=None):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_auroc = 0.0\n",
    "\n",
    "    for idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).flatten().to(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs).flatten()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_val = loss.detach()\n",
    "        running_loss += loss_val * inputs.size(0)\n",
    "\n",
    "        auroc = binary_auroc(input=outputs, target=labels).item()\n",
    "        running_auroc += auroc * inputs.size(0)\n",
    "\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            train_step += 1\n",
    "            wandb.log({\"train_loss\": loss_val, \"train_auroc\": auroc, \"train_step\": train_step})\n",
    "            print(f\"Train Batch Loss: {loss_val} | Train AUROC: {auroc}\")\n",
    "\n",
    "    epoch_loss = running_loss / dataset_sizes[\"train\"]\n",
    "    epoch_auroc = running_auroc / dataset_sizes[\"train\"]\n",
    "\n",
    "    return model, epoch_loss, epoch_auroc\n",
    "\n",
    "\n",
    "def validate_model(model, dataloader, criterions, optimizer, valid_step):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_auroc = 0.0\n",
    "\n",
    "    for idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).to(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss_val = loss.detach()\n",
    "        running_loss += loss_val * inputs.size(0)\n",
    "\n",
    "        auroc = binary_auroc(input=outputs, target=labels).item()\n",
    "        running_auroc += auroc * inputs.size(0)\n",
    "\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            valid_step += 1\n",
    "            wandb.log({\"valid_loss\": loss_val, \"valid_auroc\": auroc, \"valid_step\": valid_step})\n",
    "            print(f\"valid Batch Loss: {loss_val} | Valid AUROC: {auroc}\")\n",
    "\n",
    "    valid_loss = running_loss / dataset_sizes[\"val\"]\n",
    "    valid_auroc = running_auroc / dataset_sizes[\"val\"]\n",
    "\n",
    "    return model, valid_loss, valid_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinClassifier(nn.Module):\n",
    "    def __init__(self, model_name='resnet18', freeze_backbone=False):\n",
    "        super(SkinClassifier, self).__init__()\n",
    "        \n",
    "        # Load the specified pre-trained model\n",
    "        if model_name == 'resnet18':\n",
    "            self.backbone = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "            if freeze_backbone:\n",
    "                self.freeze_backbone()\n",
    "            num_ftrs = self.backbone.fc.in_features\n",
    "            self.backbone.fc = self.get_clf_head(num_ftrs, 1)\n",
    "        elif model_name == 'convnext_tiny':\n",
    "            self.backbone = models.convnext_tiny(weights=\"IMAGENET1K_V1\")\n",
    "            if freeze_backbone:\n",
    "                self.freeze_backbone()\n",
    "            num_ftrs = self.backbone.classifier[2].in_features\n",
    "            self.backbone.classifier[2] = self.get_clf_head(num_ftrs, 1)\n",
    "        elif model_name == \"efficientnet_v2_s\":\n",
    "            self.backbone = models.efficientnet_v2_s(weights=\"IMAGENET1K_V1\")\n",
    "            if freeze_backbone:\n",
    "                self.freeze_backbone()\n",
    "            num_ftrs = self.backbone.classifier[2].in_features\n",
    "            self.backbone.classifier[2] = self.get_clf_head(num_ftrs, 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} not supported\")        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def get_clf_head(self, in_features, out_features, bias=None):\n",
    "        return nn.Linear(in_features, out_features)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        non_trainable_params = sum(p.numel() for p in self.parameters() if not p.requires_grad)\n",
    "        return trainable_params, non_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = SkinClassifier(model_name='resnet18', freeze_backbone=True)\n",
    "model = model.to(device)\n",
    "model = torch.compile(model)\n",
    "print(model)\n",
    "\n",
    "# Example to get the number of trainable and non-trainable parameters\n",
    "trainable_params, non_trainable_params = model.count_parameters()\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit on few samples :check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_metadata_df.loc[train_metadata_df.fold == 0] # using a subset for training\n",
    "# valid_df = train_metadata_df.loc[train_metadata_df.fold == 1] # using another fold for validation\n",
    "\n",
    "# num_workers = 24 # based on profiling\n",
    "\n",
    "# train_dataset = SkinDataset(train_df.iloc[:10], transform=transforms_train)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=num_workers, pin_memory=True, persistent_workers=True, drop_last=True)\n",
    "\n",
    "# dataset_sizes = {\"train\": len(train_dataset)}\n",
    "# dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = wandb.init(project=\"isic_lesions_24\", group=\"exps\")\n",
    "# wandb.define_metric(\"train_step\")\n",
    "# wandb.define_metric(\"valid_step\")\n",
    "\n",
    "# train_step = 0\n",
    "# valid_step = 0\n",
    "\n",
    "# best_epoch_auroc = -np.inf\n",
    "# best_valid_loss = np.inf\n",
    "# early_stopping_patience = 4\n",
    "# epochs_no_improve = 0\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     model_ft, epoch_loss, epoch_auroc = train_model(\n",
    "#         model, train_dataloader, criterion, optimizer, train_step,\n",
    "#     )\n",
    "\n",
    "#     print(\n",
    "#         f\"Epoch: {epoch} | Train Loss: {epoch_loss} | AUROC: {epoch_auroc}\"\n",
    "#     )\n",
    "#     wandb.log(\n",
    "#         {\n",
    "#             \"epoch\": epoch,\n",
    "#             \"epoch_loss\": epoch_loss,\n",
    "#             \"epoch_auroc\": epoch_auroc,\n",
    "#         }\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss at init :check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.log(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train fully and see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_metadata_df.loc[train_metadata_df.fold == 0] # using a subset for training\n",
    "valid_df = train_metadata_df.loc[train_metadata_df.fold == 1] # using another fold for validation\n",
    "\n",
    "num_workers = 24 # based on profiling\n",
    "\n",
    "train_dataset = SkinDataset(train_df, transform=transforms_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True, persistent_workers=True, drop_last=True)\n",
    "\n",
    "valid_dataset = SkinDataset(valid_df, transform=transforms_valid)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=128, shuffle=False, num_workers=num_workers, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "dataset_sizes = {\"train\": len(train_dataset), \"val\": len(valid_dataset)}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, train_step, scheduler=None):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_auroc = 0.0\n",
    "\n",
    "    for idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).flatten().to(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs).flatten()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_val = loss.detach()\n",
    "        running_loss += loss_val * inputs.size(0)\n",
    "\n",
    "        auroc = binary_auroc(input=outputs, target=labels).item()\n",
    "        running_auroc += auroc * inputs.size(0)\n",
    "\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            train_step += 1\n",
    "            wandb.log({\"train_loss\": loss_val, \"train_auroc\": auroc, \"train_step\": train_step})\n",
    "            print(f\"Train Batch Loss: {loss_val} | Train AUROC: {auroc}\")\n",
    "\n",
    "    epoch_loss = running_loss / dataset_sizes[\"train\"]\n",
    "    epoch_auroc = running_auroc / dataset_sizes[\"train\"]\n",
    "\n",
    "    return model, epoch_loss, epoch_auroc\n",
    "\n",
    "\n",
    "def validate_model(model, dataloader, criterions, optimizer, valid_step):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_auroc = 0.0\n",
    "\n",
    "    for idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).flatten().to(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs).flatten()\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss_val = loss.detach()\n",
    "        running_loss += loss_val * inputs.size(0)\n",
    "\n",
    "        auroc = binary_auroc(input=outputs, target=labels).item()\n",
    "        running_auroc += auroc * inputs.size(0)\n",
    "\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            valid_step += 1\n",
    "            wandb.log({\"valid_loss\": loss_val, \"valid_auroc\": auroc, \"valid_step\": valid_step})\n",
    "            print(f\"valid Batch Loss: {loss_val} | Valid AUROC: {auroc}\")\n",
    "\n",
    "    valid_loss = running_loss / dataset_sizes[\"val\"]\n",
    "    valid_auroc = running_auroc / dataset_sizes[\"val\"]\n",
    "\n",
    "    return model, valid_loss, valid_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"isic_lesions_24\", job_type=\"pretrain\")\n",
    "\n",
    "model_name = \"resnet18\"\n",
    "exp_name = \"train_linear\"\n",
    "\n",
    "wandb.define_metric(\"train_step\")\n",
    "wandb.define_metric(\"valid_step\")\n",
    "\n",
    "train_step = 0\n",
    "valid_step = 0\n",
    "\n",
    "best_epoch_auroc = -np.inf\n",
    "best_valid_loss = np.inf\n",
    "early_stopping_patience = 4\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(40):\n",
    "    model_ft, epoch_loss, epoch_train_auroc = train_model(\n",
    "        model, train_dataloader, criterion, optimizer, train_step,\n",
    "    )\n",
    "\n",
    "    model_ft, valid_loss, epoch_valid_auroc = validate_model(\n",
    "        model, valid_dataloader, criterion, optimizer, valid_step\n",
    "    )\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"epoch_loss\": epoch_loss,\n",
    "            \"epoch_val_loss\": valid_loss,\n",
    "            \"epoch_train_auroc\": epoch_train_auroc,\n",
    "            \"epoch_valid_auroc\": epoch_valid_auroc\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch} | Train Loss: {epoch_loss} | Valid Loss: {valid_loss}\\n\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch: {epoch} | Train AUROC: {epoch_train_auroc} | Valid AUROC: {epoch_valid_auroc}\\n\"\n",
    "    )\n",
    "\n",
    "    # earlystopping dependent on validation loss\n",
    "    if best_valid_loss >= valid_loss:\n",
    "        print(f\"{b_}Validation Loss Improved ({best_valid_loss} ---> {valid_loss}){sr_}\")\n",
    "        \n",
    "        # checkpointing\n",
    "        best_model_wts = copy.deepcopy(model_ft.state_dict())\n",
    "        PATH = f\"../models/{model_name}_{exp_name}_valid_loss{valid_loss}_epoch{epoch}.bin\"\n",
    "        torch.save(model_ft.state_dict(), PATH)\n",
    "        # Save a model file from the current directory\n",
    "        print(f\"{b_}Model Saved{sr_}\")\n",
    "        best_valid_loss = valid_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stopping_patience:\n",
    "        print(\n",
    "            f\"{b_}Early stopping triggered after {epochs_no_improve} epochs with no improvement.{sr_}\"\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch various models and train with frozen backbone - find best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the effect of batch size - try to consume the most GPU allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with bias of the linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
